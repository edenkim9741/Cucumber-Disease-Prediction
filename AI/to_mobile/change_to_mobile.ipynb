{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52bfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/executorch/exir/dialects/edge/_ops.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading the fine-tuned model structure and weights...\n",
      "--> Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/eden/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import time\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. ëª¨ë¸ êµ¬ì¡° ì •ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "# -----------------------------------\n",
    "print(\"1. Loading the fine-tuned model structure and weights...\")\n",
    "\n",
    "# --- ì„¤ì •ê°’  ---\n",
    "MODEL_REPO = \"facebookresearch/dinov2\"\n",
    "MODEL_NAME = \"dinov2_vits14\"\n",
    "# í´ëž˜ìŠ¤ ìˆ˜ì™€ ìˆœì„œë¥¼ ì •í™•ížˆ ë§žì¶”ê¸°\n",
    "NUM_CLASSES = 3 \n",
    "CLASS_NAMES = ['downy','healthy', 'powdery']\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"--> Using device: {DEVICE}\")\n",
    "# --- ëª¨ë¸ êµ¬ì¡° ë§Œë“¤ê¸°  ---\n",
    "try:\n",
    "    # torch.hubë¥¼ ì´ìš©í•´ DINOv2 ëª¨ë¸ êµ¬ì¡° ë¡œë“œ\n",
    "    model = torch.hub.load(MODEL_REPO, MODEL_NAME, pretrained=False) # pretrained=Falseë¡œ ì„¤ì •\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸°(ë¨¸ë¦¬)\n",
    "    num_features = 384 # ViT-Smallì˜ íŠ¹ì§• ë²¡í„° í¬ê¸°\n",
    "    model.head = nn.Linear(num_features, NUM_CLASSES)\n",
    "    \n",
    "    # --- ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° ---\n",
    "    model.load_state_dict(torch.load('/Cucumber-Disease-Prediction/AI/dinov2_hub_finetuned_model.pth', map_location=DEVICE))\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval() # í‰ê°€ ëª¨ë“œ\n",
    "\n",
    "    # ëª¨ë¸ì„ contiguous í¬ë§·ìœ¼ë¡œ ê°•ì œ ë³€í™˜\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "    for buffer_name, buffer in model.named_buffers():\n",
    "        model.register_buffer(buffer_name, buffer.contiguous())\n",
    "\n",
    "    model = model.to(memory_format=torch.contiguous_format)\n",
    "\n",
    "    print(\"--> Model loaded successfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to load the model.\")\n",
    "    print(f\"--> Ensure 'dinov2_hub_finetuned_model.pth' is in the same directory.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.is_contiguous(memory_format=torch.channels_last):\n",
    "        print(f\"âš ï¸ Parameter {name} uses channels_last\")\n",
    "\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224).contiguous(), )\n",
    "\n",
    "edge_ir = torch.export.export(model, sample_inputs)\n",
    "\n",
    "\n",
    "# export í›„, executorch ë³€í™˜ ì „ì—\n",
    "for node in edge_ir.graph.nodes:\n",
    "    if hasattr(node, 'meta') and 'memory_format' in node.meta:\n",
    "        if node.meta['memory_format'] == torch.channels_last:\n",
    "            print(f\"âš ï¸ Node {node} uses channels_last\")\n",
    "            node.meta['memory_format'] = torch.contiguous_format\n",
    "\n",
    "et_program = to_edge_transform_and_lower(\n",
    "    edge_ir,\n",
    "    partitioner=[XnnpackPartitioner()]\n",
    ").to_executorch()\n",
    "\n",
    "with open(\"vit_model.pte\", \"wb\") as f:\n",
    "    f.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2883cfb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'executorch' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexecutorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexecutorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'executorch' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import executorch\n",
    "print(executorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bff111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Converting model to TorchScript...\n",
      "--> Tracing successful.\n",
      "--> Mobile optimization successful.\n",
      "\n",
      "SUCCESS! Model saved as 'dinov2_mobile.ptl'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "# -----------------------------------\n",
    "# 2. ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜ (Tracing)\n",
    "# -----------------------------------\n",
    "print(\"\\n2. Converting model to TorchScript...\")\n",
    "\n",
    "# DINOv2 (ViT-S)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 224x224 ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "# (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 3, ë†’ì´ 224, ë„ˆë¹„ 224)\n",
    "# ë§Œì•½ ë‹¤ë¥¸ í¬ê¸°ë¡œ fine-tuning í•˜ì…¨ë‹¤ë©´ ì´ ê°’ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "try:\n",
    "    # 1. Tracing: ëª¨ë¸ì— ë”ë¯¸ ìž…ë ¥ì„ í†µê³¼ì‹œì¼œ ì—°ì‚° ê·¸ëž˜í”„ë¥¼ ê¸°ë¡\n",
    "    traced_script_module = torch.jit.trace(model, dummy_input)\n",
    "    print(\"--> Tracing successful.\")\n",
    "    \n",
    "    # 2. Optimize for Mobile: ëª¨ë°”ì¼ í™˜ê²½ì— ë§žê²Œ ëª¨ë¸ ìµœì í™”\n",
    "    optimized_lit_module = optimize_for_mobile(traced_script_module)\n",
    "    print(\"--> Mobile optimization successful.\")\n",
    "\n",
    "    # 3. Save: ìµœì í™”ëœ .ptl íŒŒì¼ë¡œ ì €ìž¥\n",
    "    output_filename = \"dinov2_mobile.ptl\"\n",
    "    optimized_lit_module.save(output_filename)\n",
    "    \n",
    "    print(f\"\\nSUCCESS! Model saved as '{output_filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to convert the model.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f66ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models, datasets # datasets ì¶”ê°€\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "class_names = ['disease','downy', 'healthy', 'powdery']\n",
    "\n",
    "# ==============================================================\n",
    "# 2. ëª¨ë¸ ì •ì˜\n",
    "# ==============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\n",
    "model = model.eval().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('/Cucumber-Disease-Prediction/AI/to_mobile/mob_ood_nc.pth', weights_only=False, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6511d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'good_img.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      7\u001b[39m preprocess = transforms.Compose([\n\u001b[32m      8\u001b[39m     transforms.Resize(\u001b[32m256\u001b[39m),\n\u001b[32m      9\u001b[39m     transforms.CenterCrop(\u001b[32m224\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m                          std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m     13\u001b[39m ])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ì´ë¯¸ì§€ ì—´ê¸°\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgood_img.jpeg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m img_tensor = preprocess(img).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     19\u001b[39m random_input = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/PIL/Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'good_img.jpeg'"
     ]
    }
   ],
   "source": [
    "# image.pngë¥¼ ìž…ë ¥í•´ë³´ëŠ” ì½”ë“œ\n",
    "\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ì´ë¯¸ì§€ ì—´ê¸°\n",
    "img = Image.open(\"/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/good_img.jpeg\").convert(\"RGB\")\n",
    "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "random_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    output = nn.functional.softmax(model(img_tensor), dim=1)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a9c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Converting model to TorchScript...\n",
      "3. Converting model to TorchScript Lite (.ptl)...\n",
      "--> Attempting torch.jit.script(model)...\n",
      "--> Scripting successful.\n",
      "--> Mobile optimization successful.\n",
      "\n",
      "SUCCESS! Model saved as 'mobilenetCucumberMobile.ptl'\n",
      "--> This file is ready for your Kotlin (Android) app.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "# -----------------------------------\n",
    "# 2. ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜ (Tracing)\n",
    "# -----------------------------------\n",
    "print(\"\\n2. Converting model to TorchScript...\")\n",
    "\n",
    "# DINOv2 (ViT-S)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 224x224 ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "# (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 3, ë†’ì´ 224, ë„ˆë¹„ 224)\n",
    "# ë§Œì•½ ë‹¤ë¥¸ í¬ê¸°ë¡œ fine-tuning í•˜ì…¨ë‹¤ë©´ ì´ ê°’ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "model.eval()\n",
    "\n",
    "OUTPUT_MODEL_NAME = \"mobilenetCucumberMobile.ptl\"\n",
    "NO_OPT_MODEL_NAME = \"mobilenetCucumberNoOpt.ptl\"\n",
    "\n",
    "# --- 4. TorchScriptë¡œ ë³€í™˜ ë° ëª¨ë°”ì¼ ìµœì í™” ---\n",
    "print(f\"3. Converting model to TorchScript Lite (.ptl)...\")\n",
    "\n",
    "# âš ï¸ 'torch.jit.trace' ëŒ€ì‹  'torch.jit.script'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# 'script' ë°©ì‹ì€ ë”ë¯¸ ìž…ë ¥(dummy_input)ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n",
    "try:\n",
    "    # 1. Scripting: ëª¨ë¸ì˜ ì½”ë“œë¥¼ ì§ì ‘ ë¶„ì„í•˜ì—¬ ë³€í™˜\n",
    "    print(\"--> Attempting torch.jit.script(model)...\")\n",
    "    scripted_module = torch.jit.script(model) # ðŸŒŸ ì´ ë¶€ë¶„ì´ ë³€ê²½ë¨\n",
    "    print(\"--> Scripting successful.\")\n",
    "\n",
    "    scripted_module.save(NO_OPT_MODEL_NAME)\n",
    "    \n",
    "    # 2. Optimize for Mobile: ëª¨ë°”ì¼ í™˜ê²½ì— ë§žê²Œ ëª¨ë¸ ìµœì í™”\n",
    "    optimized_lit_module = optimize_for_mobile(scripted_module) # ðŸŒŸ ìž…ë ¥ ë³€ìˆ˜ ë³€ê²½\n",
    "    print(\"--> Mobile optimization successful.\")\n",
    "\n",
    "    # 3. Save: ìµœì í™”ëœ .ptl íŒŒì¼ë¡œ ì €ìž¥\n",
    "    optimized_lit_module.save(OUTPUT_MODEL_NAME)\n",
    "    \n",
    "    print(f\"\\nSUCCESS! Model saved as '{OUTPUT_MODEL_NAME}'\")\n",
    "    print(\"--> This file is ready for your Kotlin (Android) app.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to convert the model with 'torch.jit.script'.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a016307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/executorch/exir/dialects/edge/_ops.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m model = models.mobilenet_v2(pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model.classifier[\u001b[32m1\u001b[39m] = \u001b[43mnn\u001b[49m.Linear(model.last_channel, \u001b[38;5;28mlen\u001b[39m(class_names))\n\u001b[32m     15\u001b[39m model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33m/Cucumber-Disease-Prediction/AI/to_mobile/mob_ood.pth\u001b[39m\u001b[33m'\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m, map_location=device))\n\u001b[32m     16\u001b[39m model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "\n",
    "class_names = ['disease', 'downy', 'healthy', 'powdery']\n",
    "\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224), )\n",
    "\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\n",
    "\n",
    "model.load_state_dict(torch.load('/Cucumber-Disease-Prediction/AI/to_mobile/mob_ood.pth', weights_only=False, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "et_program = to_edge_transform_and_lower(\n",
    "    torch.export.export(model, sample_inputs),\n",
    "    partitioner=[XnnpackPartitioner()]\n",
    ").to_executorch()\n",
    "\n",
    "with open(\"model_ood.pte\", \"wb\") as f:\n",
    "    f.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaab797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:134] InternalConsistency verification requested but not available\n",
      "[tensor_util_portable.cpp:130] Check failed (all_contiguous || all_channels_last): 2 input tensors have different dim orders\n",
      "[op_clone.cpp:41] Check failed (tensors_have_same_dim_order(self, out)): \n",
      "[method.cpp:1314] KernelCall failed at instruction 0:78 in operator aten::clone.out: 0x12\n",
      "[method.cpp:1324] arg 0 with type id 1\n",
      "[method.cpp:1324] arg 1 with type id 4\n",
      "[method.cpp:1324] arg 2 with type id 1\n",
      "[method.cpp:1324] arg 3 with type id 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "method->execute() failed with error 0x12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m program = runtime.load_program(\u001b[33m\"\u001b[39m\u001b[33mvit_model.pte\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m method = program.load_method(\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output: List[torch.Tensor] = \u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRun successfully via executorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmobilenetv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MobileNet_V2_Weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/executorch/runtime/__init__.py:79\u001b[39m, in \u001b[36mMethod.execute\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Sequence[Any]) -> Sequence[Any]:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes the method with the given inputs.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m        The outputs of the method.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: method->execute() failed with error 0x12"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from executorch.runtime import Runtime\n",
    "from typing import List\n",
    "\n",
    "runtime = Runtime.get()\n",
    "\n",
    "input_tensor: torch.Tensor = torch.randn(1, 3, 224, 224)\n",
    "program = runtime.load_program(\"vit_model.pte\")\n",
    "method = program.load_method(\"forward\")\n",
    "output: List[torch.Tensor] = method.execute([input_tensor])\n",
    "print(\"Run successfully via executorch\")\n",
    "\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "import torchvision.models as models\n",
    "\n",
    "eager_reference_model = model.eval()\n",
    "eager_reference_output = eager_reference_model(input_tensor)\n",
    "\n",
    "print(\"Comparing against original PyTorch module\")\n",
    "print(torch.allclose(output[0], eager_reference_output, rtol=1e-3, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332b49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cucumber-Disease-Prediction (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
