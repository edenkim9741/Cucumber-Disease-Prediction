{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52bfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/executorch/exir/dialects/edge/_ops.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading the fine-tuned model structure and weights...\n",
      "--> Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/eden/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import time\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. ëª¨ë¸ êµ¬ì¡° ì •ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "# -----------------------------------\n",
    "print(\"1. Loading the fine-tuned model structure and weights...\")\n",
    "\n",
    "# --- ì„¤ì •ê°’  ---\n",
    "MODEL_REPO = \"facebookresearch/dinov2\"\n",
    "MODEL_NAME = \"dinov2_vits14\"\n",
    "# í´ëž˜ìŠ¤ ìˆ˜ì™€ ìˆœì„œë¥¼ ì •í™•ížˆ ë§žì¶”ê¸°\n",
    "NUM_CLASSES = 3 \n",
    "CLASS_NAMES = ['downy','healthy', 'powdery']\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"--> Using device: {DEVICE}\")\n",
    "# --- ëª¨ë¸ êµ¬ì¡° ë§Œë“¤ê¸°  ---\n",
    "try:\n",
    "    # torch.hubë¥¼ ì´ìš©í•´ DINOv2 ëª¨ë¸ êµ¬ì¡° ë¡œë“œ\n",
    "    model = torch.hub.load(MODEL_REPO, MODEL_NAME, pretrained=False) # pretrained=Falseë¡œ ì„¤ì •\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸°(ë¨¸ë¦¬)\n",
    "    num_features = 384 # ViT-Smallì˜ íŠ¹ì§• ë²¡í„° í¬ê¸°\n",
    "    model.head = nn.Linear(num_features, NUM_CLASSES)\n",
    "    \n",
    "    # --- ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° ---\n",
    "    model.load_state_dict(torch.load('/Cucumber-Disease-Prediction/AI/dinov2_hub_finetuned_model.pth', map_location=DEVICE))\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval() # í‰ê°€ ëª¨ë“œ\n",
    "\n",
    "    # ëª¨ë¸ì„ contiguous í¬ë§·ìœ¼ë¡œ ê°•ì œ ë³€í™˜\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "    for buffer_name, buffer in model.named_buffers():\n",
    "        model.register_buffer(buffer_name, buffer.contiguous())\n",
    "\n",
    "    model = model.to(memory_format=torch.contiguous_format)\n",
    "\n",
    "    print(\"--> Model loaded successfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to load the model.\")\n",
    "    print(f\"--> Ensure 'dinov2_hub_finetuned_model.pth' is in the same directory.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.is_contiguous(memory_format=torch.channels_last):\n",
    "        print(f\"âš ï¸ Parameter {name} uses channels_last\")\n",
    "\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224).contiguous(), )\n",
    "\n",
    "edge_ir = torch.export.export(model, sample_inputs)\n",
    "\n",
    "\n",
    "# export í›„, executorch ë³€í™˜ ì „ì—\n",
    "for node in edge_ir.graph.nodes:\n",
    "    if hasattr(node, 'meta') and 'memory_format' in node.meta:\n",
    "        if node.meta['memory_format'] == torch.channels_last:\n",
    "            print(f\"âš ï¸ Node {node} uses channels_last\")\n",
    "            node.meta['memory_format'] = torch.contiguous_format\n",
    "\n",
    "et_program = to_edge_transform_and_lower(\n",
    "    edge_ir,\n",
    "    partitioner=[XnnpackPartitioner()]\n",
    ").to_executorch()\n",
    "\n",
    "with open(\"vit_model.pte\", \"wb\") as f:\n",
    "    f.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2883cfb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'executorch' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexecutorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexecutorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'executorch' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import executorch\n",
    "print(executorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bff111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Converting model to TorchScript...\n",
      "--> Tracing successful.\n",
      "--> Mobile optimization successful.\n",
      "\n",
      "SUCCESS! Model saved as 'dinov2_mobile.ptl'\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "# -----------------------------------\n",
    "# 2. ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜ (Tracing)\n",
    "# -----------------------------------\n",
    "print(\"\\n2. Converting model to TorchScript...\")\n",
    "\n",
    "# DINOv2 (ViT-S)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 224x224 ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "# (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 3, ë†’ì´ 224, ë„ˆë¹„ 224)\n",
    "# ë§Œì•½ ë‹¤ë¥¸ í¬ê¸°ë¡œ fine-tuning í•˜ì…¨ë‹¤ë©´ ì´ ê°’ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "try:\n",
    "    # 1. Tracing: ëª¨ë¸ì— ë”ë¯¸ ìž…ë ¥ì„ í†µê³¼ì‹œì¼œ ì—°ì‚° ê·¸ëž˜í”„ë¥¼ ê¸°ë¡\n",
    "    traced_script_module = torch.jit.trace(model, dummy_input)\n",
    "    print(\"--> Tracing successful.\")\n",
    "    \n",
    "    # 2. Optimize for Mobile: ëª¨ë°”ì¼ í™˜ê²½ì— ë§žê²Œ ëª¨ë¸ ìµœì í™”\n",
    "    optimized_lit_module = optimize_for_mobile(traced_script_module)\n",
    "    print(\"--> Mobile optimization successful.\")\n",
    "\n",
    "    # 3. Save: ìµœì í™”ëœ .ptl íŒŒì¼ë¡œ ì €ìž¥\n",
    "    output_filename = \"dinov2_mobile.ptl\"\n",
    "    optimized_lit_module.save(output_filename)\n",
    "    \n",
    "    print(f\"\\nSUCCESS! Model saved as '{output_filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to convert the model.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53f66ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models, datasets # datasets ì¶”ê°€\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "class_names = ['downy', 'healthy', 'powdery']\n",
    "\n",
    "# ==============================================================\n",
    "# 2. ëª¨ë¸ ì •ì˜\n",
    "# ==============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\n",
    "model = model.eval().to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/AI/MobileNet/best_mobilenet_cucumber.pth', weights_only=False, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bab6511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.8626,  5.7014, -2.9983]])\n"
     ]
    }
   ],
   "source": [
    "# image.pngë¥¼ ìž…ë ¥í•´ë³´ëŠ” ì½”ë“œ\n",
    "\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ì´ë¯¸ì§€ ì—´ê¸°\n",
    "img = Image.open(\"image.png\").convert(\"RGB\")\n",
    "img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "random_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    output = model(random_input)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Converting model to TorchScript...\n",
      "3. Converting model to TorchScript Lite (.ptl)...\n",
      "--> Attempting torch.jit.script(model)...\n",
      "--> Scripting successful.\n",
      "--> Mobile optimization successful.\n",
      "\n",
      "SUCCESS! Model saved as 'mobilenetCucumberMobile.ptl'\n",
      "--> This file is ready for your Kotlin (Android) app.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "# -----------------------------------\n",
    "# 2. ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜ (Tracing)\n",
    "# -----------------------------------\n",
    "print(\"\\n2. Converting model to TorchScript...\")\n",
    "\n",
    "# DINOv2 (ViT-S)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 224x224 ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "# (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 3, ë†’ì´ 224, ë„ˆë¹„ 224)\n",
    "# ë§Œì•½ ë‹¤ë¥¸ í¬ê¸°ë¡œ fine-tuning í•˜ì…¨ë‹¤ë©´ ì´ ê°’ì„ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "model.eval()\n",
    "\n",
    "OUTPUT_MODEL_NAME = \"mobilenetCucumberMobile.ptl\"\n",
    "NO_OPT_MODEL_NAME = \"mobilenetCucumberNoOpt.ptl\"\n",
    "\n",
    "# --- 4. TorchScriptë¡œ ë³€í™˜ ë° ëª¨ë°”ì¼ ìµœì í™” ---\n",
    "print(f\"3. Converting model to TorchScript Lite (.ptl)...\")\n",
    "\n",
    "# âš ï¸ 'torch.jit.trace' ëŒ€ì‹  'torch.jit.script'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# 'script' ë°©ì‹ì€ ë”ë¯¸ ìž…ë ¥(dummy_input)ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n",
    "try:\n",
    "    # 1. Scripting: ëª¨ë¸ì˜ ì½”ë“œë¥¼ ì§ì ‘ ë¶„ì„í•˜ì—¬ ë³€í™˜\n",
    "    print(\"--> Attempting torch.jit.script(model)...\")\n",
    "    scripted_module = torch.jit.script(model) # ðŸŒŸ ì´ ë¶€ë¶„ì´ ë³€ê²½ë¨\n",
    "    print(\"--> Scripting successful.\")\n",
    "\n",
    "    scripted_module.save(NO_OPT_MODEL_NAME)\n",
    "    \n",
    "    # 2. Optimize for Mobile: ëª¨ë°”ì¼ í™˜ê²½ì— ë§žê²Œ ëª¨ë¸ ìµœì í™”\n",
    "    optimized_lit_module = optimize_for_mobile(scripted_module) # ðŸŒŸ ìž…ë ¥ ë³€ìˆ˜ ë³€ê²½\n",
    "    print(\"--> Mobile optimization successful.\")\n",
    "\n",
    "    # 3. Save: ìµœì í™”ëœ .ptl íŒŒì¼ë¡œ ì €ìž¥\n",
    "    optimized_lit_module.save(OUTPUT_MODEL_NAME)\n",
    "    \n",
    "    print(f\"\\nSUCCESS! Model saved as '{OUTPUT_MODEL_NAME}'\")\n",
    "    print(\"--> This file is ready for your Kotlin (Android) app.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to convert the model with 'torch.jit.script'.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a016307",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed with error: Tensor has a memory_format that is unsupported in ExecuTorch: torch.channels_last\nHere is the node in the graph module:\ngraph():\n    %p_cls_token : [num_users=1] = placeholder[target=p_cls_token]\n    %p_pos_embed : [num_users=2] = placeholder[target=p_pos_embed]\n    %p_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_blocks_0_norm1_weight]\n    %p_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_blocks_0_norm1_bias]\n    %p_blocks_0_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_0_ls1_gamma]\n    %p_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_blocks_0_norm2_weight]\n    %p_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_blocks_0_norm2_bias]\n    %p_blocks_0_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_0_ls2_gamma]\n    %p_blocks_1_norm1_weight : [num_users=1] = placeholder[target=p_blocks_1_norm1_weight]\n    %p_blocks_1_norm1_bias : [num_users=1] = placeholder[target=p_blocks_1_norm1_bias]\n    %p_blocks_1_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_1_ls1_gamma]\n    %p_blocks_1_norm2_weight : [num_users=1] = placeholder[target=p_blocks_1_norm2_weight]\n    %p_blocks_1_norm2_bias : [num_users=1] = placeholder[target=p_blocks_1_norm2_bias]\n    %p_blocks_1_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_1_ls2_gamma]\n    %p_blocks_2_norm1_weight : [num_users=1] = placeholder[target=p_blocks_2_norm1_weight]\n    %p_blocks_2_norm1_bias : [num_users=1] = placeholder[target=p_blocks_2_norm1_bias]\n    %p_blocks_2_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_2_ls1_gamma]\n    %p_blocks_2_norm2_weight : [num_users=1] = placeholder[target=p_blocks_2_norm2_weight]\n    %p_blocks_2_norm2_bias : [num_users=1] = placeholder[target=p_blocks_2_norm2_bias]\n    %p_blocks_2_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_2_ls2_gamma]\n    %p_blocks_3_norm1_weight : [num_users=1] = placeholder[target=p_blocks_3_norm1_weight]\n    %p_blocks_3_norm1_bias : [num_users=1] = placeholder[target=p_blocks_3_norm1_bias]\n    %p_blocks_3_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_3_ls1_gamma]\n    %p_blocks_3_norm2_weight : [num_users=1] = placeholder[target=p_blocks_3_norm2_weight]\n    %p_blocks_3_norm2_bias : [num_users=1] = placeholder[target=p_blocks_3_norm2_bias]\n    %p_blocks_3_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_3_ls2_gamma]\n    %p_blocks_4_norm1_weight : [num_users=1] = placeholder[target=p_blocks_4_norm1_weight]\n    %p_blocks_4_norm1_bias : [num_users=1] = placeholder[target=p_blocks_4_norm1_bias]\n    %p_blocks_4_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_4_ls1_gamma]\n    %p_blocks_4_norm2_weight : [num_users=1] = placeholder[target=p_blocks_4_norm2_weight]\n    %p_blocks_4_norm2_bias : [num_users=1] = placeholder[target=p_blocks_4_norm2_bias]\n    %p_blocks_4_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_4_ls2_gamma]\n    %p_blocks_5_norm1_weight : [num_users=1] = placeholder[target=p_blocks_5_norm1_weight]\n    %p_blocks_5_norm1_bias : [num_users=1] = placeholder[target=p_blocks_5_norm1_bias]\n    %p_blocks_5_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_5_ls1_gamma]\n    %p_blocks_5_norm2_weight : [num_users=1] = placeholder[target=p_blocks_5_norm2_weight]\n    %p_blocks_5_norm2_bias : [num_users=1] = placeholder[target=p_blocks_5_norm2_bias]\n    %p_blocks_5_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_5_ls2_gamma]\n    %p_blocks_6_norm1_weight : [num_users=1] = placeholder[target=p_blocks_6_norm1_weight]\n    %p_blocks_6_norm1_bias : [num_users=1] = placeholder[target=p_blocks_6_norm1_bias]\n    %p_blocks_6_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_6_ls1_gamma]\n    %p_blocks_6_norm2_weight : [num_users=1] = placeholder[target=p_blocks_6_norm2_weight]\n    %p_blocks_6_norm2_bias : [num_users=1] = placeholder[target=p_blocks_6_norm2_bias]\n    %p_blocks_6_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_6_ls2_gamma]\n    %p_blocks_7_norm1_weight : [num_users=1] = placeholder[target=p_blocks_7_norm1_weight]\n    %p_blocks_7_norm1_bias : [num_users=1] = placeholder[target=p_blocks_7_norm1_bias]\n    %p_blocks_7_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_7_ls1_gamma]\n    %p_blocks_7_norm2_weight : [num_users=1] = placeholder[target=p_blocks_7_norm2_weight]\n    %p_blocks_7_norm2_bias : [num_users=1] = placeholder[target=p_blocks_7_norm2_bias]\n    %p_blocks_7_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_7_ls2_gamma]\n    %p_blocks_8_norm1_weight : [num_users=1] = placeholder[target=p_blocks_8_norm1_weight]\n    %p_blocks_8_norm1_bias : [num_users=1] = placeholder[target=p_blocks_8_norm1_bias]\n    %p_blocks_8_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_8_ls1_gamma]\n    %p_blocks_8_norm2_weight : [num_users=1] = placeholder[target=p_blocks_8_norm2_weight]\n    %p_blocks_8_norm2_bias : [num_users=1] = placeholder[target=p_blocks_8_norm2_bias]\n    %p_blocks_8_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_8_ls2_gamma]\n    %p_blocks_9_norm1_weight : [num_users=1] = placeholder[target=p_blocks_9_norm1_weight]\n    %p_blocks_9_norm1_bias : [num_users=1] = placeholder[target=p_blocks_9_norm1_bias]\n    %p_blocks_9_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_9_ls1_gamma]\n    %p_blocks_9_norm2_weight : [num_users=1] = placeholder[target=p_blocks_9_norm2_weight]\n    %p_blocks_9_norm2_bias : [num_users=1] = placeholder[target=p_blocks_9_norm2_bias]\n    %p_blocks_9_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_9_ls2_gamma]\n    %p_blocks_10_norm1_weight : [num_users=1] = placeholder[target=p_blocks_10_norm1_weight]\n    %p_blocks_10_norm1_bias : [num_users=1] = placeholder[target=p_blocks_10_norm1_bias]\n    %p_blocks_10_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_10_ls1_gamma]\n    %p_blocks_10_norm2_weight : [num_users=1] = placeholder[target=p_blocks_10_norm2_weight]\n    %p_blocks_10_norm2_bias : [num_users=1] = placeholder[target=p_blocks_10_norm2_bias]\n    %p_blocks_10_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_10_ls2_gamma]\n    %p_blocks_11_norm1_weight : [num_users=1] = placeholder[target=p_blocks_11_norm1_weight]\n    %p_blocks_11_norm1_bias : [num_users=1] = placeholder[target=p_blocks_11_norm1_bias]\n    %p_blocks_11_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_11_ls1_gamma]\n    %p_blocks_11_norm2_weight : [num_users=1] = placeholder[target=p_blocks_11_norm2_weight]\n    %p_blocks_11_norm2_bias : [num_users=1] = placeholder[target=p_blocks_11_norm2_bias]\n    %p_blocks_11_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_11_ls2_gamma]\n    %p_norm_weight : [num_users=1] = placeholder[target=p_norm_weight]\n    %p_norm_bias : [num_users=1] = placeholder[target=p_norm_bias]\n    %_lifted_tensor_constant0 : [num_users=1] = placeholder[target=_lifted_tensor_constant0]\n    %_lifted_tensor_constant1 : [num_users=1] = placeholder[target=_lifted_tensor_constant1]\n    %_lifted_tensor_constant2 : [num_users=1] = placeholder[target=_lifted_tensor_constant2]\n    %_lifted_tensor_constant3 : [num_users=1] = placeholder[target=_lifted_tensor_constant3]\n    %_lifted_tensor_constant4 : [num_users=1] = placeholder[target=_lifted_tensor_constant4]\n    %_lifted_tensor_constant5 : [num_users=1] = placeholder[target=_lifted_tensor_constant5]\n    %_lifted_tensor_constant6 : [num_users=1] = placeholder[target=_lifted_tensor_constant6]\n    %_lifted_tensor_constant7 : [num_users=1] = placeholder[target=_lifted_tensor_constant7]\n    %_lifted_tensor_constant8 : [num_users=1] = placeholder[target=_lifted_tensor_constant8]\n    %_lifted_tensor_constant9 : [num_users=1] = placeholder[target=_lifted_tensor_constant9]\n    %_lifted_tensor_constant10 : [num_users=1] = placeholder[target=_lifted_tensor_constant10]\n    %_lifted_tensor_constant11 : [num_users=1] = placeholder[target=_lifted_tensor_constant11]\n    %_lifted_tensor_constant12 : [num_users=1] = placeholder[target=_lifted_tensor_constant12]\n    %_lifted_tensor_constant13 : [num_users=1] = placeholder[target=_lifted_tensor_constant13]\n    %_lifted_tensor_constant14 : [num_users=1] = placeholder[target=_lifted_tensor_constant14]\n    %_lifted_tensor_constant15 : [num_users=1] = placeholder[target=_lifted_tensor_constant15]\n    %_lifted_tensor_constant16 : [num_users=1] = placeholder[target=_lifted_tensor_constant16]\n    %_lifted_tensor_constant17 : [num_users=1] = placeholder[target=_lifted_tensor_constant17]\n    %_lifted_tensor_constant18 : [num_users=1] = placeholder[target=_lifted_tensor_constant18]\n    %_lifted_tensor_constant19 : [num_users=1] = placeholder[target=_lifted_tensor_constant19]\n    %_lifted_tensor_constant20 : [num_users=1] = placeholder[target=_lifted_tensor_constant20]\n    %_lifted_tensor_constant21 : [num_users=1] = placeholder[target=_lifted_tensor_constant21]\n    %_lifted_tensor_constant22 : [num_users=1] = placeholder[target=_lifted_tensor_constant22]\n    %_lifted_tensor_constant23 : [num_users=1] = placeholder[target=_lifted_tensor_constant23]\n    %_lifted_tensor_constant24 : [num_users=1] = placeholder[target=_lifted_tensor_constant24]\n    %_lifted_tensor_constant25 : [num_users=1] = placeholder[target=_lifted_tensor_constant25]\n    %_lifted_tensor_constant26 : [num_users=1] = placeholder[target=_lifted_tensor_constant26]\n    %_lifted_tensor_constant27 : [num_users=1] = placeholder[target=_lifted_tensor_constant27]\n    %_lifted_tensor_constant28 : [num_users=1] = placeholder[target=_lifted_tensor_constant28]\n    %_lifted_tensor_constant29 : [num_users=1] = placeholder[target=_lifted_tensor_constant29]\n    %_lifted_tensor_constant30 : [num_users=1] = placeholder[target=_lifted_tensor_constant30]\n    %_lifted_tensor_constant31 : [num_users=1] = placeholder[target=_lifted_tensor_constant31]\n    %_lifted_tensor_constant32 : [num_users=1] = placeholder[target=_lifted_tensor_constant32]\n    %_lifted_tensor_constant33 : [num_users=1] = placeholder[target=_lifted_tensor_constant33]\n    %_lifted_tensor_constant34 : [num_users=1] = placeholder[target=_lifted_tensor_constant34]\n    %_lifted_tensor_constant35 : [num_users=1] = placeholder[target=_lifted_tensor_constant35]\n    %_lifted_tensor_constant36 : [num_users=1] = placeholder[target=_lifted_tensor_constant36]\n    %_lifted_tensor_constant37 : [num_users=1] = placeholder[target=_lifted_tensor_constant37]\n    %_lifted_tensor_constant38 : [num_users=1] = placeholder[target=_lifted_tensor_constant38]\n    %_lifted_tensor_constant39 : [num_users=1] = placeholder[target=_lifted_tensor_constant39]\n    %_lifted_tensor_constant40 : [num_users=1] = placeholder[target=_lifted_tensor_constant40]\n    %_lifted_tensor_constant41 : [num_users=1] = placeholder[target=_lifted_tensor_constant41]\n    %_lifted_tensor_constant42 : [num_users=1] = placeholder[target=_lifted_tensor_constant42]\n    %_lifted_tensor_constant43 : [num_users=1] = placeholder[target=_lifted_tensor_constant43]\n    %args_0 : [num_users=1] = placeholder[target=args_0]\n    %alloc : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_arange_start_step : [num_users=1] = call_function[target=torch.ops.aten.arange.start_out](args = (0, 16), kwargs = {out: %alloc})\n    %alloc_1 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_arange_start_step_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_out](args = (0, 16), kwargs = {out: %alloc_1})\n    %alloc_2 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 384), torch.float32),), kwargs = {})\n    %aten_expand_copy_default : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%p_cls_token, [1, -1, -1]), kwargs = {out: %alloc_2})\n    %alloc_3 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384), torch.float32),), kwargs = {})\n    %aten_select_copy_int : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%p_pos_embed, 1, 0), kwargs = {out: %alloc_3})\n    %alloc_4 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%_lifted_tensor_constant21,), kwargs = {dim_order: [], out: %alloc_4})\n    %alloc_5 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_1 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%_lifted_tensor_constant31,), kwargs = {dim_order: [], out: %alloc_5})\n    %alloc_6 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_2 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%aten_arange_start_step,), kwargs = {dim_order: [0], out: %alloc_6})\n    %alloc_7 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_3 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%aten_arange_start_step_1,), kwargs = {dim_order: [0], out: %alloc_7})\n    %alloc_8 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 384), torch.float32),), kwargs = {})\n    %aten_unsqueeze_copy_default : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze_copy.out](args = (%aten_select_copy_int, 0), kwargs = {out: %alloc_8})\n    %lowered_module_0 : [num_users=1] = get_attr[target=lowered_module_0]\n    %executorch_call_delegate : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_0, %dim_order_ops__to_dim_order_copy_default_2, %_lifted_tensor_constant3, %_lifted_tensor_constant4, %_lifted_tensor_constant5), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 0), kwargs = {})\n    %alloc_9 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_unsqueeze_copy_default_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze_copy.out](args = (%getitem, -1), kwargs = {out: %alloc_9})\n    %lowered_module_1 : [num_users=1] = get_attr[target=lowered_module_1]\n    %executorch_call_delegate_1 : [num_users=7] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_1, %p_pos_embed, %dim_order_ops__to_dim_order_copy_default_3, %_lifted_tensor_constant0, %aten_unsqueeze_copy_default_1, %_lifted_tensor_constant1, %_lifted_tensor_constant2, %_lifted_tensor_constant22, %_lifted_tensor_constant23, %_lifted_tensor_constant24, %_lifted_tensor_constant12, %_lifted_tensor_constant13, %_lifted_tensor_constant14), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 0), kwargs = {})\n    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 1), kwargs = {})\n    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 2), kwargs = {})\n    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 3), kwargs = {})\n    %getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 4), kwargs = {})\n    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 5), kwargs = {})\n    %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 6), kwargs = {})\n    %aten_view_copy_default : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_1, [1, 37, 37, 384]), kwargs = {})\n    %alloc_10 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_4 : [num_users=7] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%getitem_2,), kwargs = {dim_order: [0, 1], out: %alloc_10})\n    %alloc_11 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_5 : [num_users=7] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%getitem_3,), kwargs = {dim_order: [0], out: %alloc_11})\n    %aten_view_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_4, [2, 16, 1]), kwargs = {})\n    %aten_view_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_5, [2, 16, 1]), kwargs = {})\n    %aten_view_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_6, [2, 16]), kwargs = {})\n    %aten_view_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_7, [2, 16]), kwargs = {})\n    %alloc_12 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_sub_tensor : [num_users=4] = call_function[target=torch.ops.aten.sub.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant6), kwargs = {out: %alloc_12})\n    %alloc_13 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_add_tensor : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant7), kwargs = {out: %alloc_13})\n    %alloc_14 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_add_tensor_1 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant8), kwargs = {out: %alloc_14})\n    %alloc_15 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_15})\n    %alloc_16 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_16})\n    %alloc_17 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_17})\n    %alloc_18 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_18})\n    %alloc_19 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_sub_tensor_1 : [num_users=4] = call_function[target=torch.ops.aten.sub.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant9), kwargs = {out: %alloc_19})\n    %alloc_20 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_add_tensor_2 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant10), kwargs = {out: %alloc_20})\n    %alloc_21 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_add_tensor_3 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant11), kwargs = {out: %alloc_21})\n    %alloc_22 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_22})\n    %alloc_23 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_5 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_23})\n    %alloc_24 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_6 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_24})\n    %alloc_25 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_7 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_25})\n    %lowered_module_2 : [num_users=1] = get_attr[target=lowered_module_2]\n    %executorch_call_delegate_2 : [num_users=9] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_2, %aten_view_copy_default, %aten_view_copy_default_3, %_lifted_tensor_constant19, %aten_view_copy_default_4, %_lifted_tensor_constant15, %aten_view_copy_default_1, %_lifted_tensor_constant29, %aten_view_copy_default_2, %_lifted_tensor_constant25, %_lifted_tensor_constant20, %_lifted_tensor_constant16, %_lifted_tensor_constant30, %_lifted_tensor_constant26, %_lifted_tensor_constant17, %_lifted_tensor_constant27, %dim_order_ops__to_dim_order_copy_default, %dim_order_ops__to_dim_order_copy_default_1, %_lifted_tensor_constant18, %_lifted_tensor_constant28), kwargs = {})\n    %alloc_26 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_8 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_26})\n    %alloc_27 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_9 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_27})\n    %alloc_28 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_10 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_28})\n    %alloc_29 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_11 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_29})\n    %alloc_30 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_12 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_30})\n    %alloc_31 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_13 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_31})\n    %alloc_32 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_14 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_32})\n    %alloc_33 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_15 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_33})\n    %alloc_34 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_16 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_34})\n    %alloc_35 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_17 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_35})\n    %alloc_36 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_18 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_36})\n    %alloc_37 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_19 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_37})\n    %alloc_38 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_20 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_38})\n    %alloc_39 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_21 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_39})\n    %alloc_40 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_22 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_40})\n    %alloc_41 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_23 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_41})\n    %alloc_42 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_24 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_42})\n    %alloc_43 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_25 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_43})\n    %alloc_44 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_26 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_44})\n    %alloc_45 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_27 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_45})\n    %alloc_46 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_28 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_46})\n    %alloc_47 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_29 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_47})\n    %alloc_48 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_30 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_48})\n    %alloc_49 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_31 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_49})\n    %getitem_8 : [num_users=16] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 0), kwargs = {})\n    %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 1), kwargs = {})\n    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 2), kwargs = {})\n    %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 3), kwargs = {})\n    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 4), kwargs = {})\n    %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 5), kwargs = {})\n    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 6), kwargs = {})\n    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 7), kwargs = {})\n    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 8), kwargs = {})\n    %alloc_50 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_1, %aten_clamp_default_5]), kwargs = {out: %alloc_50})\n    %alloc_51 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_1 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default, %aten_clamp_default_21]), kwargs = {out: %alloc_51})\n    %alloc_52 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_2 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_2, %aten_clamp_default_25]), kwargs = {out: %alloc_52})\n    %alloc_53 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_3 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_3, %aten_clamp_default_29]), kwargs = {out: %alloc_53})\n    %alloc_54 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_4 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_8, %aten_clamp_default_20]), kwargs = {out: %alloc_54})\n    %alloc_55 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_5 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_9, %aten_clamp_default_4]), kwargs = {out: %alloc_55})\n    %alloc_56 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_10, %aten_clamp_default_24]), kwargs = {out: %alloc_56})\n    %alloc_57 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_7 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_11, %aten_clamp_default_28]), kwargs = {out: %alloc_57})\n    %alloc_58 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_8 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_12, %aten_clamp_default_22]), kwargs = {out: %alloc_58})\n    %alloc_59 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_9 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_13, %aten_clamp_default_6]), kwargs = {out: %alloc_59})\n    %alloc_60 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_10 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_14, %aten_clamp_default_26]), kwargs = {out: %alloc_60})\n    %alloc_61 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_11 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_15, %aten_clamp_default_30]), kwargs = {out: %alloc_61})\n    %alloc_62 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_12 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_16, %aten_clamp_default_23]), kwargs = {out: %alloc_62})\n    %alloc_63 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_13 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_17, %aten_clamp_default_7]), kwargs = {out: %alloc_63})\n    %alloc_64 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_14 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_18, %aten_clamp_default_27]), kwargs = {out: %alloc_64})\n    %alloc_65 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_15 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_19, %aten_clamp_default_31]), kwargs = {out: %alloc_65})\n    %alloc_66 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_9, [0]), kwargs = {out: %alloc_66})\n    %alloc_67 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_10, [0]), kwargs = {out: %alloc_67})\n    %alloc_68 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_11, [0]), kwargs = {out: %alloc_68})\n    %alloc_69 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_12, [0]), kwargs = {out: %alloc_69})\n    %alloc_70 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_4 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_13, [0]), kwargs = {out: %alloc_70})\n    %alloc_71 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_5 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_14, [0]), kwargs = {out: %alloc_71})\n    %alloc_72 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_6 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_15, [0]), kwargs = {out: %alloc_72})\n    %alloc_73 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_7 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_16, [0]), kwargs = {out: %alloc_73})\n    %lowered_module_3 : [num_users=1] = get_attr[target=lowered_module_3]\n    %executorch_call_delegate_3 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_3, %aten_index_tensor_5, %aten_squeeze_copy_dims, %aten_index_tensor, %aten_index_tensor_9, %aten_index_tensor_13, %aten_index_tensor_6, %aten_squeeze_copy_dims_1, %aten_index_tensor_2, %aten_index_tensor_10, %aten_index_tensor_14, %aten_index_tensor_4, %aten_squeeze_copy_dims_4, %aten_index_tensor_1, %aten_index_tensor_8, %aten_index_tensor_12, %aten_index_tensor_7, %aten_squeeze_copy_dims_5, %aten_index_tensor_3, %aten_index_tensor_11, %aten_index_tensor_15, %aten_squeeze_copy_dims_6, %aten_squeeze_copy_dims_2, %aten_squeeze_copy_dims_3, %aten_squeeze_copy_dims_7), kwargs = {})\n    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 0), kwargs = {})\n    %alloc_74 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n--> %aten_clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_17,), kwargs = {memory_format: torch.channels_last, out: %alloc_74})\n    %lowered_module_4 : [num_users=1] = get_attr[target=lowered_module_4]\n    %executorch_call_delegate_4 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_4, %aten_clone_default, %args_0), kwargs = {})\n    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 0), kwargs = {})\n    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 1), kwargs = {})\n    %aten_view_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_18, [1, -1, 384]), kwargs = {})\n    %aten_view_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_19, [1, 384, 256]), kwargs = {})\n    %lowered_module_5 : [num_users=1] = get_attr[target=lowered_module_5]\n    %executorch_call_delegate_5 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_5, %aten_view_copy_default_6, %aten_unsqueeze_copy_default, %aten_view_copy_default_5, %aten_expand_copy_default), kwargs = {})\n    %getitem_20 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 0), kwargs = {})\n    %alloc_75 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_76 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_77 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_20, [384], %p_blocks_0_norm1_weight, %p_blocks_0_norm1_bias, 1e-06), kwargs = {out0: %alloc_75, out1: %alloc_76, out2: %alloc_77})\n    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default, 0), kwargs = {})\n    %lowered_module_6 : [num_users=1] = get_attr[target=lowered_module_6]\n    %executorch_call_delegate_6 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_6, %getitem_21), kwargs = {})\n    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 0), kwargs = {})\n    %aten_view_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_22, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_7 : [num_users=1] = get_attr[target=lowered_module_7]\n    %executorch_call_delegate_7 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_7, %aten_view_copy_default_7), kwargs = {})\n    %getitem_23 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_7, 0), kwargs = {})\n    %alloc_78 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_1 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 0), kwargs = {out: %alloc_78})\n    %alloc_79 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_2 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 1), kwargs = {out: %alloc_79})\n    %alloc_80 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_3 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 2), kwargs = {out: %alloc_80})\n    %lowered_module_8 : [num_users=1] = get_attr[target=lowered_module_8]\n    %executorch_call_delegate_8 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_8, %aten_select_copy_int_1, %_lifted_tensor_constant32, %aten_select_copy_int_2), kwargs = {})\n    %alloc_81 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_1 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_3, [1, 6, 257, 64]), kwargs = {out: %alloc_81})\n    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 0), kwargs = {})\n    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 1), kwargs = {})\n    %aten_view_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_1, [6, 257, 64]), kwargs = {})\n    %alloc_82 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_2 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_24, [1, 6, 257, 64]), kwargs = {out: %alloc_82})\n    %alloc_83 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_3 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_25, [1, 6, 64, 257]), kwargs = {out: %alloc_83})\n    %aten_view_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_2, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_3, [6, 64, 257]), kwargs = {})\n    %lowered_module_9 : [num_users=1] = get_attr[target=lowered_module_9]\n    %executorch_call_delegate_9 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_9, %aten_view_copy_default_9, %aten_view_copy_default_10), kwargs = {})\n    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 0), kwargs = {})\n    %aten_view_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_26, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_10 : [num_users=1] = get_attr[target=lowered_module_10]\n    %executorch_call_delegate_10 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_10, %aten_view_copy_default_11), kwargs = {})\n    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 0), kwargs = {})\n    %alloc_84 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_27,), kwargs = {out: %alloc_84})\n    %alloc_85 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_4 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_1, [1, 6, 257, 257]), kwargs = {out: %alloc_85})\n    %aten_view_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_4, [6, 257, 257]), kwargs = {})\n    %lowered_module_11 : [num_users=1] = get_attr[target=lowered_module_11]\n    %executorch_call_delegate_11 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_11, %aten_view_copy_default_12, %aten_view_copy_default_8), kwargs = {})\n    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_11, 0), kwargs = {})\n    %aten_view_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_28, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_12 : [num_users=1] = get_attr[target=lowered_module_12]\n    %executorch_call_delegate_12 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_12, %aten_view_copy_default_13), kwargs = {})\n    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 0), kwargs = {})\n    %alloc_86 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_29,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_86})\n    %aten_view_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_2, [1, 257, 384]), kwargs = {})\n    %lowered_module_13 : [num_users=1] = get_attr[target=lowered_module_13]\n    %executorch_call_delegate_13 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_13, %aten_view_copy_default_14), kwargs = {})\n    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 0), kwargs = {})\n    %alloc_87 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_30,), kwargs = {out: %alloc_87})\n    %lowered_module_14 : [num_users=1] = get_attr[target=lowered_module_14]\n    %executorch_call_delegate_14 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_14, %aten_clone_default_3, %p_blocks_0_ls1_gamma, %getitem_20), kwargs = {})\n    %getitem_31 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 0), kwargs = {})\n    %alloc_88 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_89 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_90 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_1 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_31, [384], %p_blocks_0_norm2_weight, %p_blocks_0_norm2_bias, 1e-06), kwargs = {out0: %alloc_88, out1: %alloc_89, out2: %alloc_90})\n    %getitem_32 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_1, 0), kwargs = {})\n    %lowered_module_15 : [num_users=1] = get_attr[target=lowered_module_15]\n    %executorch_call_delegate_15 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_15, %getitem_32), kwargs = {})\n    %getitem_33 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_15, 0), kwargs = {})\n    %alloc_91 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_33,), kwargs = {out: %alloc_91})\n    %lowered_module_16 : [num_users=1] = get_attr[target=lowered_module_16]\n    %executorch_call_delegate_16 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_16, %aten_clone_default_4), kwargs = {})\n    %getitem_34 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 0), kwargs = {})\n    %alloc_92 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_5 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_34,), kwargs = {out: %alloc_92})\n    %lowered_module_17 : [num_users=1] = get_attr[target=lowered_module_17]\n    %executorch_call_delegate_17 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_17, %aten_clone_default_5, %p_blocks_0_ls2_gamma, %getitem_31), kwargs = {})\n    %getitem_35 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 0), kwargs = {})\n    %alloc_93 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_94 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_95 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_2 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_35, [384], %p_blocks_1_norm1_weight, %p_blocks_1_norm1_bias, 1e-06), kwargs = {out0: %alloc_93, out1: %alloc_94, out2: %alloc_95})\n    %getitem_36 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_2, 0), kwargs = {})\n    %lowered_module_18 : [num_users=1] = get_attr[target=lowered_module_18]\n    %executorch_call_delegate_18 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_18, %getitem_36), kwargs = {})\n    %getitem_37 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 0), kwargs = {})\n    %aten_view_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_37, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_19 : [num_users=1] = get_attr[target=lowered_module_19]\n    %executorch_call_delegate_19 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_19, %aten_view_copy_default_15), kwargs = {})\n    %getitem_38 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_19, 0), kwargs = {})\n    %alloc_96 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_4 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 0), kwargs = {out: %alloc_96})\n    %alloc_97 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_5 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 1), kwargs = {out: %alloc_97})\n    %alloc_98 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_6 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 2), kwargs = {out: %alloc_98})\n    %lowered_module_20 : [num_users=1] = get_attr[target=lowered_module_20]\n    %executorch_call_delegate_20 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_20, %aten_select_copy_int_4, %_lifted_tensor_constant33, %aten_select_copy_int_5), kwargs = {})\n    %alloc_99 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_5 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_6, [1, 6, 257, 64]), kwargs = {out: %alloc_99})\n    %getitem_39 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 0), kwargs = {})\n    %getitem_40 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 1), kwargs = {})\n    %aten_view_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_5, [6, 257, 64]), kwargs = {})\n    %alloc_100 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_6 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_39, [1, 6, 257, 64]), kwargs = {out: %alloc_100})\n    %alloc_101 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_7 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_40, [1, 6, 64, 257]), kwargs = {out: %alloc_101})\n    %aten_view_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_6, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_7, [6, 64, 257]), kwargs = {})\n    %lowered_module_21 : [num_users=1] = get_attr[target=lowered_module_21]\n    %executorch_call_delegate_21 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_21, %aten_view_copy_default_17, %aten_view_copy_default_18), kwargs = {})\n    %getitem_41 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 0), kwargs = {})\n    %aten_view_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_41, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_22 : [num_users=1] = get_attr[target=lowered_module_22]\n    %executorch_call_delegate_22 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_22, %aten_view_copy_default_19), kwargs = {})\n    %getitem_42 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 0), kwargs = {})\n    %alloc_102 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_6 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_42,), kwargs = {out: %alloc_102})\n    %alloc_103 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_8 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_6, [1, 6, 257, 257]), kwargs = {out: %alloc_103})\n    %aten_view_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_8, [6, 257, 257]), kwargs = {})\n    %lowered_module_23 : [num_users=1] = get_attr[target=lowered_module_23]\n    %executorch_call_delegate_23 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_23, %aten_view_copy_default_20, %aten_view_copy_default_16), kwargs = {})\n    %getitem_43 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_23, 0), kwargs = {})\n    %aten_view_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_43, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_24 : [num_users=1] = get_attr[target=lowered_module_24]\n    %executorch_call_delegate_24 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_24, %aten_view_copy_default_21), kwargs = {})\n    %getitem_44 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 0), kwargs = {})\n    %alloc_104 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_44,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_104})\n    %aten_view_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_7, [1, 257, 384]), kwargs = {})\n    %lowered_module_25 : [num_users=1] = get_attr[target=lowered_module_25]\n    %executorch_call_delegate_25 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_25, %aten_view_copy_default_22), kwargs = {})\n    %getitem_45 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_25, 0), kwargs = {})\n    %alloc_105 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_8 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_45,), kwargs = {out: %alloc_105})\n    %lowered_module_26 : [num_users=1] = get_attr[target=lowered_module_26]\n    %executorch_call_delegate_26 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_26, %aten_clone_default_8, %p_blocks_1_ls1_gamma, %getitem_35), kwargs = {})\n    %getitem_46 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_26, 0), kwargs = {})\n    %alloc_106 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_107 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_108 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_3 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_46, [384], %p_blocks_1_norm2_weight, %p_blocks_1_norm2_bias, 1e-06), kwargs = {out0: %alloc_106, out1: %alloc_107, out2: %alloc_108})\n    %getitem_47 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_3, 0), kwargs = {})\n    %lowered_module_27 : [num_users=1] = get_attr[target=lowered_module_27]\n    %executorch_call_delegate_27 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_27, %getitem_47), kwargs = {})\n    %getitem_48 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_27, 0), kwargs = {})\n    %alloc_109 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_9 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_48,), kwargs = {out: %alloc_109})\n    %lowered_module_28 : [num_users=1] = get_attr[target=lowered_module_28]\n    %executorch_call_delegate_28 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_28, %aten_clone_default_9), kwargs = {})\n    %getitem_49 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 0), kwargs = {})\n    %alloc_110 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_49,), kwargs = {out: %alloc_110})\n    %lowered_module_29 : [num_users=1] = get_attr[target=lowered_module_29]\n    %executorch_call_delegate_29 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_29, %aten_clone_default_10, %p_blocks_1_ls2_gamma, %getitem_46), kwargs = {})\n    %getitem_50 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_29, 0), kwargs = {})\n    %alloc_111 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_112 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_113 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_4 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_50, [384], %p_blocks_2_norm1_weight, %p_blocks_2_norm1_bias, 1e-06), kwargs = {out0: %alloc_111, out1: %alloc_112, out2: %alloc_113})\n    %getitem_51 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_4, 0), kwargs = {})\n    %lowered_module_30 : [num_users=1] = get_attr[target=lowered_module_30]\n    %executorch_call_delegate_30 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_30, %getitem_51), kwargs = {})\n    %getitem_52 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_30, 0), kwargs = {})\n    %aten_view_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_52, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_31 : [num_users=1] = get_attr[target=lowered_module_31]\n    %executorch_call_delegate_31 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_31, %aten_view_copy_default_23), kwargs = {})\n    %getitem_53 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_31, 0), kwargs = {})\n    %alloc_114 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_7 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 0), kwargs = {out: %alloc_114})\n    %alloc_115 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_8 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 1), kwargs = {out: %alloc_115})\n    %alloc_116 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_9 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 2), kwargs = {out: %alloc_116})\n    %lowered_module_32 : [num_users=1] = get_attr[target=lowered_module_32]\n    %executorch_call_delegate_32 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_32, %aten_select_copy_int_7, %_lifted_tensor_constant34, %aten_select_copy_int_8), kwargs = {})\n    %alloc_117 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_9 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_9, [1, 6, 257, 64]), kwargs = {out: %alloc_117})\n    %getitem_54 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 0), kwargs = {})\n    %getitem_55 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 1), kwargs = {})\n    %aten_view_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_9, [6, 257, 64]), kwargs = {})\n    %alloc_118 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_10 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_54, [1, 6, 257, 64]), kwargs = {out: %alloc_118})\n    %alloc_119 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_11 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_55, [1, 6, 64, 257]), kwargs = {out: %alloc_119})\n    %aten_view_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_10, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_11, [6, 64, 257]), kwargs = {})\n    %lowered_module_33 : [num_users=1] = get_attr[target=lowered_module_33]\n    %executorch_call_delegate_33 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_33, %aten_view_copy_default_25, %aten_view_copy_default_26), kwargs = {})\n    %getitem_56 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_33, 0), kwargs = {})\n    %aten_view_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_56, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_34 : [num_users=1] = get_attr[target=lowered_module_34]\n    %executorch_call_delegate_34 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_34, %aten_view_copy_default_27), kwargs = {})\n    %getitem_57 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_34, 0), kwargs = {})\n    %alloc_120 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_11 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_57,), kwargs = {out: %alloc_120})\n    %alloc_121 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_12 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_11, [1, 6, 257, 257]), kwargs = {out: %alloc_121})\n    %aten_view_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_12, [6, 257, 257]), kwargs = {})\n    %lowered_module_35 : [num_users=1] = get_attr[target=lowered_module_35]\n    %executorch_call_delegate_35 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_35, %aten_view_copy_default_28, %aten_view_copy_default_24), kwargs = {})\n    %getitem_58 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_35, 0), kwargs = {})\n    %aten_view_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_58, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_36 : [num_users=1] = get_attr[target=lowered_module_36]\n    %executorch_call_delegate_36 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_36, %aten_view_copy_default_29), kwargs = {})\n    %getitem_59 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 0), kwargs = {})\n    %alloc_122 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_12 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_59,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_122})\n    %aten_view_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_12, [1, 257, 384]), kwargs = {})\n    %lowered_module_37 : [num_users=1] = get_attr[target=lowered_module_37]\n    %executorch_call_delegate_37 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_37, %aten_view_copy_default_30), kwargs = {})\n    %getitem_60 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_37, 0), kwargs = {})\n    %alloc_123 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_13 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_60,), kwargs = {out: %alloc_123})\n    %lowered_module_38 : [num_users=1] = get_attr[target=lowered_module_38]\n    %executorch_call_delegate_38 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_38, %aten_clone_default_13, %p_blocks_2_ls1_gamma, %getitem_50), kwargs = {})\n    %getitem_61 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_38, 0), kwargs = {})\n    %alloc_124 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_125 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_126 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_5 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_61, [384], %p_blocks_2_norm2_weight, %p_blocks_2_norm2_bias, 1e-06), kwargs = {out0: %alloc_124, out1: %alloc_125, out2: %alloc_126})\n    %getitem_62 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_5, 0), kwargs = {})\n    %lowered_module_39 : [num_users=1] = get_attr[target=lowered_module_39]\n    %executorch_call_delegate_39 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_39, %getitem_62), kwargs = {})\n    %getitem_63 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_39, 0), kwargs = {})\n    %alloc_127 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_14 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_63,), kwargs = {out: %alloc_127})\n    %lowered_module_40 : [num_users=1] = get_attr[target=lowered_module_40]\n    %executorch_call_delegate_40 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_40, %aten_clone_default_14), kwargs = {})\n    %getitem_64 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 0), kwargs = {})\n    %alloc_128 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_64,), kwargs = {out: %alloc_128})\n    %lowered_module_41 : [num_users=1] = get_attr[target=lowered_module_41]\n    %executorch_call_delegate_41 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_41, %aten_clone_default_15, %p_blocks_2_ls2_gamma, %getitem_61), kwargs = {})\n    %getitem_65 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_41, 0), kwargs = {})\n    %alloc_129 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_130 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_131 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_6 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_65, [384], %p_blocks_3_norm1_weight, %p_blocks_3_norm1_bias, 1e-06), kwargs = {out0: %alloc_129, out1: %alloc_130, out2: %alloc_131})\n    %getitem_66 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_6, 0), kwargs = {})\n    %lowered_module_42 : [num_users=1] = get_attr[target=lowered_module_42]\n    %executorch_call_delegate_42 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_42, %getitem_66), kwargs = {})\n    %getitem_67 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_42, 0), kwargs = {})\n    %aten_view_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_67, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_43 : [num_users=1] = get_attr[target=lowered_module_43]\n    %executorch_call_delegate_43 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_43, %aten_view_copy_default_31), kwargs = {})\n    %getitem_68 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_43, 0), kwargs = {})\n    %alloc_132 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_10 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 0), kwargs = {out: %alloc_132})\n    %alloc_133 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_11 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 1), kwargs = {out: %alloc_133})\n    %alloc_134 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_12 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 2), kwargs = {out: %alloc_134})\n    %lowered_module_44 : [num_users=1] = get_attr[target=lowered_module_44]\n    %executorch_call_delegate_44 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_44, %aten_select_copy_int_10, %_lifted_tensor_constant35, %aten_select_copy_int_11), kwargs = {})\n    %alloc_135 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_13 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_12, [1, 6, 257, 64]), kwargs = {out: %alloc_135})\n    %getitem_69 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 0), kwargs = {})\n    %getitem_70 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 1), kwargs = {})\n    %aten_view_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_13, [6, 257, 64]), kwargs = {})\n    %alloc_136 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_14 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_69, [1, 6, 257, 64]), kwargs = {out: %alloc_136})\n    %alloc_137 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_15 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_70, [1, 6, 64, 257]), kwargs = {out: %alloc_137})\n    %aten_view_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_14, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_15, [6, 64, 257]), kwargs = {})\n    %lowered_module_45 : [num_users=1] = get_attr[target=lowered_module_45]\n    %executorch_call_delegate_45 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_45, %aten_view_copy_default_33, %aten_view_copy_default_34), kwargs = {})\n    %getitem_71 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_45, 0), kwargs = {})\n    %aten_view_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_71, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_46 : [num_users=1] = get_attr[target=lowered_module_46]\n    %executorch_call_delegate_46 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_46, %aten_view_copy_default_35), kwargs = {})\n    %getitem_72 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_46, 0), kwargs = {})\n    %alloc_138 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_72,), kwargs = {out: %alloc_138})\n    %alloc_139 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_16 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_16, [1, 6, 257, 257]), kwargs = {out: %alloc_139})\n    %aten_view_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_16, [6, 257, 257]), kwargs = {})\n    %lowered_module_47 : [num_users=1] = get_attr[target=lowered_module_47]\n    %executorch_call_delegate_47 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_47, %aten_view_copy_default_36, %aten_view_copy_default_32), kwargs = {})\n    %getitem_73 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_47, 0), kwargs = {})\n    %aten_view_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_73, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_48 : [num_users=1] = get_attr[target=lowered_module_48]\n    %executorch_call_delegate_48 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_48, %aten_view_copy_default_37), kwargs = {})\n    %getitem_74 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_48, 0), kwargs = {})\n    %alloc_140 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_74,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_140})\n    %aten_view_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_17, [1, 257, 384]), kwargs = {})\n    %lowered_module_49 : [num_users=1] = get_attr[target=lowered_module_49]\n    %executorch_call_delegate_49 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_49, %aten_view_copy_default_38), kwargs = {})\n    %getitem_75 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_49, 0), kwargs = {})\n    %alloc_141 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_75,), kwargs = {out: %alloc_141})\n    %lowered_module_50 : [num_users=1] = get_attr[target=lowered_module_50]\n    %executorch_call_delegate_50 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_50, %aten_clone_default_18, %p_blocks_3_ls1_gamma, %getitem_65), kwargs = {})\n    %getitem_76 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_50, 0), kwargs = {})\n    %alloc_142 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_143 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_144 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_7 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_76, [384], %p_blocks_3_norm2_weight, %p_blocks_3_norm2_bias, 1e-06), kwargs = {out0: %alloc_142, out1: %alloc_143, out2: %alloc_144})\n    %getitem_77 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_7, 0), kwargs = {})\n    %lowered_module_51 : [num_users=1] = get_attr[target=lowered_module_51]\n    %executorch_call_delegate_51 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_51, %getitem_77), kwargs = {})\n    %getitem_78 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_51, 0), kwargs = {})\n    %alloc_145 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_78,), kwargs = {out: %alloc_145})\n    %lowered_module_52 : [num_users=1] = get_attr[target=lowered_module_52]\n    %executorch_call_delegate_52 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_52, %aten_clone_default_19), kwargs = {})\n    %getitem_79 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_52, 0), kwargs = {})\n    %alloc_146 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_79,), kwargs = {out: %alloc_146})\n    %lowered_module_53 : [num_users=1] = get_attr[target=lowered_module_53]\n    %executorch_call_delegate_53 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_53, %aten_clone_default_20, %p_blocks_3_ls2_gamma, %getitem_76), kwargs = {})\n    %getitem_80 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_53, 0), kwargs = {})\n    %alloc_147 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_148 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_149 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_8 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_80, [384], %p_blocks_4_norm1_weight, %p_blocks_4_norm1_bias, 1e-06), kwargs = {out0: %alloc_147, out1: %alloc_148, out2: %alloc_149})\n    %getitem_81 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_8, 0), kwargs = {})\n    %lowered_module_54 : [num_users=1] = get_attr[target=lowered_module_54]\n    %executorch_call_delegate_54 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_54, %getitem_81), kwargs = {})\n    %getitem_82 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_54, 0), kwargs = {})\n    %aten_view_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_82, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_55 : [num_users=1] = get_attr[target=lowered_module_55]\n    %executorch_call_delegate_55 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_55, %aten_view_copy_default_39), kwargs = {})\n    %getitem_83 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_55, 0), kwargs = {})\n    %alloc_150 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_13 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 0), kwargs = {out: %alloc_150})\n    %alloc_151 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_14 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 1), kwargs = {out: %alloc_151})\n    %alloc_152 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_15 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 2), kwargs = {out: %alloc_152})\n    %lowered_module_56 : [num_users=1] = get_attr[target=lowered_module_56]\n    %executorch_call_delegate_56 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_56, %aten_select_copy_int_13, %_lifted_tensor_constant36, %aten_select_copy_int_14), kwargs = {})\n    %alloc_153 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_17 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_15, [1, 6, 257, 64]), kwargs = {out: %alloc_153})\n    %getitem_84 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_56, 0), kwargs = {})\n    %getitem_85 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_56, 1), kwargs = {})\n    %aten_view_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_17, [6, 257, 64]), kwargs = {})\n    %alloc_154 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_18 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_84, [1, 6, 257, 64]), kwargs = {out: %alloc_154})\n    %alloc_155 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_19 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_85, [1, 6, 64, 257]), kwargs = {out: %alloc_155})\n    %aten_view_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_18, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_19, [6, 64, 257]), kwargs = {})\n    %lowered_module_57 : [num_users=1] = get_attr[target=lowered_module_57]\n    %executorch_call_delegate_57 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_57, %aten_view_copy_default_41, %aten_view_copy_default_42), kwargs = {})\n    %getitem_86 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_57, 0), kwargs = {})\n    %aten_view_copy_default_43 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_86, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_58 : [num_users=1] = get_attr[target=lowered_module_58]\n    %executorch_call_delegate_58 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_58, %aten_view_copy_default_43), kwargs = {})\n    %getitem_87 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_58, 0), kwargs = {})\n    %alloc_156 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_87,), kwargs = {out: %alloc_156})\n    %alloc_157 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_20 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_21, [1, 6, 257, 257]), kwargs = {out: %alloc_157})\n    %aten_view_copy_default_44 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_20, [6, 257, 257]), kwargs = {})\n    %lowered_module_59 : [num_users=1] = get_attr[target=lowered_module_59]\n    %executorch_call_delegate_59 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_59, %aten_view_copy_default_44, %aten_view_copy_default_40), kwargs = {})\n    %getitem_88 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_59, 0), kwargs = {})\n    %aten_view_copy_default_45 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_88, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_60 : [num_users=1] = get_attr[target=lowered_module_60]\n    %executorch_call_delegate_60 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_60, %aten_view_copy_default_45), kwargs = {})\n    %getitem_89 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_60, 0), kwargs = {})\n    %alloc_158 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_89,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_158})\n    %aten_view_copy_default_46 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_22, [1, 257, 384]), kwargs = {})\n    %lowered_module_61 : [num_users=1] = get_attr[target=lowered_module_61]\n    %executorch_call_delegate_61 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_61, %aten_view_copy_default_46), kwargs = {})\n    %getitem_90 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_61, 0), kwargs = {})\n    %alloc_159 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_90,), kwargs = {out: %alloc_159})\n    %lowered_module_62 : [num_users=1] = get_attr[target=lowered_module_62]\n    %executorch_call_delegate_62 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_62, %aten_clone_default_23, %p_blocks_4_ls1_gamma, %getitem_80), kwargs = {})\n    %getitem_91 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_62, 0), kwargs = {})\n    %alloc_160 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_161 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_162 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_9 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_91, [384], %p_blocks_4_norm2_weight, %p_blocks_4_norm2_bias, 1e-06), kwargs = {out0: %alloc_160, out1: %alloc_161, out2: %alloc_162})\n    %getitem_92 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_9, 0), kwargs = {})\n    %lowered_module_63 : [num_users=1] = get_attr[target=lowered_module_63]\n    %executorch_call_delegate_63 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_63, %getitem_92), kwargs = {})\n    %getitem_93 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_63, 0), kwargs = {})\n    %alloc_163 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_93,), kwargs = {out: %alloc_163})\n    %lowered_module_64 : [num_users=1] = get_attr[target=lowered_module_64]\n    %executorch_call_delegate_64 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_64, %aten_clone_default_24), kwargs = {})\n    %getitem_94 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_64, 0), kwargs = {})\n    %alloc_164 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_25 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_94,), kwargs = {out: %alloc_164})\n    %lowered_module_65 : [num_users=1] = get_attr[target=lowered_module_65]\n    %executorch_call_delegate_65 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_65, %aten_clone_default_25, %p_blocks_4_ls2_gamma, %getitem_91), kwargs = {})\n    %getitem_95 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_65, 0), kwargs = {})\n    %alloc_165 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_166 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_167 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_10 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_95, [384], %p_blocks_5_norm1_weight, %p_blocks_5_norm1_bias, 1e-06), kwargs = {out0: %alloc_165, out1: %alloc_166, out2: %alloc_167})\n    %getitem_96 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_10, 0), kwargs = {})\n    %lowered_module_66 : [num_users=1] = get_attr[target=lowered_module_66]\n    %executorch_call_delegate_66 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_66, %getitem_96), kwargs = {})\n    %getitem_97 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_66, 0), kwargs = {})\n    %aten_view_copy_default_47 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_97, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_67 : [num_users=1] = get_attr[target=lowered_module_67]\n    %executorch_call_delegate_67 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_67, %aten_view_copy_default_47), kwargs = {})\n    %getitem_98 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_67, 0), kwargs = {})\n    %alloc_168 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_16 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 0), kwargs = {out: %alloc_168})\n    %alloc_169 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_17 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 1), kwargs = {out: %alloc_169})\n    %alloc_170 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_18 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 2), kwargs = {out: %alloc_170})\n    %lowered_module_68 : [num_users=1] = get_attr[target=lowered_module_68]\n    %executorch_call_delegate_68 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_68, %aten_select_copy_int_16, %_lifted_tensor_constant37, %aten_select_copy_int_17), kwargs = {})\n    %alloc_171 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_21 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_18, [1, 6, 257, 64]), kwargs = {out: %alloc_171})\n    %getitem_99 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_68, 0), kwargs = {})\n    %getitem_100 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_68, 1), kwargs = {})\n    %aten_view_copy_default_48 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_21, [6, 257, 64]), kwargs = {})\n    %alloc_172 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_22 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_99, [1, 6, 257, 64]), kwargs = {out: %alloc_172})\n    %alloc_173 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_23 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_100, [1, 6, 64, 257]), kwargs = {out: %alloc_173})\n    %aten_view_copy_default_49 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_22, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_50 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_23, [6, 64, 257]), kwargs = {})\n    %lowered_module_69 : [num_users=1] = get_attr[target=lowered_module_69]\n    %executorch_call_delegate_69 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_69, %aten_view_copy_default_49, %aten_view_copy_default_50), kwargs = {})\n    %getitem_101 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_69, 0), kwargs = {})\n    %aten_view_copy_default_51 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_101, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_70 : [num_users=1] = get_attr[target=lowered_module_70]\n    %executorch_call_delegate_70 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_70, %aten_view_copy_default_51), kwargs = {})\n    %getitem_102 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_70, 0), kwargs = {})\n    %alloc_174 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_26 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_102,), kwargs = {out: %alloc_174})\n    %alloc_175 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_24 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_26, [1, 6, 257, 257]), kwargs = {out: %alloc_175})\n    %aten_view_copy_default_52 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_24, [6, 257, 257]), kwargs = {})\n    %lowered_module_71 : [num_users=1] = get_attr[target=lowered_module_71]\n    %executorch_call_delegate_71 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_71, %aten_view_copy_default_52, %aten_view_copy_default_48), kwargs = {})\n    %getitem_103 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_71, 0), kwargs = {})\n    %aten_view_copy_default_53 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_103, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_72 : [num_users=1] = get_attr[target=lowered_module_72]\n    %executorch_call_delegate_72 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_72, %aten_view_copy_default_53), kwargs = {})\n    %getitem_104 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_72, 0), kwargs = {})\n    %alloc_176 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_27 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_104,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_176})\n    %aten_view_copy_default_54 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_27, [1, 257, 384]), kwargs = {})\n    %lowered_module_73 : [num_users=1] = get_attr[target=lowered_module_73]\n    %executorch_call_delegate_73 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_73, %aten_view_copy_default_54), kwargs = {})\n    %getitem_105 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_73, 0), kwargs = {})\n    %alloc_177 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_28 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_105,), kwargs = {out: %alloc_177})\n    %lowered_module_74 : [num_users=1] = get_attr[target=lowered_module_74]\n    %executorch_call_delegate_74 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_74, %aten_clone_default_28, %p_blocks_5_ls1_gamma, %getitem_95), kwargs = {})\n    %getitem_106 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_74, 0), kwargs = {})\n    %alloc_178 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_179 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_180 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_11 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_106, [384], %p_blocks_5_norm2_weight, %p_blocks_5_norm2_bias, 1e-06), kwargs = {out0: %alloc_178, out1: %alloc_179, out2: %alloc_180})\n    %getitem_107 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_11, 0), kwargs = {})\n    %lowered_module_75 : [num_users=1] = get_attr[target=lowered_module_75]\n    %executorch_call_delegate_75 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_75, %getitem_107), kwargs = {})\n    %getitem_108 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_75, 0), kwargs = {})\n    %alloc_181 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_29 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_108,), kwargs = {out: %alloc_181})\n    %lowered_module_76 : [num_users=1] = get_attr[target=lowered_module_76]\n    %executorch_call_delegate_76 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_76, %aten_clone_default_29), kwargs = {})\n    %getitem_109 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_76, 0), kwargs = {})\n    %alloc_182 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_30 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_109,), kwargs = {out: %alloc_182})\n    %lowered_module_77 : [num_users=1] = get_attr[target=lowered_module_77]\n    %executorch_call_delegate_77 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_77, %aten_clone_default_30, %p_blocks_5_ls2_gamma, %getitem_106), kwargs = {})\n    %getitem_110 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_77, 0), kwargs = {})\n    %alloc_183 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_184 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_185 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_12 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_110, [384], %p_blocks_6_norm1_weight, %p_blocks_6_norm1_bias, 1e-06), kwargs = {out0: %alloc_183, out1: %alloc_184, out2: %alloc_185})\n    %getitem_111 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_12, 0), kwargs = {})\n    %lowered_module_78 : [num_users=1] = get_attr[target=lowered_module_78]\n    %executorch_call_delegate_78 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_78, %getitem_111), kwargs = {})\n    %getitem_112 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_78, 0), kwargs = {})\n    %aten_view_copy_default_55 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_112, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_79 : [num_users=1] = get_attr[target=lowered_module_79]\n    %executorch_call_delegate_79 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_79, %aten_view_copy_default_55), kwargs = {})\n    %getitem_113 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_79, 0), kwargs = {})\n    %alloc_186 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_19 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 0), kwargs = {out: %alloc_186})\n    %alloc_187 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_20 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 1), kwargs = {out: %alloc_187})\n    %alloc_188 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_21 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 2), kwargs = {out: %alloc_188})\n    %lowered_module_80 : [num_users=1] = get_attr[target=lowered_module_80]\n    %executorch_call_delegate_80 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_80, %aten_select_copy_int_19, %_lifted_tensor_constant38, %aten_select_copy_int_20), kwargs = {})\n    %alloc_189 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_25 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_21, [1, 6, 257, 64]), kwargs = {out: %alloc_189})\n    %getitem_114 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_80, 0), kwargs = {})\n    %getitem_115 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_80, 1), kwargs = {})\n    %aten_view_copy_default_56 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_25, [6, 257, 64]), kwargs = {})\n    %alloc_190 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_26 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_114, [1, 6, 257, 64]), kwargs = {out: %alloc_190})\n    %alloc_191 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_27 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_115, [1, 6, 64, 257]), kwargs = {out: %alloc_191})\n    %aten_view_copy_default_57 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_26, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_58 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_27, [6, 64, 257]), kwargs = {})\n    %lowered_module_81 : [num_users=1] = get_attr[target=lowered_module_81]\n    %executorch_call_delegate_81 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_81, %aten_view_copy_default_57, %aten_view_copy_default_58), kwargs = {})\n    %getitem_116 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_81, 0), kwargs = {})\n    %aten_view_copy_default_59 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_116, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_82 : [num_users=1] = get_attr[target=lowered_module_82]\n    %executorch_call_delegate_82 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_82, %aten_view_copy_default_59), kwargs = {})\n    %getitem_117 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_82, 0), kwargs = {})\n    %alloc_192 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_31 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_117,), kwargs = {out: %alloc_192})\n    %alloc_193 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_28 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_31, [1, 6, 257, 257]), kwargs = {out: %alloc_193})\n    %aten_view_copy_default_60 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_28, [6, 257, 257]), kwargs = {})\n    %lowered_module_83 : [num_users=1] = get_attr[target=lowered_module_83]\n    %executorch_call_delegate_83 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_83, %aten_view_copy_default_60, %aten_view_copy_default_56), kwargs = {})\n    %getitem_118 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_83, 0), kwargs = {})\n    %aten_view_copy_default_61 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_118, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_84 : [num_users=1] = get_attr[target=lowered_module_84]\n    %executorch_call_delegate_84 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_84, %aten_view_copy_default_61), kwargs = {})\n    %getitem_119 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_84, 0), kwargs = {})\n    %alloc_194 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_32 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_119,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_194})\n    %aten_view_copy_default_62 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_32, [1, 257, 384]), kwargs = {})\n    %lowered_module_85 : [num_users=1] = get_attr[target=lowered_module_85]\n    %executorch_call_delegate_85 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_85, %aten_view_copy_default_62), kwargs = {})\n    %getitem_120 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_85, 0), kwargs = {})\n    %alloc_195 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_33 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_120,), kwargs = {out: %alloc_195})\n    %lowered_module_86 : [num_users=1] = get_attr[target=lowered_module_86]\n    %executorch_call_delegate_86 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_86, %aten_clone_default_33, %p_blocks_6_ls1_gamma, %getitem_110), kwargs = {})\n    %getitem_121 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_86, 0), kwargs = {})\n    %alloc_196 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_197 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_198 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_13 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_121, [384], %p_blocks_6_norm2_weight, %p_blocks_6_norm2_bias, 1e-06), kwargs = {out0: %alloc_196, out1: %alloc_197, out2: %alloc_198})\n    %getitem_122 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_13, 0), kwargs = {})\n    %lowered_module_87 : [num_users=1] = get_attr[target=lowered_module_87]\n    %executorch_call_delegate_87 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_87, %getitem_122), kwargs = {})\n    %getitem_123 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_87, 0), kwargs = {})\n    %alloc_199 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_34 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_123,), kwargs = {out: %alloc_199})\n    %lowered_module_88 : [num_users=1] = get_attr[target=lowered_module_88]\n    %executorch_call_delegate_88 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_88, %aten_clone_default_34), kwargs = {})\n    %getitem_124 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_88, 0), kwargs = {})\n    %alloc_200 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_35 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_124,), kwargs = {out: %alloc_200})\n    %lowered_module_89 : [num_users=1] = get_attr[target=lowered_module_89]\n    %executorch_call_delegate_89 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_89, %aten_clone_default_35, %p_blocks_6_ls2_gamma, %getitem_121), kwargs = {})\n    %getitem_125 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_89, 0), kwargs = {})\n    %alloc_201 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_202 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_203 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_14 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_125, [384], %p_blocks_7_norm1_weight, %p_blocks_7_norm1_bias, 1e-06), kwargs = {out0: %alloc_201, out1: %alloc_202, out2: %alloc_203})\n    %getitem_126 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_14, 0), kwargs = {})\n    %lowered_module_90 : [num_users=1] = get_attr[target=lowered_module_90]\n    %executorch_call_delegate_90 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_90, %getitem_126), kwargs = {})\n    %getitem_127 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_90, 0), kwargs = {})\n    %aten_view_copy_default_63 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_127, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_91 : [num_users=1] = get_attr[target=lowered_module_91]\n    %executorch_call_delegate_91 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_91, %aten_view_copy_default_63), kwargs = {})\n    %getitem_128 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_91, 0), kwargs = {})\n    %alloc_204 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_22 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 0), kwargs = {out: %alloc_204})\n    %alloc_205 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_23 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 1), kwargs = {out: %alloc_205})\n    %alloc_206 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_24 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 2), kwargs = {out: %alloc_206})\n    %lowered_module_92 : [num_users=1] = get_attr[target=lowered_module_92]\n    %executorch_call_delegate_92 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_92, %aten_select_copy_int_22, %_lifted_tensor_constant39, %aten_select_copy_int_23), kwargs = {})\n    %alloc_207 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_29 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_24, [1, 6, 257, 64]), kwargs = {out: %alloc_207})\n    %getitem_129 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_92, 0), kwargs = {})\n    %getitem_130 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_92, 1), kwargs = {})\n    %aten_view_copy_default_64 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_29, [6, 257, 64]), kwargs = {})\n    %alloc_208 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_30 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_129, [1, 6, 257, 64]), kwargs = {out: %alloc_208})\n    %alloc_209 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_31 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_130, [1, 6, 64, 257]), kwargs = {out: %alloc_209})\n    %aten_view_copy_default_65 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_30, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_66 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_31, [6, 64, 257]), kwargs = {})\n    %lowered_module_93 : [num_users=1] = get_attr[target=lowered_module_93]\n    %executorch_call_delegate_93 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_93, %aten_view_copy_default_65, %aten_view_copy_default_66), kwargs = {})\n    %getitem_131 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_93, 0), kwargs = {})\n    %aten_view_copy_default_67 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_131, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_94 : [num_users=1] = get_attr[target=lowered_module_94]\n    %executorch_call_delegate_94 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_94, %aten_view_copy_default_67), kwargs = {})\n    %getitem_132 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_94, 0), kwargs = {})\n    %alloc_210 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_36 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_132,), kwargs = {out: %alloc_210})\n    %alloc_211 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_32 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_36, [1, 6, 257, 257]), kwargs = {out: %alloc_211})\n    %aten_view_copy_default_68 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_32, [6, 257, 257]), kwargs = {})\n    %lowered_module_95 : [num_users=1] = get_attr[target=lowered_module_95]\n    %executorch_call_delegate_95 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_95, %aten_view_copy_default_68, %aten_view_copy_default_64), kwargs = {})\n    %getitem_133 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_95, 0), kwargs = {})\n    %aten_view_copy_default_69 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_133, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_96 : [num_users=1] = get_attr[target=lowered_module_96]\n    %executorch_call_delegate_96 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_96, %aten_view_copy_default_69), kwargs = {})\n    %getitem_134 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_96, 0), kwargs = {})\n    %alloc_212 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_37 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_134,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_212})\n    %aten_view_copy_default_70 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_37, [1, 257, 384]), kwargs = {})\n    %lowered_module_97 : [num_users=1] = get_attr[target=lowered_module_97]\n    %executorch_call_delegate_97 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_97, %aten_view_copy_default_70), kwargs = {})\n    %getitem_135 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_97, 0), kwargs = {})\n    %alloc_213 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_38 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_135,), kwargs = {out: %alloc_213})\n    %lowered_module_98 : [num_users=1] = get_attr[target=lowered_module_98]\n    %executorch_call_delegate_98 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_98, %aten_clone_default_38, %p_blocks_7_ls1_gamma, %getitem_125), kwargs = {})\n    %getitem_136 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_98, 0), kwargs = {})\n    %alloc_214 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_215 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_216 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_15 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_136, [384], %p_blocks_7_norm2_weight, %p_blocks_7_norm2_bias, 1e-06), kwargs = {out0: %alloc_214, out1: %alloc_215, out2: %alloc_216})\n    %getitem_137 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_15, 0), kwargs = {})\n    %lowered_module_99 : [num_users=1] = get_attr[target=lowered_module_99]\n    %executorch_call_delegate_99 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_99, %getitem_137), kwargs = {})\n    %getitem_138 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_99, 0), kwargs = {})\n    %alloc_217 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_39 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_138,), kwargs = {out: %alloc_217})\n    %lowered_module_100 : [num_users=1] = get_attr[target=lowered_module_100]\n    %executorch_call_delegate_100 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_100, %aten_clone_default_39), kwargs = {})\n    %getitem_139 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_100, 0), kwargs = {})\n    %alloc_218 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_40 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_139,), kwargs = {out: %alloc_218})\n    %lowered_module_101 : [num_users=1] = get_attr[target=lowered_module_101]\n    %executorch_call_delegate_101 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_101, %aten_clone_default_40, %p_blocks_7_ls2_gamma, %getitem_136), kwargs = {})\n    %getitem_140 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_101, 0), kwargs = {})\n    %alloc_219 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_220 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_221 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_16 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_140, [384], %p_blocks_8_norm1_weight, %p_blocks_8_norm1_bias, 1e-06), kwargs = {out0: %alloc_219, out1: %alloc_220, out2: %alloc_221})\n    %getitem_141 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_16, 0), kwargs = {})\n    %lowered_module_102 : [num_users=1] = get_attr[target=lowered_module_102]\n    %executorch_call_delegate_102 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_102, %getitem_141), kwargs = {})\n    %getitem_142 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_102, 0), kwargs = {})\n    %aten_view_copy_default_71 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_142, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_103 : [num_users=1] = get_attr[target=lowered_module_103]\n    %executorch_call_delegate_103 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_103, %aten_view_copy_default_71), kwargs = {})\n    %getitem_143 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_103, 0), kwargs = {})\n    %alloc_222 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_25 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 0), kwargs = {out: %alloc_222})\n    %alloc_223 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_26 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 1), kwargs = {out: %alloc_223})\n    %alloc_224 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_27 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 2), kwargs = {out: %alloc_224})\n    %lowered_module_104 : [num_users=1] = get_attr[target=lowered_module_104]\n    %executorch_call_delegate_104 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_104, %aten_select_copy_int_25, %_lifted_tensor_constant40, %aten_select_copy_int_26), kwargs = {})\n    %alloc_225 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_33 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_27, [1, 6, 257, 64]), kwargs = {out: %alloc_225})\n    %getitem_144 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_104, 0), kwargs = {})\n    %getitem_145 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_104, 1), kwargs = {})\n    %aten_view_copy_default_72 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_33, [6, 257, 64]), kwargs = {})\n    %alloc_226 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_34 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_144, [1, 6, 257, 64]), kwargs = {out: %alloc_226})\n    %alloc_227 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_35 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_145, [1, 6, 64, 257]), kwargs = {out: %alloc_227})\n    %aten_view_copy_default_73 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_34, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_74 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_35, [6, 64, 257]), kwargs = {})\n    %lowered_module_105 : [num_users=1] = get_attr[target=lowered_module_105]\n    %executorch_call_delegate_105 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_105, %aten_view_copy_default_73, %aten_view_copy_default_74), kwargs = {})\n    %getitem_146 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_105, 0), kwargs = {})\n    %aten_view_copy_default_75 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_146, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_106 : [num_users=1] = get_attr[target=lowered_module_106]\n    %executorch_call_delegate_106 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_106, %aten_view_copy_default_75), kwargs = {})\n    %getitem_147 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_106, 0), kwargs = {})\n    %alloc_228 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_41 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_147,), kwargs = {out: %alloc_228})\n    %alloc_229 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_36 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_41, [1, 6, 257, 257]), kwargs = {out: %alloc_229})\n    %aten_view_copy_default_76 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_36, [6, 257, 257]), kwargs = {})\n    %lowered_module_107 : [num_users=1] = get_attr[target=lowered_module_107]\n    %executorch_call_delegate_107 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_107, %aten_view_copy_default_76, %aten_view_copy_default_72), kwargs = {})\n    %getitem_148 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_107, 0), kwargs = {})\n    %aten_view_copy_default_77 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_148, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_108 : [num_users=1] = get_attr[target=lowered_module_108]\n    %executorch_call_delegate_108 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_108, %aten_view_copy_default_77), kwargs = {})\n    %getitem_149 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_108, 0), kwargs = {})\n    %alloc_230 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_42 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_149,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_230})\n    %aten_view_copy_default_78 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_42, [1, 257, 384]), kwargs = {})\n    %lowered_module_109 : [num_users=1] = get_attr[target=lowered_module_109]\n    %executorch_call_delegate_109 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_109, %aten_view_copy_default_78), kwargs = {})\n    %getitem_150 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_109, 0), kwargs = {})\n    %alloc_231 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_43 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_150,), kwargs = {out: %alloc_231})\n    %lowered_module_110 : [num_users=1] = get_attr[target=lowered_module_110]\n    %executorch_call_delegate_110 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_110, %aten_clone_default_43, %p_blocks_8_ls1_gamma, %getitem_140), kwargs = {})\n    %getitem_151 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_110, 0), kwargs = {})\n    %alloc_232 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_233 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_234 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_17 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_151, [384], %p_blocks_8_norm2_weight, %p_blocks_8_norm2_bias, 1e-06), kwargs = {out0: %alloc_232, out1: %alloc_233, out2: %alloc_234})\n    %getitem_152 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_17, 0), kwargs = {})\n    %lowered_module_111 : [num_users=1] = get_attr[target=lowered_module_111]\n    %executorch_call_delegate_111 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_111, %getitem_152), kwargs = {})\n    %getitem_153 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_111, 0), kwargs = {})\n    %alloc_235 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_44 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_153,), kwargs = {out: %alloc_235})\n    %lowered_module_112 : [num_users=1] = get_attr[target=lowered_module_112]\n    %executorch_call_delegate_112 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_112, %aten_clone_default_44), kwargs = {})\n    %getitem_154 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_112, 0), kwargs = {})\n    %alloc_236 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_45 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_154,), kwargs = {out: %alloc_236})\n    %lowered_module_113 : [num_users=1] = get_attr[target=lowered_module_113]\n    %executorch_call_delegate_113 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_113, %aten_clone_default_45, %p_blocks_8_ls2_gamma, %getitem_151), kwargs = {})\n    %getitem_155 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_113, 0), kwargs = {})\n    %alloc_237 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_238 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_239 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_18 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_155, [384], %p_blocks_9_norm1_weight, %p_blocks_9_norm1_bias, 1e-06), kwargs = {out0: %alloc_237, out1: %alloc_238, out2: %alloc_239})\n    %getitem_156 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_18, 0), kwargs = {})\n    %lowered_module_114 : [num_users=1] = get_attr[target=lowered_module_114]\n    %executorch_call_delegate_114 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_114, %getitem_156), kwargs = {})\n    %getitem_157 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_114, 0), kwargs = {})\n    %aten_view_copy_default_79 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_157, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_115 : [num_users=1] = get_attr[target=lowered_module_115]\n    %executorch_call_delegate_115 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_115, %aten_view_copy_default_79), kwargs = {})\n    %getitem_158 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_115, 0), kwargs = {})\n    %alloc_240 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_28 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 0), kwargs = {out: %alloc_240})\n    %alloc_241 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_29 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 1), kwargs = {out: %alloc_241})\n    %alloc_242 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_30 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 2), kwargs = {out: %alloc_242})\n    %lowered_module_116 : [num_users=1] = get_attr[target=lowered_module_116]\n    %executorch_call_delegate_116 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_116, %aten_select_copy_int_28, %_lifted_tensor_constant41, %aten_select_copy_int_29), kwargs = {})\n    %alloc_243 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_37 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_30, [1, 6, 257, 64]), kwargs = {out: %alloc_243})\n    %getitem_159 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_116, 0), kwargs = {})\n    %getitem_160 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_116, 1), kwargs = {})\n    %aten_view_copy_default_80 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_37, [6, 257, 64]), kwargs = {})\n    %alloc_244 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_38 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_159, [1, 6, 257, 64]), kwargs = {out: %alloc_244})\n    %alloc_245 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_39 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_160, [1, 6, 64, 257]), kwargs = {out: %alloc_245})\n    %aten_view_copy_default_81 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_38, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_82 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_39, [6, 64, 257]), kwargs = {})\n    %lowered_module_117 : [num_users=1] = get_attr[target=lowered_module_117]\n    %executorch_call_delegate_117 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_117, %aten_view_copy_default_81, %aten_view_copy_default_82), kwargs = {})\n    %getitem_161 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_117, 0), kwargs = {})\n    %aten_view_copy_default_83 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_161, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_118 : [num_users=1] = get_attr[target=lowered_module_118]\n    %executorch_call_delegate_118 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_118, %aten_view_copy_default_83), kwargs = {})\n    %getitem_162 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_118, 0), kwargs = {})\n    %alloc_246 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_46 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_162,), kwargs = {out: %alloc_246})\n    %alloc_247 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_40 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_46, [1, 6, 257, 257]), kwargs = {out: %alloc_247})\n    %aten_view_copy_default_84 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_40, [6, 257, 257]), kwargs = {})\n    %lowered_module_119 : [num_users=1] = get_attr[target=lowered_module_119]\n    %executorch_call_delegate_119 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_119, %aten_view_copy_default_84, %aten_view_copy_default_80), kwargs = {})\n    %getitem_163 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_119, 0), kwargs = {})\n    %aten_view_copy_default_85 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_163, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_120 : [num_users=1] = get_attr[target=lowered_module_120]\n    %executorch_call_delegate_120 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_120, %aten_view_copy_default_85), kwargs = {})\n    %getitem_164 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_120, 0), kwargs = {})\n    %alloc_248 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_47 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_164,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_248})\n    %aten_view_copy_default_86 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_47, [1, 257, 384]), kwargs = {})\n    %lowered_module_121 : [num_users=1] = get_attr[target=lowered_module_121]\n    %executorch_call_delegate_121 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_121, %aten_view_copy_default_86), kwargs = {})\n    %getitem_165 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_121, 0), kwargs = {})\n    %alloc_249 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_48 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_165,), kwargs = {out: %alloc_249})\n    %lowered_module_122 : [num_users=1] = get_attr[target=lowered_module_122]\n    %executorch_call_delegate_122 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_122, %aten_clone_default_48, %p_blocks_9_ls1_gamma, %getitem_155), kwargs = {})\n    %getitem_166 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_122, 0), kwargs = {})\n    %alloc_250 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_251 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_252 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_19 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_166, [384], %p_blocks_9_norm2_weight, %p_blocks_9_norm2_bias, 1e-06), kwargs = {out0: %alloc_250, out1: %alloc_251, out2: %alloc_252})\n    %getitem_167 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_19, 0), kwargs = {})\n    %lowered_module_123 : [num_users=1] = get_attr[target=lowered_module_123]\n    %executorch_call_delegate_123 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_123, %getitem_167), kwargs = {})\n    %getitem_168 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_123, 0), kwargs = {})\n    %alloc_253 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_49 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_168,), kwargs = {out: %alloc_253})\n    %lowered_module_124 : [num_users=1] = get_attr[target=lowered_module_124]\n    %executorch_call_delegate_124 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_124, %aten_clone_default_49), kwargs = {})\n    %getitem_169 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_124, 0), kwargs = {})\n    %alloc_254 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_50 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_169,), kwargs = {out: %alloc_254})\n    %lowered_module_125 : [num_users=1] = get_attr[target=lowered_module_125]\n    %executorch_call_delegate_125 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_125, %aten_clone_default_50, %p_blocks_9_ls2_gamma, %getitem_166), kwargs = {})\n    %getitem_170 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_125, 0), kwargs = {})\n    %alloc_255 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_256 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_257 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_20 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_170, [384], %p_blocks_10_norm1_weight, %p_blocks_10_norm1_bias, 1e-06), kwargs = {out0: %alloc_255, out1: %alloc_256, out2: %alloc_257})\n    %getitem_171 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_20, 0), kwargs = {})\n    %lowered_module_126 : [num_users=1] = get_attr[target=lowered_module_126]\n    %executorch_call_delegate_126 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_126, %getitem_171), kwargs = {})\n    %getitem_172 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_126, 0), kwargs = {})\n    %aten_view_copy_default_87 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_172, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_127 : [num_users=1] = get_attr[target=lowered_module_127]\n    %executorch_call_delegate_127 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_127, %aten_view_copy_default_87), kwargs = {})\n    %getitem_173 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_127, 0), kwargs = {})\n    %alloc_258 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_31 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 0), kwargs = {out: %alloc_258})\n    %alloc_259 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_32 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 1), kwargs = {out: %alloc_259})\n    %alloc_260 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_33 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 2), kwargs = {out: %alloc_260})\n    %lowered_module_128 : [num_users=1] = get_attr[target=lowered_module_128]\n    %executorch_call_delegate_128 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_128, %aten_select_copy_int_31, %_lifted_tensor_constant42, %aten_select_copy_int_32), kwargs = {})\n    %alloc_261 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_41 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_33, [1, 6, 257, 64]), kwargs = {out: %alloc_261})\n    %getitem_174 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_128, 0), kwargs = {})\n    %getitem_175 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_128, 1), kwargs = {})\n    %aten_view_copy_default_88 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_41, [6, 257, 64]), kwargs = {})\n    %alloc_262 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_42 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_174, [1, 6, 257, 64]), kwargs = {out: %alloc_262})\n    %alloc_263 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_43 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_175, [1, 6, 64, 257]), kwargs = {out: %alloc_263})\n    %aten_view_copy_default_89 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_42, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_90 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_43, [6, 64, 257]), kwargs = {})\n    %lowered_module_129 : [num_users=1] = get_attr[target=lowered_module_129]\n    %executorch_call_delegate_129 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_129, %aten_view_copy_default_89, %aten_view_copy_default_90), kwargs = {})\n    %getitem_176 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_129, 0), kwargs = {})\n    %aten_view_copy_default_91 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_176, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_130 : [num_users=1] = get_attr[target=lowered_module_130]\n    %executorch_call_delegate_130 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_130, %aten_view_copy_default_91), kwargs = {})\n    %getitem_177 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_130, 0), kwargs = {})\n    %alloc_264 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_51 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_177,), kwargs = {out: %alloc_264})\n    %alloc_265 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_44 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_51, [1, 6, 257, 257]), kwargs = {out: %alloc_265})\n    %aten_view_copy_default_92 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_44, [6, 257, 257]), kwargs = {})\n    %lowered_module_131 : [num_users=1] = get_attr[target=lowered_module_131]\n    %executorch_call_delegate_131 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_131, %aten_view_copy_default_92, %aten_view_copy_default_88), kwargs = {})\n    %getitem_178 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_131, 0), kwargs = {})\n    %aten_view_copy_default_93 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_178, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_132 : [num_users=1] = get_attr[target=lowered_module_132]\n    %executorch_call_delegate_132 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_132, %aten_view_copy_default_93), kwargs = {})\n    %getitem_179 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_132, 0), kwargs = {})\n    %alloc_266 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_52 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_179,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_266})\n    %aten_view_copy_default_94 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_52, [1, 257, 384]), kwargs = {})\n    %lowered_module_133 : [num_users=1] = get_attr[target=lowered_module_133]\n    %executorch_call_delegate_133 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_133, %aten_view_copy_default_94), kwargs = {})\n    %getitem_180 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_133, 0), kwargs = {})\n    %alloc_267 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_53 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_180,), kwargs = {out: %alloc_267})\n    %lowered_module_134 : [num_users=1] = get_attr[target=lowered_module_134]\n    %executorch_call_delegate_134 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_134, %aten_clone_default_53, %p_blocks_10_ls1_gamma, %getitem_170), kwargs = {})\n    %getitem_181 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_134, 0), kwargs = {})\n    %alloc_268 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_269 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_270 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_21 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_181, [384], %p_blocks_10_norm2_weight, %p_blocks_10_norm2_bias, 1e-06), kwargs = {out0: %alloc_268, out1: %alloc_269, out2: %alloc_270})\n    %getitem_182 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_21, 0), kwargs = {})\n    %lowered_module_135 : [num_users=1] = get_attr[target=lowered_module_135]\n    %executorch_call_delegate_135 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_135, %getitem_182), kwargs = {})\n    %getitem_183 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_135, 0), kwargs = {})\n    %alloc_271 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_54 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_183,), kwargs = {out: %alloc_271})\n    %lowered_module_136 : [num_users=1] = get_attr[target=lowered_module_136]\n    %executorch_call_delegate_136 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_136, %aten_clone_default_54), kwargs = {})\n    %getitem_184 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_136, 0), kwargs = {})\n    %alloc_272 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_55 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_184,), kwargs = {out: %alloc_272})\n    %lowered_module_137 : [num_users=1] = get_attr[target=lowered_module_137]\n    %executorch_call_delegate_137 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_137, %aten_clone_default_55, %p_blocks_10_ls2_gamma, %getitem_181), kwargs = {})\n    %getitem_185 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_137, 0), kwargs = {})\n    %alloc_273 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_274 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_275 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_22 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_185, [384], %p_blocks_11_norm1_weight, %p_blocks_11_norm1_bias, 1e-06), kwargs = {out0: %alloc_273, out1: %alloc_274, out2: %alloc_275})\n    %getitem_186 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_22, 0), kwargs = {})\n    %lowered_module_138 : [num_users=1] = get_attr[target=lowered_module_138]\n    %executorch_call_delegate_138 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_138, %getitem_186), kwargs = {})\n    %getitem_187 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_138, 0), kwargs = {})\n    %aten_view_copy_default_95 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_187, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_139 : [num_users=1] = get_attr[target=lowered_module_139]\n    %executorch_call_delegate_139 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_139, %aten_view_copy_default_95), kwargs = {})\n    %getitem_188 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_139, 0), kwargs = {})\n    %alloc_276 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_34 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 0), kwargs = {out: %alloc_276})\n    %alloc_277 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_35 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 1), kwargs = {out: %alloc_277})\n    %alloc_278 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_36 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 2), kwargs = {out: %alloc_278})\n    %lowered_module_140 : [num_users=1] = get_attr[target=lowered_module_140]\n    %executorch_call_delegate_140 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_140, %aten_select_copy_int_34, %_lifted_tensor_constant43, %aten_select_copy_int_35), kwargs = {})\n    %alloc_279 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_45 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_36, [1, 6, 257, 64]), kwargs = {out: %alloc_279})\n    %getitem_189 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_140, 0), kwargs = {})\n    %getitem_190 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_140, 1), kwargs = {})\n    %aten_view_copy_default_96 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_45, [6, 257, 64]), kwargs = {})\n    %alloc_280 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_46 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_189, [1, 6, 257, 64]), kwargs = {out: %alloc_280})\n    %alloc_281 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_47 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_190, [1, 6, 64, 257]), kwargs = {out: %alloc_281})\n    %aten_view_copy_default_97 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_46, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_98 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_47, [6, 64, 257]), kwargs = {})\n    %lowered_module_141 : [num_users=1] = get_attr[target=lowered_module_141]\n    %executorch_call_delegate_141 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_141, %aten_view_copy_default_97, %aten_view_copy_default_98), kwargs = {})\n    %getitem_191 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_141, 0), kwargs = {})\n    %aten_view_copy_default_99 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_191, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_142 : [num_users=1] = get_attr[target=lowered_module_142]\n    %executorch_call_delegate_142 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_142, %aten_view_copy_default_99), kwargs = {})\n    %getitem_192 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_142, 0), kwargs = {})\n    %alloc_282 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_56 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_192,), kwargs = {out: %alloc_282})\n    %alloc_283 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_48 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_56, [1, 6, 257, 257]), kwargs = {out: %alloc_283})\n    %aten_view_copy_default_100 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_48, [6, 257, 257]), kwargs = {})\n    %lowered_module_143 : [num_users=1] = get_attr[target=lowered_module_143]\n    %executorch_call_delegate_143 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_143, %aten_view_copy_default_100, %aten_view_copy_default_96), kwargs = {})\n    %getitem_193 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_143, 0), kwargs = {})\n    %aten_view_copy_default_101 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_193, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_144 : [num_users=1] = get_attr[target=lowered_module_144]\n    %executorch_call_delegate_144 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_144, %aten_view_copy_default_101), kwargs = {})\n    %getitem_194 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_144, 0), kwargs = {})\n    %alloc_284 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_57 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_194,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_284})\n    %aten_view_copy_default_102 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_57, [1, 257, 384]), kwargs = {})\n    %lowered_module_145 : [num_users=1] = get_attr[target=lowered_module_145]\n    %executorch_call_delegate_145 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_145, %aten_view_copy_default_102), kwargs = {})\n    %getitem_195 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_145, 0), kwargs = {})\n    %alloc_285 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_58 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_195,), kwargs = {out: %alloc_285})\n    %lowered_module_146 : [num_users=1] = get_attr[target=lowered_module_146]\n    %executorch_call_delegate_146 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_146, %aten_clone_default_58, %p_blocks_11_ls1_gamma, %getitem_185), kwargs = {})\n    %getitem_196 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_146, 0), kwargs = {})\n    %alloc_286 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_287 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_288 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_23 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_196, [384], %p_blocks_11_norm2_weight, %p_blocks_11_norm2_bias, 1e-06), kwargs = {out0: %alloc_286, out1: %alloc_287, out2: %alloc_288})\n    %getitem_197 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_23, 0), kwargs = {})\n    %lowered_module_147 : [num_users=1] = get_attr[target=lowered_module_147]\n    %executorch_call_delegate_147 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_147, %getitem_197), kwargs = {})\n    %getitem_198 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_147, 0), kwargs = {})\n    %alloc_289 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_59 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_198,), kwargs = {out: %alloc_289})\n    %lowered_module_148 : [num_users=1] = get_attr[target=lowered_module_148]\n    %executorch_call_delegate_148 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_148, %aten_clone_default_59), kwargs = {})\n    %getitem_199 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_148, 0), kwargs = {})\n    %alloc_290 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_60 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_199,), kwargs = {out: %alloc_290})\n    %lowered_module_149 : [num_users=1] = get_attr[target=lowered_module_149]\n    %executorch_call_delegate_149 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_149, %aten_clone_default_60, %p_blocks_11_ls2_gamma, %getitem_196), kwargs = {})\n    %getitem_200 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_149, 0), kwargs = {})\n    %alloc_291 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_292 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_293 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_24 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_200, [384], %p_norm_weight, %p_norm_bias, 1e-06), kwargs = {out0: %alloc_291, out1: %alloc_292, out2: %alloc_293})\n    %getitem_201 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_24, 0), kwargs = {})\n    %alloc_294 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384), torch.float32),), kwargs = {})\n    %aten_select_copy_int_37 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_201, 1, 0), kwargs = {out: %alloc_294})\n    %lowered_module_150 : [num_users=1] = get_attr[target=lowered_module_150]\n    %executorch_call_delegate_150 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_150, %aten_select_copy_int_37), kwargs = {})\n    %getitem_202 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_150, 0), kwargs = {})\n    return (getitem_202,)\nThis node aten_clone_default has metadata of:\nThe node stacktrace:\nTraceback (most recent call last): \n    File \"/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 325, in forward\nret = self.forward_features(*args, **kwargs) \n\n\n\n\nWhile executing %aten_clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_17,), kwargs = {memory_format: torch.channels_last, out: %alloc_74})\nGraphModule: class GraphModule(torch.nn.Module):\n    def forward(self, p_cls_token: \"f32[1, 1, 384][384, 384, 1]\", p_pos_embed: \"f32[1, 1370, 384][526080, 384, 1]\", p_blocks_0_norm1_weight: \"f32[384][1]\", p_blocks_0_norm1_bias: \"f32[384][1]\", p_blocks_0_ls1_gamma: \"f32[384][1]\", p_blocks_0_norm2_weight: \"f32[384][1]\", p_blocks_0_norm2_bias: \"f32[384][1]\", p_blocks_0_ls2_gamma: \"f32[384][1]\", p_blocks_1_norm1_weight: \"f32[384][1]\", p_blocks_1_norm1_bias: \"f32[384][1]\", p_blocks_1_ls1_gamma: \"f32[384][1]\", p_blocks_1_norm2_weight: \"f32[384][1]\", p_blocks_1_norm2_bias: \"f32[384][1]\", p_blocks_1_ls2_gamma: \"f32[384][1]\", p_blocks_2_norm1_weight: \"f32[384][1]\", p_blocks_2_norm1_bias: \"f32[384][1]\", p_blocks_2_ls1_gamma: \"f32[384][1]\", p_blocks_2_norm2_weight: \"f32[384][1]\", p_blocks_2_norm2_bias: \"f32[384][1]\", p_blocks_2_ls2_gamma: \"f32[384][1]\", p_blocks_3_norm1_weight: \"f32[384][1]\", p_blocks_3_norm1_bias: \"f32[384][1]\", p_blocks_3_ls1_gamma: \"f32[384][1]\", p_blocks_3_norm2_weight: \"f32[384][1]\", p_blocks_3_norm2_bias: \"f32[384][1]\", p_blocks_3_ls2_gamma: \"f32[384][1]\", p_blocks_4_norm1_weight: \"f32[384][1]\", p_blocks_4_norm1_bias: \"f32[384][1]\", p_blocks_4_ls1_gamma: \"f32[384][1]\", p_blocks_4_norm2_weight: \"f32[384][1]\", p_blocks_4_norm2_bias: \"f32[384][1]\", p_blocks_4_ls2_gamma: \"f32[384][1]\", p_blocks_5_norm1_weight: \"f32[384][1]\", p_blocks_5_norm1_bias: \"f32[384][1]\", p_blocks_5_ls1_gamma: \"f32[384][1]\", p_blocks_5_norm2_weight: \"f32[384][1]\", p_blocks_5_norm2_bias: \"f32[384][1]\", p_blocks_5_ls2_gamma: \"f32[384][1]\", p_blocks_6_norm1_weight: \"f32[384][1]\", p_blocks_6_norm1_bias: \"f32[384][1]\", p_blocks_6_ls1_gamma: \"f32[384][1]\", p_blocks_6_norm2_weight: \"f32[384][1]\", p_blocks_6_norm2_bias: \"f32[384][1]\", p_blocks_6_ls2_gamma: \"f32[384][1]\", p_blocks_7_norm1_weight: \"f32[384][1]\", p_blocks_7_norm1_bias: \"f32[384][1]\", p_blocks_7_ls1_gamma: \"f32[384][1]\", p_blocks_7_norm2_weight: \"f32[384][1]\", p_blocks_7_norm2_bias: \"f32[384][1]\", p_blocks_7_ls2_gamma: \"f32[384][1]\", p_blocks_8_norm1_weight: \"f32[384][1]\", p_blocks_8_norm1_bias: \"f32[384][1]\", p_blocks_8_ls1_gamma: \"f32[384][1]\", p_blocks_8_norm2_weight: \"f32[384][1]\", p_blocks_8_norm2_bias: \"f32[384][1]\", p_blocks_8_ls2_gamma: \"f32[384][1]\", p_blocks_9_norm1_weight: \"f32[384][1]\", p_blocks_9_norm1_bias: \"f32[384][1]\", p_blocks_9_ls1_gamma: \"f32[384][1]\", p_blocks_9_norm2_weight: \"f32[384][1]\", p_blocks_9_norm2_bias: \"f32[384][1]\", p_blocks_9_ls2_gamma: \"f32[384][1]\", p_blocks_10_norm1_weight: \"f32[384][1]\", p_blocks_10_norm1_bias: \"f32[384][1]\", p_blocks_10_ls1_gamma: \"f32[384][1]\", p_blocks_10_norm2_weight: \"f32[384][1]\", p_blocks_10_norm2_bias: \"f32[384][1]\", p_blocks_10_ls2_gamma: \"f32[384][1]\", p_blocks_11_norm1_weight: \"f32[384][1]\", p_blocks_11_norm1_bias: \"f32[384][1]\", p_blocks_11_ls1_gamma: \"f32[384][1]\", p_blocks_11_norm2_weight: \"f32[384][1]\", p_blocks_11_norm2_bias: \"f32[384][1]\", p_blocks_11_ls2_gamma: \"f32[384][1]\", p_norm_weight: \"f32[384][1]\", p_norm_bias: \"f32[384][1]\", _lifted_tensor_constant0: \"f32[][]\", _lifted_tensor_constant1: \"f32[][]\", _lifted_tensor_constant2: \"f32[][]\", _lifted_tensor_constant3: \"f32[][]\", _lifted_tensor_constant4: \"f32[][]\", _lifted_tensor_constant5: \"f32[][]\", _lifted_tensor_constant6: \"i64[][]\", _lifted_tensor_constant7: \"i64[][]\", _lifted_tensor_constant8: \"i64[][]\", _lifted_tensor_constant9: \"i64[][]\", _lifted_tensor_constant10: \"i64[][]\", _lifted_tensor_constant11: \"i64[][]\", _lifted_tensor_constant12: \"f32[][]\", _lifted_tensor_constant13: \"f32[][]\", _lifted_tensor_constant14: \"f32[][]\", _lifted_tensor_constant15: \"f32[][]\", _lifted_tensor_constant16: \"f32[][]\", _lifted_tensor_constant17: \"f32[][]\", _lifted_tensor_constant18: \"f32[][]\", _lifted_tensor_constant19: \"f32[][]\", _lifted_tensor_constant20: \"f32[][]\", _lifted_tensor_constant21: \"i64[][]\", _lifted_tensor_constant22: \"f32[][]\", _lifted_tensor_constant23: \"f32[][]\", _lifted_tensor_constant24: \"f32[][]\", _lifted_tensor_constant25: \"f32[][]\", _lifted_tensor_constant26: \"f32[][]\", _lifted_tensor_constant27: \"f32[][]\", _lifted_tensor_constant28: \"f32[][]\", _lifted_tensor_constant29: \"f32[][]\", _lifted_tensor_constant30: \"f32[][]\", _lifted_tensor_constant31: \"i64[][]\", _lifted_tensor_constant32: \"f32[][]\", _lifted_tensor_constant33: \"f32[][]\", _lifted_tensor_constant34: \"f32[][]\", _lifted_tensor_constant35: \"f32[][]\", _lifted_tensor_constant36: \"f32[][]\", _lifted_tensor_constant37: \"f32[][]\", _lifted_tensor_constant38: \"f32[][]\", _lifted_tensor_constant39: \"f32[][]\", _lifted_tensor_constant40: \"f32[][]\", _lifted_tensor_constant41: \"f32[][]\", _lifted_tensor_constant42: \"f32[][]\", _lifted_tensor_constant43: \"f32[][]\", args_0: \"f32[1, 3, 224, 224][150528, 50176, 224, 1]\"):\n        # No stacktrace found for following nodes\n        alloc: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_arange_start_step: \"i64[16][1]\" = torch.ops.aten.arange.start_out(0, 16, out = alloc);  alloc = None\n        \n        # No stacktrace found for following nodes\n        alloc_1: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_arange_start_step_1: \"i64[16][1]\" = torch.ops.aten.arange.start_out(0, 16, out = alloc_1);  alloc_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_2: \"f32[1, 1, 384][384, 384, 1]\" = executorch_exir_memory_alloc(((1, 1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_expand_copy_default: \"f32[1, 1, 384][384, 384, 1]\" = torch.ops.aten.expand_copy.out(p_cls_token, [1, -1, -1], out = alloc_2);  p_cls_token = alloc_2 = None\n        \n        # No stacktrace found for following nodes\n        alloc_3: \"f32[1, 384][384, 1]\" = executorch_exir_memory_alloc(((1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_select_copy_int: \"f32[1, 384][384, 1]\" = torch.ops.aten.select_copy.int_out(p_pos_embed, 1, 0, out = alloc_3);  alloc_3 = None\n        \n        # No stacktrace found for following nodes\n        alloc_4: \"f32[][]\" = executorch_exir_memory_alloc(((), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default: \"f32[][]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(_lifted_tensor_constant21, dim_order = [], out = alloc_4);  _lifted_tensor_constant21 = alloc_4 = None\n        \n        # No stacktrace found for following nodes\n        alloc_5: \"f32[][]\" = executorch_exir_memory_alloc(((), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_1: \"f32[][]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(_lifted_tensor_constant31, dim_order = [], out = alloc_5);  _lifted_tensor_constant31 = alloc_5 = None\n        \n        # No stacktrace found for following nodes\n        alloc_6: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_2: \"f32[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(aten_arange_start_step, dim_order = [0], out = alloc_6);  aten_arange_start_step = alloc_6 = None\n        \n        # No stacktrace found for following nodes\n        alloc_7: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_3: \"f32[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(aten_arange_start_step_1, dim_order = [0], out = alloc_7);  aten_arange_start_step_1 = alloc_7 = None\n        \n        # No stacktrace found for following nodes\n        alloc_8: \"f32[1, 1, 384][384, 384, 1]\" = executorch_exir_memory_alloc(((1, 1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_unsqueeze_copy_default: \"f32[1, 1, 384][384, 384, 1]\" = torch.ops.aten.unsqueeze_copy.out(aten_select_copy_int, 0, out = alloc_8);  aten_select_copy_int = alloc_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_0 = self.lowered_module_0\n        executorch_call_delegate = torch.ops.higher_order.executorch_call_delegate(lowered_module_0, dim_order_ops__to_dim_order_copy_default_2, _lifted_tensor_constant3, _lifted_tensor_constant4, _lifted_tensor_constant5);  lowered_module_0 = dim_order_ops__to_dim_order_copy_default_2 = _lifted_tensor_constant3 = _lifted_tensor_constant4 = _lifted_tensor_constant5 = None\n        getitem: \"f32[16][1]\" = executorch_call_delegate[0];  executorch_call_delegate = None\n        alloc_9: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_unsqueeze_copy_default_1: \"f32[16, 1][1, 1]\" = torch.ops.aten.unsqueeze_copy.out(getitem, -1, out = alloc_9);  getitem = alloc_9 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_1 = self.lowered_module_1\n        executorch_call_delegate_1 = torch.ops.higher_order.executorch_call_delegate(lowered_module_1, p_pos_embed, dim_order_ops__to_dim_order_copy_default_3, _lifted_tensor_constant0, aten_unsqueeze_copy_default_1, _lifted_tensor_constant1, _lifted_tensor_constant2, _lifted_tensor_constant22, _lifted_tensor_constant23, _lifted_tensor_constant24, _lifted_tensor_constant12, _lifted_tensor_constant13, _lifted_tensor_constant14);  lowered_module_1 = p_pos_embed = dim_order_ops__to_dim_order_copy_default_3 = _lifted_tensor_constant0 = aten_unsqueeze_copy_default_1 = _lifted_tensor_constant1 = _lifted_tensor_constant2 = _lifted_tensor_constant22 = _lifted_tensor_constant23 = _lifted_tensor_constant24 = _lifted_tensor_constant12 = _lifted_tensor_constant13 = _lifted_tensor_constant14 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        getitem_1: \"f32[1, 1369, 384][525696, 384, 1]\" = executorch_call_delegate_1[0]\n        getitem_2: \"f32[16, 1][1, 1]\" = executorch_call_delegate_1[1]\n        getitem_3: \"f32[16][1]\" = executorch_call_delegate_1[2]\n        getitem_4: \"f32[32, 1][1, 1]\" = executorch_call_delegate_1[3]\n        getitem_5: \"f32[32, 1][1, 1]\" = executorch_call_delegate_1[4]\n        getitem_6: \"f32[32][1]\" = executorch_call_delegate_1[5]\n        getitem_7: \"f32[32][1]\" = executorch_call_delegate_1[6];  executorch_call_delegate_1 = None\n        aten_view_copy_default: \"f32[1, 37, 37, 384][525696, 14208, 384, 1]\" = executorch_exir_memory_view(getitem_1, [1, 37, 37, 384]);  getitem_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_10: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_4: \"i64[16, 1][1, 1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(getitem_2, dim_order = [0, 1], out = alloc_10);  getitem_2 = alloc_10 = None\n        \n        # No stacktrace found for following nodes\n        alloc_11: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_5: \"i64[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(getitem_3, dim_order = [0], out = alloc_11);  getitem_3 = alloc_11 = None\n        aten_view_copy_default_1: \"f32[2, 16, 1][16, 1, 1]\" = executorch_exir_memory_view(getitem_4, [2, 16, 1]);  getitem_4 = None\n        aten_view_copy_default_2: \"f32[2, 16, 1][16, 1, 1]\" = executorch_exir_memory_view(getitem_5, [2, 16, 1]);  getitem_5 = None\n        aten_view_copy_default_3: \"f32[2, 16][16, 1]\" = executorch_exir_memory_view(getitem_6, [2, 16]);  getitem_6 = None\n        aten_view_copy_default_4: \"f32[2, 16][16, 1]\" = executorch_exir_memory_view(getitem_7, [2, 16]);  getitem_7 = None\n        \n        # No stacktrace found for following nodes\n        alloc_12: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_sub_tensor: \"i64[16, 1][1, 1]\" = torch.ops.aten.sub.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant6, out = alloc_12);  _lifted_tensor_constant6 = alloc_12 = None\n        \n        # No stacktrace found for following nodes\n        alloc_13: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor: \"i64[16, 1][1, 1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant7, out = alloc_13);  _lifted_tensor_constant7 = alloc_13 = None\n        \n        # No stacktrace found for following nodes\n        alloc_14: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_1: \"i64[16, 1][1, 1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant8, out = alloc_14);  _lifted_tensor_constant8 = alloc_14 = None\n        \n        # No stacktrace found for following nodes\n        alloc_15: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_15);  alloc_15 = None\n        \n        # No stacktrace found for following nodes\n        alloc_16: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_1: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_16);  alloc_16 = None\n        \n        # No stacktrace found for following nodes\n        alloc_17: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_2: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_17);  alloc_17 = None\n        \n        # No stacktrace found for following nodes\n        alloc_18: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_3: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_18);  dim_order_ops__to_dim_order_copy_default_4 = alloc_18 = None\n        \n        # No stacktrace found for following nodes\n        alloc_19: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_sub_tensor_1: \"i64[16][1]\" = torch.ops.aten.sub.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant9, out = alloc_19);  _lifted_tensor_constant9 = alloc_19 = None\n        \n        # No stacktrace found for following nodes\n        alloc_20: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_2: \"i64[16][1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant10, out = alloc_20);  _lifted_tensor_constant10 = alloc_20 = None\n        \n        # No stacktrace found for following nodes\n        alloc_21: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_3: \"i64[16][1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant11, out = alloc_21);  _lifted_tensor_constant11 = alloc_21 = None\n        \n        # No stacktrace found for following nodes\n        alloc_22: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_4: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_22);  alloc_22 = None\n        \n        # No stacktrace found for following nodes\n        alloc_23: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_5: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_23);  alloc_23 = None\n        \n        # No stacktrace found for following nodes\n        alloc_24: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_6: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_24);  alloc_24 = None\n        \n        # No stacktrace found for following nodes\n        alloc_25: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_7: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_25);  dim_order_ops__to_dim_order_copy_default_5 = alloc_25 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_2 = self.lowered_module_2\n        executorch_call_delegate_2 = torch.ops.higher_order.executorch_call_delegate(lowered_module_2, aten_view_copy_default, aten_view_copy_default_3, _lifted_tensor_constant19, aten_view_copy_default_4, _lifted_tensor_constant15, aten_view_copy_default_1, _lifted_tensor_constant29, aten_view_copy_default_2, _lifted_tensor_constant25, _lifted_tensor_constant20, _lifted_tensor_constant16, _lifted_tensor_constant30, _lifted_tensor_constant26, _lifted_tensor_constant17, _lifted_tensor_constant27, dim_order_ops__to_dim_order_copy_default, dim_order_ops__to_dim_order_copy_default_1, _lifted_tensor_constant18, _lifted_tensor_constant28);  lowered_module_2 = aten_view_copy_default = aten_view_copy_default_3 = _lifted_tensor_constant19 = aten_view_copy_default_4 = _lifted_tensor_constant15 = aten_view_copy_default_1 = _lifted_tensor_constant29 = aten_view_copy_default_2 = _lifted_tensor_constant25 = _lifted_tensor_constant20 = _lifted_tensor_constant16 = _lifted_tensor_constant30 = _lifted_tensor_constant26 = _lifted_tensor_constant17 = _lifted_tensor_constant27 = dim_order_ops__to_dim_order_copy_default = dim_order_ops__to_dim_order_copy_default_1 = _lifted_tensor_constant18 = _lifted_tensor_constant28 = None\n        alloc_26: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_8: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_26);  alloc_26 = None\n        \n        # No stacktrace found for following nodes\n        alloc_27: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_9: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_27);  alloc_27 = None\n        \n        # No stacktrace found for following nodes\n        alloc_28: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_10: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_28);  alloc_28 = None\n        \n        # No stacktrace found for following nodes\n        alloc_29: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_11: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_29);  aten_sub_tensor = alloc_29 = None\n        \n        # No stacktrace found for following nodes\n        alloc_30: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_12: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_30);  alloc_30 = None\n        \n        # No stacktrace found for following nodes\n        alloc_31: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_13: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_31);  alloc_31 = None\n        \n        # No stacktrace found for following nodes\n        alloc_32: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_14: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_32);  alloc_32 = None\n        \n        # No stacktrace found for following nodes\n        alloc_33: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_15: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_33);  aten_add_tensor = alloc_33 = None\n        \n        # No stacktrace found for following nodes\n        alloc_34: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_16: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_34);  alloc_34 = None\n        \n        # No stacktrace found for following nodes\n        alloc_35: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_17: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_35);  alloc_35 = None\n        \n        # No stacktrace found for following nodes\n        alloc_36: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_18: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_36);  alloc_36 = None\n        \n        # No stacktrace found for following nodes\n        alloc_37: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_19: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_37);  aten_add_tensor_1 = alloc_37 = None\n        \n        # No stacktrace found for following nodes\n        alloc_38: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_20: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_38);  alloc_38 = None\n        \n        # No stacktrace found for following nodes\n        alloc_39: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_21: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_39);  alloc_39 = None\n        \n        # No stacktrace found for following nodes\n        alloc_40: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_22: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_40);  alloc_40 = None\n        \n        # No stacktrace found for following nodes\n        alloc_41: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_23: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_41);  aten_sub_tensor_1 = alloc_41 = None\n        \n        # No stacktrace found for following nodes\n        alloc_42: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_24: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_42);  alloc_42 = None\n        \n        # No stacktrace found for following nodes\n        alloc_43: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_25: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_43);  alloc_43 = None\n        \n        # No stacktrace found for following nodes\n        alloc_44: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_26: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_44);  alloc_44 = None\n        \n        # No stacktrace found for following nodes\n        alloc_45: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_27: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_45);  aten_add_tensor_2 = alloc_45 = None\n        \n        # No stacktrace found for following nodes\n        alloc_46: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_28: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_46);  alloc_46 = None\n        \n        # No stacktrace found for following nodes\n        alloc_47: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_29: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_47);  alloc_47 = None\n        \n        # No stacktrace found for following nodes\n        alloc_48: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_30: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_48);  alloc_48 = None\n        \n        # No stacktrace found for following nodes\n        alloc_49: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_31: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_49);  aten_add_tensor_3 = alloc_49 = None\n        getitem_8: \"f32[1, 384, 37, 37][525696, 1369, 37, 1]\" = executorch_call_delegate_2[0]\n        getitem_9: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[1]\n        getitem_10: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[2]\n        getitem_11: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[3]\n        getitem_12: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[4]\n        getitem_13: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[5]\n        getitem_14: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[6]\n        getitem_15: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[7]\n        getitem_16: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[8];  executorch_call_delegate_2 = None\n        \n        # No stacktrace found for following nodes\n        alloc_50: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_1, aten_clamp_default_5], out = alloc_50);  aten_clamp_default_1 = aten_clamp_default_5 = alloc_50 = None\n        \n        # No stacktrace found for following nodes\n        alloc_51: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_1: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default, aten_clamp_default_21], out = alloc_51);  aten_clamp_default = aten_clamp_default_21 = alloc_51 = None\n        \n        # No stacktrace found for following nodes\n        alloc_52: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_2: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_2, aten_clamp_default_25], out = alloc_52);  aten_clamp_default_2 = aten_clamp_default_25 = alloc_52 = None\n        \n        # No stacktrace found for following nodes\n        alloc_53: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_3: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_3, aten_clamp_default_29], out = alloc_53);  aten_clamp_default_3 = aten_clamp_default_29 = alloc_53 = None\n        \n        # No stacktrace found for following nodes\n        alloc_54: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_4: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_8, aten_clamp_default_20], out = alloc_54);  aten_clamp_default_8 = aten_clamp_default_20 = alloc_54 = None\n        \n        # No stacktrace found for following nodes\n        alloc_55: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_5: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_9, aten_clamp_default_4], out = alloc_55);  aten_clamp_default_9 = aten_clamp_default_4 = alloc_55 = None\n        \n        # No stacktrace found for following nodes\n        alloc_56: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_6: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_10, aten_clamp_default_24], out = alloc_56);  aten_clamp_default_10 = aten_clamp_default_24 = alloc_56 = None\n        \n        # No stacktrace found for following nodes\n        alloc_57: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_7: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_11, aten_clamp_default_28], out = alloc_57);  aten_clamp_default_11 = aten_clamp_default_28 = alloc_57 = None\n        \n        # No stacktrace found for following nodes\n        alloc_58: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_8: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_12, aten_clamp_default_22], out = alloc_58);  aten_clamp_default_12 = aten_clamp_default_22 = alloc_58 = None\n        \n        # No stacktrace found for following nodes\n        alloc_59: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_9: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_13, aten_clamp_default_6], out = alloc_59);  aten_clamp_default_13 = aten_clamp_default_6 = alloc_59 = None\n        \n        # No stacktrace found for following nodes\n        alloc_60: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_10: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_14, aten_clamp_default_26], out = alloc_60);  aten_clamp_default_14 = aten_clamp_default_26 = alloc_60 = None\n        \n        # No stacktrace found for following nodes\n        alloc_61: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_11: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_15, aten_clamp_default_30], out = alloc_61);  aten_clamp_default_15 = aten_clamp_default_30 = alloc_61 = None\n        \n        # No stacktrace found for following nodes\n        alloc_62: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_12: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_16, aten_clamp_default_23], out = alloc_62);  aten_clamp_default_16 = aten_clamp_default_23 = alloc_62 = None\n        \n        # No stacktrace found for following nodes\n        alloc_63: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_13: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_17, aten_clamp_default_7], out = alloc_63);  aten_clamp_default_17 = aten_clamp_default_7 = alloc_63 = None\n        \n        # No stacktrace found for following nodes\n        alloc_64: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_14: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_18, aten_clamp_default_27], out = alloc_64);  aten_clamp_default_18 = aten_clamp_default_27 = alloc_64 = None\n        \n        # No stacktrace found for following nodes\n        alloc_65: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_15: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_19, aten_clamp_default_31], out = alloc_65);  getitem_8 = aten_clamp_default_19 = aten_clamp_default_31 = alloc_65 = None\n        \n        # No stacktrace found for following nodes\n        alloc_66: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_9, [0], out = alloc_66);  getitem_9 = alloc_66 = None\n        \n        # No stacktrace found for following nodes\n        alloc_67: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_1: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_10, [0], out = alloc_67);  getitem_10 = alloc_67 = None\n        \n        # No stacktrace found for following nodes\n        alloc_68: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_2: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_11, [0], out = alloc_68);  getitem_11 = alloc_68 = None\n        \n        # No stacktrace found for following nodes\n        alloc_69: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_3: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_12, [0], out = alloc_69);  getitem_12 = alloc_69 = None\n        \n        # No stacktrace found for following nodes\n        alloc_70: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_4: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_13, [0], out = alloc_70);  getitem_13 = alloc_70 = None\n        \n        # No stacktrace found for following nodes\n        alloc_71: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_5: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_14, [0], out = alloc_71);  getitem_14 = alloc_71 = None\n        \n        # No stacktrace found for following nodes\n        alloc_72: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_6: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_15, [0], out = alloc_72);  getitem_15 = alloc_72 = None\n        \n        # No stacktrace found for following nodes\n        alloc_73: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_7: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_16, [0], out = alloc_73);  getitem_16 = alloc_73 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_3 = self.lowered_module_3\n        executorch_call_delegate_3 = torch.ops.higher_order.executorch_call_delegate(lowered_module_3, aten_index_tensor_5, aten_squeeze_copy_dims, aten_index_tensor, aten_index_tensor_9, aten_index_tensor_13, aten_index_tensor_6, aten_squeeze_copy_dims_1, aten_index_tensor_2, aten_index_tensor_10, aten_index_tensor_14, aten_index_tensor_4, aten_squeeze_copy_dims_4, aten_index_tensor_1, aten_index_tensor_8, aten_index_tensor_12, aten_index_tensor_7, aten_squeeze_copy_dims_5, aten_index_tensor_3, aten_index_tensor_11, aten_index_tensor_15, aten_squeeze_copy_dims_6, aten_squeeze_copy_dims_2, aten_squeeze_copy_dims_3, aten_squeeze_copy_dims_7);  lowered_module_3 = aten_index_tensor_5 = aten_squeeze_copy_dims = aten_index_tensor = aten_index_tensor_9 = aten_index_tensor_13 = aten_index_tensor_6 = aten_squeeze_copy_dims_1 = aten_index_tensor_2 = aten_index_tensor_10 = aten_index_tensor_14 = aten_index_tensor_4 = aten_squeeze_copy_dims_4 = aten_index_tensor_1 = aten_index_tensor_8 = aten_index_tensor_12 = aten_index_tensor_7 = aten_squeeze_copy_dims_5 = aten_index_tensor_3 = aten_index_tensor_11 = aten_index_tensor_15 = aten_squeeze_copy_dims_6 = aten_squeeze_copy_dims_2 = aten_squeeze_copy_dims_3 = aten_squeeze_copy_dims_7 = None\n        getitem_17: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_call_delegate_3[0];  executorch_call_delegate_3 = None\n        alloc_74: \"f32[1, 384, 16, 16][98304, 1, 6144, 384]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clone_default: \"f32[1, 384, 16, 16][98304, 1, 6144, 384]\" = torch.ops.aten.clone.out(getitem_17, memory_format = torch.channels_last, out = alloc_74);  getitem_17 = alloc_74 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_4 = self.lowered_module_4\n        executorch_call_delegate_4 = torch.ops.higher_order.executorch_call_delegate(lowered_module_4, aten_clone_default, args_0);  lowered_module_4 = aten_clone_default = args_0 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        getitem_18: \"f32[1, 16, 16, 384][98304, 6144, 384, 1]\" = executorch_call_delegate_4[0]\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        getitem_19: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_call_delegate_4[1];  executorch_call_delegate_4 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_view_copy_default_5: \"f32[1, 256, 384][98304, 384, 1]\" = executorch_exir_memory_view(getitem_18, [1, -1, 384]);  getitem_18 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:77 in forward, code: x = x.flatten(2).transpose(1, 2)  # B HW C\n        aten_view_copy_default_6: \"f32[1, 384, 256][98304, 256, 1]\" = executorch_exir_memory_view(getitem_19, [1, 384, 256]);  getitem_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_5 = self.lowered_module_5\n        executorch_call_delegate_5 = torch.ops.higher_order.executorch_call_delegate(lowered_module_5, aten_view_copy_default_6, aten_unsqueeze_copy_default, aten_view_copy_default_5, aten_expand_copy_default);  lowered_module_5 = aten_view_copy_default_6 = aten_unsqueeze_copy_default = aten_view_copy_default_5 = aten_expand_copy_default = None\n        getitem_20: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_5[0];  executorch_call_delegate_5 = None\n        alloc_75: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_76: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_77: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default = torch.ops.aten.native_layer_norm.out(getitem_20, [384], p_blocks_0_norm1_weight, p_blocks_0_norm1_bias, 1e-06, out0 = alloc_75, out1 = alloc_76, out2 = alloc_77);  p_blocks_0_norm1_weight = p_blocks_0_norm1_bias = alloc_75 = alloc_76 = alloc_77 = None\n        getitem_21: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default[0];  aten_native_layer_norm_default = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_6 = self.lowered_module_6\n        executorch_call_delegate_6 = torch.ops.higher_order.executorch_call_delegate(lowered_module_6, getitem_21);  lowered_module_6 = getitem_21 = None\n        getitem_22: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_6[0];  executorch_call_delegate_6 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_7: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_22, [1, 257, 3, 6, 64]);  getitem_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_7 = self.lowered_module_7\n        executorch_call_delegate_7 = torch.ops.higher_order.executorch_call_delegate(lowered_module_7, aten_view_copy_default_7);  lowered_module_7 = aten_view_copy_default_7 = None\n        getitem_23: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_7[0];  executorch_call_delegate_7 = None\n        alloc_78: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_1: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 0, out = alloc_78);  alloc_78 = None\n        \n        # No stacktrace found for following nodes\n        alloc_79: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_2: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 1, out = alloc_79);  alloc_79 = None\n        \n        # No stacktrace found for following nodes\n        alloc_80: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_3: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 2, out = alloc_80);  getitem_23 = alloc_80 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_8 = self.lowered_module_8\n        executorch_call_delegate_8 = torch.ops.higher_order.executorch_call_delegate(lowered_module_8, aten_select_copy_int_1, _lifted_tensor_constant32, aten_select_copy_int_2);  lowered_module_8 = aten_select_copy_int_1 = _lifted_tensor_constant32 = aten_select_copy_int_2 = None\n        alloc_81: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_1: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_3, [1, 6, 257, 64], out = alloc_81);  aten_select_copy_int_3 = alloc_81 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_24: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_8[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_25: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_8[1];  executorch_call_delegate_8 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_8: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_1, [6, 257, 64]);  aten_expand_copy_default_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_82: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_2: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_24, [1, 6, 257, 64], out = alloc_82);  getitem_24 = alloc_82 = None\n        \n        # No stacktrace found for following nodes\n        alloc_83: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_3: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_25, [1, 6, 64, 257], out = alloc_83);  getitem_25 = alloc_83 = None\n        aten_view_copy_default_9: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_2, [6, 257, 64]);  aten_expand_copy_default_2 = None\n        aten_view_copy_default_10: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_3, [6, 64, 257]);  aten_expand_copy_default_3 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_9 = self.lowered_module_9\n        executorch_call_delegate_9 = torch.ops.higher_order.executorch_call_delegate(lowered_module_9, aten_view_copy_default_9, aten_view_copy_default_10);  lowered_module_9 = aten_view_copy_default_9 = aten_view_copy_default_10 = None\n        getitem_26: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_9[0];  executorch_call_delegate_9 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_11: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_26, [1, 6, 257, 257]);  getitem_26 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_10 = self.lowered_module_10\n        executorch_call_delegate_10 = torch.ops.higher_order.executorch_call_delegate(lowered_module_10, aten_view_copy_default_11);  lowered_module_10 = aten_view_copy_default_11 = None\n        getitem_27: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_10[0];  executorch_call_delegate_10 = None\n        alloc_84: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_1: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_27, out = alloc_84);  getitem_27 = alloc_84 = None\n        \n        # No stacktrace found for following nodes\n        alloc_85: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_4: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_1, [1, 6, 257, 257], out = alloc_85);  aten_clone_default_1 = alloc_85 = None\n        aten_view_copy_default_12: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_4, [6, 257, 257]);  aten_expand_copy_default_4 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_11 = self.lowered_module_11\n        executorch_call_delegate_11 = torch.ops.higher_order.executorch_call_delegate(lowered_module_11, aten_view_copy_default_12, aten_view_copy_default_8);  lowered_module_11 = aten_view_copy_default_12 = aten_view_copy_default_8 = None\n        getitem_28: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_11[0];  executorch_call_delegate_11 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_28, [1, 6, 257, 64]);  getitem_28 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_12 = self.lowered_module_12\n        executorch_call_delegate_12 = torch.ops.higher_order.executorch_call_delegate(lowered_module_12, aten_view_copy_default_13);  lowered_module_12 = aten_view_copy_default_13 = None\n        getitem_29: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_12[0];  executorch_call_delegate_12 = None\n        alloc_86: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_2: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_29, memory_format = torch.contiguous_format, out = alloc_86);  getitem_29 = alloc_86 = None\n        aten_view_copy_default_14: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_2, [1, 257, 384]);  aten_clone_default_2 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_13 = self.lowered_module_13\n        executorch_call_delegate_13 = torch.ops.higher_order.executorch_call_delegate(lowered_module_13, aten_view_copy_default_14);  lowered_module_13 = aten_view_copy_default_14 = None\n        getitem_30: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_13[0];  executorch_call_delegate_13 = None\n        alloc_87: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_3: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_30, out = alloc_87);  getitem_30 = alloc_87 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_14 = self.lowered_module_14\n        executorch_call_delegate_14 = torch.ops.higher_order.executorch_call_delegate(lowered_module_14, aten_clone_default_3, p_blocks_0_ls1_gamma, getitem_20);  lowered_module_14 = aten_clone_default_3 = p_blocks_0_ls1_gamma = getitem_20 = None\n        getitem_31: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_14[0];  executorch_call_delegate_14 = None\n        alloc_88: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_89: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_90: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_1 = torch.ops.aten.native_layer_norm.out(getitem_31, [384], p_blocks_0_norm2_weight, p_blocks_0_norm2_bias, 1e-06, out0 = alloc_88, out1 = alloc_89, out2 = alloc_90);  p_blocks_0_norm2_weight = p_blocks_0_norm2_bias = alloc_88 = alloc_89 = alloc_90 = None\n        getitem_32: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_1[0];  aten_native_layer_norm_default_1 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_15 = self.lowered_module_15\n        executorch_call_delegate_15 = torch.ops.higher_order.executorch_call_delegate(lowered_module_15, getitem_32);  lowered_module_15 = getitem_32 = None\n        getitem_33: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_15[0];  executorch_call_delegate_15 = None\n        alloc_91: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_4: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_33, out = alloc_91);  getitem_33 = alloc_91 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_16 = self.lowered_module_16\n        executorch_call_delegate_16 = torch.ops.higher_order.executorch_call_delegate(lowered_module_16, aten_clone_default_4);  lowered_module_16 = aten_clone_default_4 = None\n        getitem_34: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_16[0];  executorch_call_delegate_16 = None\n        alloc_92: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_5: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_34, out = alloc_92);  getitem_34 = alloc_92 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_17 = self.lowered_module_17\n        executorch_call_delegate_17 = torch.ops.higher_order.executorch_call_delegate(lowered_module_17, aten_clone_default_5, p_blocks_0_ls2_gamma, getitem_31);  lowered_module_17 = aten_clone_default_5 = p_blocks_0_ls2_gamma = getitem_31 = None\n        getitem_35: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_17[0];  executorch_call_delegate_17 = None\n        alloc_93: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_94: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_95: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_2 = torch.ops.aten.native_layer_norm.out(getitem_35, [384], p_blocks_1_norm1_weight, p_blocks_1_norm1_bias, 1e-06, out0 = alloc_93, out1 = alloc_94, out2 = alloc_95);  p_blocks_1_norm1_weight = p_blocks_1_norm1_bias = alloc_93 = alloc_94 = alloc_95 = None\n        getitem_36: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_2[0];  aten_native_layer_norm_default_2 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_18 = self.lowered_module_18\n        executorch_call_delegate_18 = torch.ops.higher_order.executorch_call_delegate(lowered_module_18, getitem_36);  lowered_module_18 = getitem_36 = None\n        getitem_37: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_18[0];  executorch_call_delegate_18 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_15: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_37, [1, 257, 3, 6, 64]);  getitem_37 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_19 = self.lowered_module_19\n        executorch_call_delegate_19 = torch.ops.higher_order.executorch_call_delegate(lowered_module_19, aten_view_copy_default_15);  lowered_module_19 = aten_view_copy_default_15 = None\n        getitem_38: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_19[0];  executorch_call_delegate_19 = None\n        alloc_96: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_4: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 0, out = alloc_96);  alloc_96 = None\n        \n        # No stacktrace found for following nodes\n        alloc_97: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_5: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 1, out = alloc_97);  alloc_97 = None\n        \n        # No stacktrace found for following nodes\n        alloc_98: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_6: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 2, out = alloc_98);  getitem_38 = alloc_98 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_20 = self.lowered_module_20\n        executorch_call_delegate_20 = torch.ops.higher_order.executorch_call_delegate(lowered_module_20, aten_select_copy_int_4, _lifted_tensor_constant33, aten_select_copy_int_5);  lowered_module_20 = aten_select_copy_int_4 = _lifted_tensor_constant33 = aten_select_copy_int_5 = None\n        alloc_99: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_5: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_6, [1, 6, 257, 64], out = alloc_99);  aten_select_copy_int_6 = alloc_99 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_39: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_20[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_40: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_20[1];  executorch_call_delegate_20 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_16: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_5, [6, 257, 64]);  aten_expand_copy_default_5 = None\n        \n        # No stacktrace found for following nodes\n        alloc_100: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_6: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_39, [1, 6, 257, 64], out = alloc_100);  getitem_39 = alloc_100 = None\n        \n        # No stacktrace found for following nodes\n        alloc_101: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_7: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_40, [1, 6, 64, 257], out = alloc_101);  getitem_40 = alloc_101 = None\n        aten_view_copy_default_17: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_6, [6, 257, 64]);  aten_expand_copy_default_6 = None\n        aten_view_copy_default_18: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_7, [6, 64, 257]);  aten_expand_copy_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_21 = self.lowered_module_21\n        executorch_call_delegate_21 = torch.ops.higher_order.executorch_call_delegate(lowered_module_21, aten_view_copy_default_17, aten_view_copy_default_18);  lowered_module_21 = aten_view_copy_default_17 = aten_view_copy_default_18 = None\n        getitem_41: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_21[0];  executorch_call_delegate_21 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_19: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_41, [1, 6, 257, 257]);  getitem_41 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_22 = self.lowered_module_22\n        executorch_call_delegate_22 = torch.ops.higher_order.executorch_call_delegate(lowered_module_22, aten_view_copy_default_19);  lowered_module_22 = aten_view_copy_default_19 = None\n        getitem_42: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_22[0];  executorch_call_delegate_22 = None\n        alloc_102: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_6: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_42, out = alloc_102);  getitem_42 = alloc_102 = None\n        \n        # No stacktrace found for following nodes\n        alloc_103: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_8: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_6, [1, 6, 257, 257], out = alloc_103);  aten_clone_default_6 = alloc_103 = None\n        aten_view_copy_default_20: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_8, [6, 257, 257]);  aten_expand_copy_default_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_23 = self.lowered_module_23\n        executorch_call_delegate_23 = torch.ops.higher_order.executorch_call_delegate(lowered_module_23, aten_view_copy_default_20, aten_view_copy_default_16);  lowered_module_23 = aten_view_copy_default_20 = aten_view_copy_default_16 = None\n        getitem_43: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_23[0];  executorch_call_delegate_23 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_43, [1, 6, 257, 64]);  getitem_43 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_24 = self.lowered_module_24\n        executorch_call_delegate_24 = torch.ops.higher_order.executorch_call_delegate(lowered_module_24, aten_view_copy_default_21);  lowered_module_24 = aten_view_copy_default_21 = None\n        getitem_44: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_24[0];  executorch_call_delegate_24 = None\n        alloc_104: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_7: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_44, memory_format = torch.contiguous_format, out = alloc_104);  getitem_44 = alloc_104 = None\n        aten_view_copy_default_22: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_7, [1, 257, 384]);  aten_clone_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_25 = self.lowered_module_25\n        executorch_call_delegate_25 = torch.ops.higher_order.executorch_call_delegate(lowered_module_25, aten_view_copy_default_22);  lowered_module_25 = aten_view_copy_default_22 = None\n        getitem_45: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_25[0];  executorch_call_delegate_25 = None\n        alloc_105: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_8: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_45, out = alloc_105);  getitem_45 = alloc_105 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_26 = self.lowered_module_26\n        executorch_call_delegate_26 = torch.ops.higher_order.executorch_call_delegate(lowered_module_26, aten_clone_default_8, p_blocks_1_ls1_gamma, getitem_35);  lowered_module_26 = aten_clone_default_8 = p_blocks_1_ls1_gamma = getitem_35 = None\n        getitem_46: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_26[0];  executorch_call_delegate_26 = None\n        alloc_106: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_107: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_108: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_3 = torch.ops.aten.native_layer_norm.out(getitem_46, [384], p_blocks_1_norm2_weight, p_blocks_1_norm2_bias, 1e-06, out0 = alloc_106, out1 = alloc_107, out2 = alloc_108);  p_blocks_1_norm2_weight = p_blocks_1_norm2_bias = alloc_106 = alloc_107 = alloc_108 = None\n        getitem_47: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_3[0];  aten_native_layer_norm_default_3 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_27 = self.lowered_module_27\n        executorch_call_delegate_27 = torch.ops.higher_order.executorch_call_delegate(lowered_module_27, getitem_47);  lowered_module_27 = getitem_47 = None\n        getitem_48: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_27[0];  executorch_call_delegate_27 = None\n        alloc_109: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_9: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_48, out = alloc_109);  getitem_48 = alloc_109 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_28 = self.lowered_module_28\n        executorch_call_delegate_28 = torch.ops.higher_order.executorch_call_delegate(lowered_module_28, aten_clone_default_9);  lowered_module_28 = aten_clone_default_9 = None\n        getitem_49: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_28[0];  executorch_call_delegate_28 = None\n        alloc_110: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_10: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_49, out = alloc_110);  getitem_49 = alloc_110 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_29 = self.lowered_module_29\n        executorch_call_delegate_29 = torch.ops.higher_order.executorch_call_delegate(lowered_module_29, aten_clone_default_10, p_blocks_1_ls2_gamma, getitem_46);  lowered_module_29 = aten_clone_default_10 = p_blocks_1_ls2_gamma = getitem_46 = None\n        getitem_50: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_29[0];  executorch_call_delegate_29 = None\n        alloc_111: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_112: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_113: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_4 = torch.ops.aten.native_layer_norm.out(getitem_50, [384], p_blocks_2_norm1_weight, p_blocks_2_norm1_bias, 1e-06, out0 = alloc_111, out1 = alloc_112, out2 = alloc_113);  p_blocks_2_norm1_weight = p_blocks_2_norm1_bias = alloc_111 = alloc_112 = alloc_113 = None\n        getitem_51: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_4[0];  aten_native_layer_norm_default_4 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_30 = self.lowered_module_30\n        executorch_call_delegate_30 = torch.ops.higher_order.executorch_call_delegate(lowered_module_30, getitem_51);  lowered_module_30 = getitem_51 = None\n        getitem_52: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_30[0];  executorch_call_delegate_30 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_23: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_52, [1, 257, 3, 6, 64]);  getitem_52 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_31 = self.lowered_module_31\n        executorch_call_delegate_31 = torch.ops.higher_order.executorch_call_delegate(lowered_module_31, aten_view_copy_default_23);  lowered_module_31 = aten_view_copy_default_23 = None\n        getitem_53: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_31[0];  executorch_call_delegate_31 = None\n        alloc_114: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_7: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 0, out = alloc_114);  alloc_114 = None\n        \n        # No stacktrace found for following nodes\n        alloc_115: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_8: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 1, out = alloc_115);  alloc_115 = None\n        \n        # No stacktrace found for following nodes\n        alloc_116: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_9: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 2, out = alloc_116);  getitem_53 = alloc_116 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_32 = self.lowered_module_32\n        executorch_call_delegate_32 = torch.ops.higher_order.executorch_call_delegate(lowered_module_32, aten_select_copy_int_7, _lifted_tensor_constant34, aten_select_copy_int_8);  lowered_module_32 = aten_select_copy_int_7 = _lifted_tensor_constant34 = aten_select_copy_int_8 = None\n        alloc_117: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_9: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_9, [1, 6, 257, 64], out = alloc_117);  aten_select_copy_int_9 = alloc_117 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_54: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_32[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_55: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_32[1];  executorch_call_delegate_32 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_24: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_9, [6, 257, 64]);  aten_expand_copy_default_9 = None\n        \n        # No stacktrace found for following nodes\n        alloc_118: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_10: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_54, [1, 6, 257, 64], out = alloc_118);  getitem_54 = alloc_118 = None\n        \n        # No stacktrace found for following nodes\n        alloc_119: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_11: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_55, [1, 6, 64, 257], out = alloc_119);  getitem_55 = alloc_119 = None\n        aten_view_copy_default_25: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_10, [6, 257, 64]);  aten_expand_copy_default_10 = None\n        aten_view_copy_default_26: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_11, [6, 64, 257]);  aten_expand_copy_default_11 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_33 = self.lowered_module_33\n        executorch_call_delegate_33 = torch.ops.higher_order.executorch_call_delegate(lowered_module_33, aten_view_copy_default_25, aten_view_copy_default_26);  lowered_module_33 = aten_view_copy_default_25 = aten_view_copy_default_26 = None\n        getitem_56: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_33[0];  executorch_call_delegate_33 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_27: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_56, [1, 6, 257, 257]);  getitem_56 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_34 = self.lowered_module_34\n        executorch_call_delegate_34 = torch.ops.higher_order.executorch_call_delegate(lowered_module_34, aten_view_copy_default_27);  lowered_module_34 = aten_view_copy_default_27 = None\n        getitem_57: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_34[0];  executorch_call_delegate_34 = None\n        alloc_120: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_11: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_57, out = alloc_120);  getitem_57 = alloc_120 = None\n        \n        # No stacktrace found for following nodes\n        alloc_121: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_12: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_11, [1, 6, 257, 257], out = alloc_121);  aten_clone_default_11 = alloc_121 = None\n        aten_view_copy_default_28: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_12, [6, 257, 257]);  aten_expand_copy_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_35 = self.lowered_module_35\n        executorch_call_delegate_35 = torch.ops.higher_order.executorch_call_delegate(lowered_module_35, aten_view_copy_default_28, aten_view_copy_default_24);  lowered_module_35 = aten_view_copy_default_28 = aten_view_copy_default_24 = None\n        getitem_58: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_35[0];  executorch_call_delegate_35 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_58, [1, 6, 257, 64]);  getitem_58 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_36 = self.lowered_module_36\n        executorch_call_delegate_36 = torch.ops.higher_order.executorch_call_delegate(lowered_module_36, aten_view_copy_default_29);  lowered_module_36 = aten_view_copy_default_29 = None\n        getitem_59: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_36[0];  executorch_call_delegate_36 = None\n        alloc_122: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_12: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_59, memory_format = torch.contiguous_format, out = alloc_122);  getitem_59 = alloc_122 = None\n        aten_view_copy_default_30: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_12, [1, 257, 384]);  aten_clone_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_37 = self.lowered_module_37\n        executorch_call_delegate_37 = torch.ops.higher_order.executorch_call_delegate(lowered_module_37, aten_view_copy_default_30);  lowered_module_37 = aten_view_copy_default_30 = None\n        getitem_60: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_37[0];  executorch_call_delegate_37 = None\n        alloc_123: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_13: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_60, out = alloc_123);  getitem_60 = alloc_123 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_38 = self.lowered_module_38\n        executorch_call_delegate_38 = torch.ops.higher_order.executorch_call_delegate(lowered_module_38, aten_clone_default_13, p_blocks_2_ls1_gamma, getitem_50);  lowered_module_38 = aten_clone_default_13 = p_blocks_2_ls1_gamma = getitem_50 = None\n        getitem_61: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_38[0];  executorch_call_delegate_38 = None\n        alloc_124: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_125: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_126: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_5 = torch.ops.aten.native_layer_norm.out(getitem_61, [384], p_blocks_2_norm2_weight, p_blocks_2_norm2_bias, 1e-06, out0 = alloc_124, out1 = alloc_125, out2 = alloc_126);  p_blocks_2_norm2_weight = p_blocks_2_norm2_bias = alloc_124 = alloc_125 = alloc_126 = None\n        getitem_62: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_5[0];  aten_native_layer_norm_default_5 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_39 = self.lowered_module_39\n        executorch_call_delegate_39 = torch.ops.higher_order.executorch_call_delegate(lowered_module_39, getitem_62);  lowered_module_39 = getitem_62 = None\n        getitem_63: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_39[0];  executorch_call_delegate_39 = None\n        alloc_127: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_14: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_63, out = alloc_127);  getitem_63 = alloc_127 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_40 = self.lowered_module_40\n        executorch_call_delegate_40 = torch.ops.higher_order.executorch_call_delegate(lowered_module_40, aten_clone_default_14);  lowered_module_40 = aten_clone_default_14 = None\n        getitem_64: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_40[0];  executorch_call_delegate_40 = None\n        alloc_128: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_15: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_64, out = alloc_128);  getitem_64 = alloc_128 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_41 = self.lowered_module_41\n        executorch_call_delegate_41 = torch.ops.higher_order.executorch_call_delegate(lowered_module_41, aten_clone_default_15, p_blocks_2_ls2_gamma, getitem_61);  lowered_module_41 = aten_clone_default_15 = p_blocks_2_ls2_gamma = getitem_61 = None\n        getitem_65: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_41[0];  executorch_call_delegate_41 = None\n        alloc_129: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_130: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_131: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_6 = torch.ops.aten.native_layer_norm.out(getitem_65, [384], p_blocks_3_norm1_weight, p_blocks_3_norm1_bias, 1e-06, out0 = alloc_129, out1 = alloc_130, out2 = alloc_131);  p_blocks_3_norm1_weight = p_blocks_3_norm1_bias = alloc_129 = alloc_130 = alloc_131 = None\n        getitem_66: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_6[0];  aten_native_layer_norm_default_6 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_42 = self.lowered_module_42\n        executorch_call_delegate_42 = torch.ops.higher_order.executorch_call_delegate(lowered_module_42, getitem_66);  lowered_module_42 = getitem_66 = None\n        getitem_67: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_42[0];  executorch_call_delegate_42 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_31: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_67, [1, 257, 3, 6, 64]);  getitem_67 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_43 = self.lowered_module_43\n        executorch_call_delegate_43 = torch.ops.higher_order.executorch_call_delegate(lowered_module_43, aten_view_copy_default_31);  lowered_module_43 = aten_view_copy_default_31 = None\n        getitem_68: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_43[0];  executorch_call_delegate_43 = None\n        alloc_132: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_10: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 0, out = alloc_132);  alloc_132 = None\n        \n        # No stacktrace found for following nodes\n        alloc_133: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_11: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 1, out = alloc_133);  alloc_133 = None\n        \n        # No stacktrace found for following nodes\n        alloc_134: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_12: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 2, out = alloc_134);  getitem_68 = alloc_134 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_44 = self.lowered_module_44\n        executorch_call_delegate_44 = torch.ops.higher_order.executorch_call_delegate(lowered_module_44, aten_select_copy_int_10, _lifted_tensor_constant35, aten_select_copy_int_11);  lowered_module_44 = aten_select_copy_int_10 = _lifted_tensor_constant35 = aten_select_copy_int_11 = None\n        alloc_135: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_12, [1, 6, 257, 64], out = alloc_135);  aten_select_copy_int_12 = alloc_135 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_69: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_44[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_70: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_44[1];  executorch_call_delegate_44 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_32: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_13, [6, 257, 64]);  aten_expand_copy_default_13 = None\n        \n        # No stacktrace found for following nodes\n        alloc_136: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_14: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_69, [1, 6, 257, 64], out = alloc_136);  getitem_69 = alloc_136 = None\n        \n        # No stacktrace found for following nodes\n        alloc_137: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_15: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_70, [1, 6, 64, 257], out = alloc_137);  getitem_70 = alloc_137 = None\n        aten_view_copy_default_33: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_14, [6, 257, 64]);  aten_expand_copy_default_14 = None\n        aten_view_copy_default_34: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_15, [6, 64, 257]);  aten_expand_copy_default_15 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_45 = self.lowered_module_45\n        executorch_call_delegate_45 = torch.ops.higher_order.executorch_call_delegate(lowered_module_45, aten_view_copy_default_33, aten_view_copy_default_34);  lowered_module_45 = aten_view_copy_default_33 = aten_view_copy_default_34 = None\n        getitem_71: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_45[0];  executorch_call_delegate_45 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_35: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_71, [1, 6, 257, 257]);  getitem_71 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_46 = self.lowered_module_46\n        executorch_call_delegate_46 = torch.ops.higher_order.executorch_call_delegate(lowered_module_46, aten_view_copy_default_35);  lowered_module_46 = aten_view_copy_default_35 = None\n        getitem_72: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_46[0];  executorch_call_delegate_46 = None\n        alloc_138: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_16: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_72, out = alloc_138);  getitem_72 = alloc_138 = None\n        \n        # No stacktrace found for following nodes\n        alloc_139: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_16: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_16, [1, 6, 257, 257], out = alloc_139);  aten_clone_default_16 = alloc_139 = None\n        aten_view_copy_default_36: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_16, [6, 257, 257]);  aten_expand_copy_default_16 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_47 = self.lowered_module_47\n        executorch_call_delegate_47 = torch.ops.higher_order.executorch_call_delegate(lowered_module_47, aten_view_copy_default_36, aten_view_copy_default_32);  lowered_module_47 = aten_view_copy_default_36 = aten_view_copy_default_32 = None\n        getitem_73: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_47[0];  executorch_call_delegate_47 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_37: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_73, [1, 6, 257, 64]);  getitem_73 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_48 = self.lowered_module_48\n        executorch_call_delegate_48 = torch.ops.higher_order.executorch_call_delegate(lowered_module_48, aten_view_copy_default_37);  lowered_module_48 = aten_view_copy_default_37 = None\n        getitem_74: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_48[0];  executorch_call_delegate_48 = None\n        alloc_140: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_17: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_74, memory_format = torch.contiguous_format, out = alloc_140);  getitem_74 = alloc_140 = None\n        aten_view_copy_default_38: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_17, [1, 257, 384]);  aten_clone_default_17 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_49 = self.lowered_module_49\n        executorch_call_delegate_49 = torch.ops.higher_order.executorch_call_delegate(lowered_module_49, aten_view_copy_default_38);  lowered_module_49 = aten_view_copy_default_38 = None\n        getitem_75: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_49[0];  executorch_call_delegate_49 = None\n        alloc_141: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_18: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_75, out = alloc_141);  getitem_75 = alloc_141 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_50 = self.lowered_module_50\n        executorch_call_delegate_50 = torch.ops.higher_order.executorch_call_delegate(lowered_module_50, aten_clone_default_18, p_blocks_3_ls1_gamma, getitem_65);  lowered_module_50 = aten_clone_default_18 = p_blocks_3_ls1_gamma = getitem_65 = None\n        getitem_76: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_50[0];  executorch_call_delegate_50 = None\n        alloc_142: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_143: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_144: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_7 = torch.ops.aten.native_layer_norm.out(getitem_76, [384], p_blocks_3_norm2_weight, p_blocks_3_norm2_bias, 1e-06, out0 = alloc_142, out1 = alloc_143, out2 = alloc_144);  p_blocks_3_norm2_weight = p_blocks_3_norm2_bias = alloc_142 = alloc_143 = alloc_144 = None\n        getitem_77: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_7[0];  aten_native_layer_norm_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_51 = self.lowered_module_51\n        executorch_call_delegate_51 = torch.ops.higher_order.executorch_call_delegate(lowered_module_51, getitem_77);  lowered_module_51 = getitem_77 = None\n        getitem_78: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_51[0];  executorch_call_delegate_51 = None\n        alloc_145: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_19: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_78, out = alloc_145);  getitem_78 = alloc_145 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_52 = self.lowered_module_52\n        executorch_call_delegate_52 = torch.ops.higher_order.executorch_call_delegate(lowered_module_52, aten_clone_default_19);  lowered_module_52 = aten_clone_default_19 = None\n        getitem_79: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_52[0];  executorch_call_delegate_52 = None\n        alloc_146: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_20: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_79, out = alloc_146);  getitem_79 = alloc_146 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_53 = self.lowered_module_53\n        executorch_call_delegate_53 = torch.ops.higher_order.executorch_call_delegate(lowered_module_53, aten_clone_default_20, p_blocks_3_ls2_gamma, getitem_76);  lowered_module_53 = aten_clone_default_20 = p_blocks_3_ls2_gamma = getitem_76 = None\n        getitem_80: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_53[0];  executorch_call_delegate_53 = None\n        alloc_147: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_148: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_149: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_8 = torch.ops.aten.native_layer_norm.out(getitem_80, [384], p_blocks_4_norm1_weight, p_blocks_4_norm1_bias, 1e-06, out0 = alloc_147, out1 = alloc_148, out2 = alloc_149);  p_blocks_4_norm1_weight = p_blocks_4_norm1_bias = alloc_147 = alloc_148 = alloc_149 = None\n        getitem_81: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_8[0];  aten_native_layer_norm_default_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_54 = self.lowered_module_54\n        executorch_call_delegate_54 = torch.ops.higher_order.executorch_call_delegate(lowered_module_54, getitem_81);  lowered_module_54 = getitem_81 = None\n        getitem_82: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_54[0];  executorch_call_delegate_54 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_39: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_82, [1, 257, 3, 6, 64]);  getitem_82 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_55 = self.lowered_module_55\n        executorch_call_delegate_55 = torch.ops.higher_order.executorch_call_delegate(lowered_module_55, aten_view_copy_default_39);  lowered_module_55 = aten_view_copy_default_39 = None\n        getitem_83: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_55[0];  executorch_call_delegate_55 = None\n        alloc_150: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 0, out = alloc_150);  alloc_150 = None\n        \n        # No stacktrace found for following nodes\n        alloc_151: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_14: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 1, out = alloc_151);  alloc_151 = None\n        \n        # No stacktrace found for following nodes\n        alloc_152: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_15: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 2, out = alloc_152);  getitem_83 = alloc_152 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_56 = self.lowered_module_56\n        executorch_call_delegate_56 = torch.ops.higher_order.executorch_call_delegate(lowered_module_56, aten_select_copy_int_13, _lifted_tensor_constant36, aten_select_copy_int_14);  lowered_module_56 = aten_select_copy_int_13 = _lifted_tensor_constant36 = aten_select_copy_int_14 = None\n        alloc_153: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_17: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_15, [1, 6, 257, 64], out = alloc_153);  aten_select_copy_int_15 = alloc_153 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_84: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_56[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_85: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_56[1];  executorch_call_delegate_56 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_40: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_17, [6, 257, 64]);  aten_expand_copy_default_17 = None\n        \n        # No stacktrace found for following nodes\n        alloc_154: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_18: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_84, [1, 6, 257, 64], out = alloc_154);  getitem_84 = alloc_154 = None\n        \n        # No stacktrace found for following nodes\n        alloc_155: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_19: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_85, [1, 6, 64, 257], out = alloc_155);  getitem_85 = alloc_155 = None\n        aten_view_copy_default_41: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_18, [6, 257, 64]);  aten_expand_copy_default_18 = None\n        aten_view_copy_default_42: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_19, [6, 64, 257]);  aten_expand_copy_default_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_57 = self.lowered_module_57\n        executorch_call_delegate_57 = torch.ops.higher_order.executorch_call_delegate(lowered_module_57, aten_view_copy_default_41, aten_view_copy_default_42);  lowered_module_57 = aten_view_copy_default_41 = aten_view_copy_default_42 = None\n        getitem_86: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_57[0];  executorch_call_delegate_57 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_43: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_86, [1, 6, 257, 257]);  getitem_86 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_58 = self.lowered_module_58\n        executorch_call_delegate_58 = torch.ops.higher_order.executorch_call_delegate(lowered_module_58, aten_view_copy_default_43);  lowered_module_58 = aten_view_copy_default_43 = None\n        getitem_87: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_58[0];  executorch_call_delegate_58 = None\n        alloc_156: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_21: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_87, out = alloc_156);  getitem_87 = alloc_156 = None\n        \n        # No stacktrace found for following nodes\n        alloc_157: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_20: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_21, [1, 6, 257, 257], out = alloc_157);  aten_clone_default_21 = alloc_157 = None\n        aten_view_copy_default_44: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_20, [6, 257, 257]);  aten_expand_copy_default_20 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_59 = self.lowered_module_59\n        executorch_call_delegate_59 = torch.ops.higher_order.executorch_call_delegate(lowered_module_59, aten_view_copy_default_44, aten_view_copy_default_40);  lowered_module_59 = aten_view_copy_default_44 = aten_view_copy_default_40 = None\n        getitem_88: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_59[0];  executorch_call_delegate_59 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_45: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_88, [1, 6, 257, 64]);  getitem_88 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_60 = self.lowered_module_60\n        executorch_call_delegate_60 = torch.ops.higher_order.executorch_call_delegate(lowered_module_60, aten_view_copy_default_45);  lowered_module_60 = aten_view_copy_default_45 = None\n        getitem_89: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_60[0];  executorch_call_delegate_60 = None\n        alloc_158: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_22: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_89, memory_format = torch.contiguous_format, out = alloc_158);  getitem_89 = alloc_158 = None\n        aten_view_copy_default_46: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_22, [1, 257, 384]);  aten_clone_default_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_61 = self.lowered_module_61\n        executorch_call_delegate_61 = torch.ops.higher_order.executorch_call_delegate(lowered_module_61, aten_view_copy_default_46);  lowered_module_61 = aten_view_copy_default_46 = None\n        getitem_90: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_61[0];  executorch_call_delegate_61 = None\n        alloc_159: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_23: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_90, out = alloc_159);  getitem_90 = alloc_159 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_62 = self.lowered_module_62\n        executorch_call_delegate_62 = torch.ops.higher_order.executorch_call_delegate(lowered_module_62, aten_clone_default_23, p_blocks_4_ls1_gamma, getitem_80);  lowered_module_62 = aten_clone_default_23 = p_blocks_4_ls1_gamma = getitem_80 = None\n        getitem_91: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_62[0];  executorch_call_delegate_62 = None\n        alloc_160: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_161: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_162: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_9 = torch.ops.aten.native_layer_norm.out(getitem_91, [384], p_blocks_4_norm2_weight, p_blocks_4_norm2_bias, 1e-06, out0 = alloc_160, out1 = alloc_161, out2 = alloc_162);  p_blocks_4_norm2_weight = p_blocks_4_norm2_bias = alloc_160 = alloc_161 = alloc_162 = None\n        getitem_92: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_9[0];  aten_native_layer_norm_default_9 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_63 = self.lowered_module_63\n        executorch_call_delegate_63 = torch.ops.higher_order.executorch_call_delegate(lowered_module_63, getitem_92);  lowered_module_63 = getitem_92 = None\n        getitem_93: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_63[0];  executorch_call_delegate_63 = None\n        alloc_163: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_24: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_93, out = alloc_163);  getitem_93 = alloc_163 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_64 = self.lowered_module_64\n        executorch_call_delegate_64 = torch.ops.higher_order.executorch_call_delegate(lowered_module_64, aten_clone_default_24);  lowered_module_64 = aten_clone_default_24 = None\n        getitem_94: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_64[0];  executorch_call_delegate_64 = None\n        alloc_164: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_25: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_94, out = alloc_164);  getitem_94 = alloc_164 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_65 = self.lowered_module_65\n        executorch_call_delegate_65 = torch.ops.higher_order.executorch_call_delegate(lowered_module_65, aten_clone_default_25, p_blocks_4_ls2_gamma, getitem_91);  lowered_module_65 = aten_clone_default_25 = p_blocks_4_ls2_gamma = getitem_91 = None\n        getitem_95: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_65[0];  executorch_call_delegate_65 = None\n        alloc_165: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_166: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_167: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_10 = torch.ops.aten.native_layer_norm.out(getitem_95, [384], p_blocks_5_norm1_weight, p_blocks_5_norm1_bias, 1e-06, out0 = alloc_165, out1 = alloc_166, out2 = alloc_167);  p_blocks_5_norm1_weight = p_blocks_5_norm1_bias = alloc_165 = alloc_166 = alloc_167 = None\n        getitem_96: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_10[0];  aten_native_layer_norm_default_10 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_66 = self.lowered_module_66\n        executorch_call_delegate_66 = torch.ops.higher_order.executorch_call_delegate(lowered_module_66, getitem_96);  lowered_module_66 = getitem_96 = None\n        getitem_97: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_66[0];  executorch_call_delegate_66 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_47: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_97, [1, 257, 3, 6, 64]);  getitem_97 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_67 = self.lowered_module_67\n        executorch_call_delegate_67 = torch.ops.higher_order.executorch_call_delegate(lowered_module_67, aten_view_copy_default_47);  lowered_module_67 = aten_view_copy_default_47 = None\n        getitem_98: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_67[0];  executorch_call_delegate_67 = None\n        alloc_168: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_16: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 0, out = alloc_168);  alloc_168 = None\n        \n        # No stacktrace found for following nodes\n        alloc_169: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_17: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 1, out = alloc_169);  alloc_169 = None\n        \n        # No stacktrace found for following nodes\n        alloc_170: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_18: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 2, out = alloc_170);  getitem_98 = alloc_170 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_68 = self.lowered_module_68\n        executorch_call_delegate_68 = torch.ops.higher_order.executorch_call_delegate(lowered_module_68, aten_select_copy_int_16, _lifted_tensor_constant37, aten_select_copy_int_17);  lowered_module_68 = aten_select_copy_int_16 = _lifted_tensor_constant37 = aten_select_copy_int_17 = None\n        alloc_171: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_18, [1, 6, 257, 64], out = alloc_171);  aten_select_copy_int_18 = alloc_171 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_99: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_68[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_100: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_68[1];  executorch_call_delegate_68 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_48: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_21, [6, 257, 64]);  aten_expand_copy_default_21 = None\n        \n        # No stacktrace found for following nodes\n        alloc_172: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_22: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_99, [1, 6, 257, 64], out = alloc_172);  getitem_99 = alloc_172 = None\n        \n        # No stacktrace found for following nodes\n        alloc_173: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_23: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_100, [1, 6, 64, 257], out = alloc_173);  getitem_100 = alloc_173 = None\n        aten_view_copy_default_49: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_22, [6, 257, 64]);  aten_expand_copy_default_22 = None\n        aten_view_copy_default_50: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_23, [6, 64, 257]);  aten_expand_copy_default_23 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_69 = self.lowered_module_69\n        executorch_call_delegate_69 = torch.ops.higher_order.executorch_call_delegate(lowered_module_69, aten_view_copy_default_49, aten_view_copy_default_50);  lowered_module_69 = aten_view_copy_default_49 = aten_view_copy_default_50 = None\n        getitem_101: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_69[0];  executorch_call_delegate_69 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_51: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_101, [1, 6, 257, 257]);  getitem_101 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_70 = self.lowered_module_70\n        executorch_call_delegate_70 = torch.ops.higher_order.executorch_call_delegate(lowered_module_70, aten_view_copy_default_51);  lowered_module_70 = aten_view_copy_default_51 = None\n        getitem_102: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_70[0];  executorch_call_delegate_70 = None\n        alloc_174: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_26: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_102, out = alloc_174);  getitem_102 = alloc_174 = None\n        \n        # No stacktrace found for following nodes\n        alloc_175: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_24: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_26, [1, 6, 257, 257], out = alloc_175);  aten_clone_default_26 = alloc_175 = None\n        aten_view_copy_default_52: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_24, [6, 257, 257]);  aten_expand_copy_default_24 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_71 = self.lowered_module_71\n        executorch_call_delegate_71 = torch.ops.higher_order.executorch_call_delegate(lowered_module_71, aten_view_copy_default_52, aten_view_copy_default_48);  lowered_module_71 = aten_view_copy_default_52 = aten_view_copy_default_48 = None\n        getitem_103: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_71[0];  executorch_call_delegate_71 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_53: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_103, [1, 6, 257, 64]);  getitem_103 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_72 = self.lowered_module_72\n        executorch_call_delegate_72 = torch.ops.higher_order.executorch_call_delegate(lowered_module_72, aten_view_copy_default_53);  lowered_module_72 = aten_view_copy_default_53 = None\n        getitem_104: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_72[0];  executorch_call_delegate_72 = None\n        alloc_176: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_27: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_104, memory_format = torch.contiguous_format, out = alloc_176);  getitem_104 = alloc_176 = None\n        aten_view_copy_default_54: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_27, [1, 257, 384]);  aten_clone_default_27 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_73 = self.lowered_module_73\n        executorch_call_delegate_73 = torch.ops.higher_order.executorch_call_delegate(lowered_module_73, aten_view_copy_default_54);  lowered_module_73 = aten_view_copy_default_54 = None\n        getitem_105: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_73[0];  executorch_call_delegate_73 = None\n        alloc_177: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_28: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_105, out = alloc_177);  getitem_105 = alloc_177 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_74 = self.lowered_module_74\n        executorch_call_delegate_74 = torch.ops.higher_order.executorch_call_delegate(lowered_module_74, aten_clone_default_28, p_blocks_5_ls1_gamma, getitem_95);  lowered_module_74 = aten_clone_default_28 = p_blocks_5_ls1_gamma = getitem_95 = None\n        getitem_106: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_74[0];  executorch_call_delegate_74 = None\n        alloc_178: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_179: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_180: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_11 = torch.ops.aten.native_layer_norm.out(getitem_106, [384], p_blocks_5_norm2_weight, p_blocks_5_norm2_bias, 1e-06, out0 = alloc_178, out1 = alloc_179, out2 = alloc_180);  p_blocks_5_norm2_weight = p_blocks_5_norm2_bias = alloc_178 = alloc_179 = alloc_180 = None\n        getitem_107: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_11[0];  aten_native_layer_norm_default_11 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_75 = self.lowered_module_75\n        executorch_call_delegate_75 = torch.ops.higher_order.executorch_call_delegate(lowered_module_75, getitem_107);  lowered_module_75 = getitem_107 = None\n        getitem_108: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_75[0];  executorch_call_delegate_75 = None\n        alloc_181: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_29: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_108, out = alloc_181);  getitem_108 = alloc_181 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_76 = self.lowered_module_76\n        executorch_call_delegate_76 = torch.ops.higher_order.executorch_call_delegate(lowered_module_76, aten_clone_default_29);  lowered_module_76 = aten_clone_default_29 = None\n        getitem_109: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_76[0];  executorch_call_delegate_76 = None\n        alloc_182: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_30: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_109, out = alloc_182);  getitem_109 = alloc_182 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_77 = self.lowered_module_77\n        executorch_call_delegate_77 = torch.ops.higher_order.executorch_call_delegate(lowered_module_77, aten_clone_default_30, p_blocks_5_ls2_gamma, getitem_106);  lowered_module_77 = aten_clone_default_30 = p_blocks_5_ls2_gamma = getitem_106 = None\n        getitem_110: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_77[0];  executorch_call_delegate_77 = None\n        alloc_183: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_184: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_185: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_12 = torch.ops.aten.native_layer_norm.out(getitem_110, [384], p_blocks_6_norm1_weight, p_blocks_6_norm1_bias, 1e-06, out0 = alloc_183, out1 = alloc_184, out2 = alloc_185);  p_blocks_6_norm1_weight = p_blocks_6_norm1_bias = alloc_183 = alloc_184 = alloc_185 = None\n        getitem_111: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_12[0];  aten_native_layer_norm_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_78 = self.lowered_module_78\n        executorch_call_delegate_78 = torch.ops.higher_order.executorch_call_delegate(lowered_module_78, getitem_111);  lowered_module_78 = getitem_111 = None\n        getitem_112: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_78[0];  executorch_call_delegate_78 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_55: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_112, [1, 257, 3, 6, 64]);  getitem_112 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_79 = self.lowered_module_79\n        executorch_call_delegate_79 = torch.ops.higher_order.executorch_call_delegate(lowered_module_79, aten_view_copy_default_55);  lowered_module_79 = aten_view_copy_default_55 = None\n        getitem_113: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_79[0];  executorch_call_delegate_79 = None\n        alloc_186: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_19: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 0, out = alloc_186);  alloc_186 = None\n        \n        # No stacktrace found for following nodes\n        alloc_187: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_20: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 1, out = alloc_187);  alloc_187 = None\n        \n        # No stacktrace found for following nodes\n        alloc_188: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 2, out = alloc_188);  getitem_113 = alloc_188 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_80 = self.lowered_module_80\n        executorch_call_delegate_80 = torch.ops.higher_order.executorch_call_delegate(lowered_module_80, aten_select_copy_int_19, _lifted_tensor_constant38, aten_select_copy_int_20);  lowered_module_80 = aten_select_copy_int_19 = _lifted_tensor_constant38 = aten_select_copy_int_20 = None\n        alloc_189: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_25: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_21, [1, 6, 257, 64], out = alloc_189);  aten_select_copy_int_21 = alloc_189 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_114: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_80[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_115: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_80[1];  executorch_call_delegate_80 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_56: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_25, [6, 257, 64]);  aten_expand_copy_default_25 = None\n        \n        # No stacktrace found for following nodes\n        alloc_190: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_26: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_114, [1, 6, 257, 64], out = alloc_190);  getitem_114 = alloc_190 = None\n        \n        # No stacktrace found for following nodes\n        alloc_191: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_27: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_115, [1, 6, 64, 257], out = alloc_191);  getitem_115 = alloc_191 = None\n        aten_view_copy_default_57: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_26, [6, 257, 64]);  aten_expand_copy_default_26 = None\n        aten_view_copy_default_58: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_27, [6, 64, 257]);  aten_expand_copy_default_27 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_81 = self.lowered_module_81\n        executorch_call_delegate_81 = torch.ops.higher_order.executorch_call_delegate(lowered_module_81, aten_view_copy_default_57, aten_view_copy_default_58);  lowered_module_81 = aten_view_copy_default_57 = aten_view_copy_default_58 = None\n        getitem_116: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_81[0];  executorch_call_delegate_81 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_59: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_116, [1, 6, 257, 257]);  getitem_116 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_82 = self.lowered_module_82\n        executorch_call_delegate_82 = torch.ops.higher_order.executorch_call_delegate(lowered_module_82, aten_view_copy_default_59);  lowered_module_82 = aten_view_copy_default_59 = None\n        getitem_117: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_82[0];  executorch_call_delegate_82 = None\n        alloc_192: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_31: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_117, out = alloc_192);  getitem_117 = alloc_192 = None\n        \n        # No stacktrace found for following nodes\n        alloc_193: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_28: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_31, [1, 6, 257, 257], out = alloc_193);  aten_clone_default_31 = alloc_193 = None\n        aten_view_copy_default_60: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_28, [6, 257, 257]);  aten_expand_copy_default_28 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_83 = self.lowered_module_83\n        executorch_call_delegate_83 = torch.ops.higher_order.executorch_call_delegate(lowered_module_83, aten_view_copy_default_60, aten_view_copy_default_56);  lowered_module_83 = aten_view_copy_default_60 = aten_view_copy_default_56 = None\n        getitem_118: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_83[0];  executorch_call_delegate_83 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_61: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_118, [1, 6, 257, 64]);  getitem_118 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_84 = self.lowered_module_84\n        executorch_call_delegate_84 = torch.ops.higher_order.executorch_call_delegate(lowered_module_84, aten_view_copy_default_61);  lowered_module_84 = aten_view_copy_default_61 = None\n        getitem_119: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_84[0];  executorch_call_delegate_84 = None\n        alloc_194: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_32: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_119, memory_format = torch.contiguous_format, out = alloc_194);  getitem_119 = alloc_194 = None\n        aten_view_copy_default_62: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_32, [1, 257, 384]);  aten_clone_default_32 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_85 = self.lowered_module_85\n        executorch_call_delegate_85 = torch.ops.higher_order.executorch_call_delegate(lowered_module_85, aten_view_copy_default_62);  lowered_module_85 = aten_view_copy_default_62 = None\n        getitem_120: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_85[0];  executorch_call_delegate_85 = None\n        alloc_195: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_33: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_120, out = alloc_195);  getitem_120 = alloc_195 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_86 = self.lowered_module_86\n        executorch_call_delegate_86 = torch.ops.higher_order.executorch_call_delegate(lowered_module_86, aten_clone_default_33, p_blocks_6_ls1_gamma, getitem_110);  lowered_module_86 = aten_clone_default_33 = p_blocks_6_ls1_gamma = getitem_110 = None\n        getitem_121: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_86[0];  executorch_call_delegate_86 = None\n        alloc_196: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_197: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_198: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_13 = torch.ops.aten.native_layer_norm.out(getitem_121, [384], p_blocks_6_norm2_weight, p_blocks_6_norm2_bias, 1e-06, out0 = alloc_196, out1 = alloc_197, out2 = alloc_198);  p_blocks_6_norm2_weight = p_blocks_6_norm2_bias = alloc_196 = alloc_197 = alloc_198 = None\n        getitem_122: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_13[0];  aten_native_layer_norm_default_13 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_87 = self.lowered_module_87\n        executorch_call_delegate_87 = torch.ops.higher_order.executorch_call_delegate(lowered_module_87, getitem_122);  lowered_module_87 = getitem_122 = None\n        getitem_123: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_87[0];  executorch_call_delegate_87 = None\n        alloc_199: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_34: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_123, out = alloc_199);  getitem_123 = alloc_199 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_88 = self.lowered_module_88\n        executorch_call_delegate_88 = torch.ops.higher_order.executorch_call_delegate(lowered_module_88, aten_clone_default_34);  lowered_module_88 = aten_clone_default_34 = None\n        getitem_124: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_88[0];  executorch_call_delegate_88 = None\n        alloc_200: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_35: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_124, out = alloc_200);  getitem_124 = alloc_200 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_89 = self.lowered_module_89\n        executorch_call_delegate_89 = torch.ops.higher_order.executorch_call_delegate(lowered_module_89, aten_clone_default_35, p_blocks_6_ls2_gamma, getitem_121);  lowered_module_89 = aten_clone_default_35 = p_blocks_6_ls2_gamma = getitem_121 = None\n        getitem_125: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_89[0];  executorch_call_delegate_89 = None\n        alloc_201: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_202: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_203: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_14 = torch.ops.aten.native_layer_norm.out(getitem_125, [384], p_blocks_7_norm1_weight, p_blocks_7_norm1_bias, 1e-06, out0 = alloc_201, out1 = alloc_202, out2 = alloc_203);  p_blocks_7_norm1_weight = p_blocks_7_norm1_bias = alloc_201 = alloc_202 = alloc_203 = None\n        getitem_126: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_14[0];  aten_native_layer_norm_default_14 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_90 = self.lowered_module_90\n        executorch_call_delegate_90 = torch.ops.higher_order.executorch_call_delegate(lowered_module_90, getitem_126);  lowered_module_90 = getitem_126 = None\n        getitem_127: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_90[0];  executorch_call_delegate_90 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_63: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_127, [1, 257, 3, 6, 64]);  getitem_127 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_91 = self.lowered_module_91\n        executorch_call_delegate_91 = torch.ops.higher_order.executorch_call_delegate(lowered_module_91, aten_view_copy_default_63);  lowered_module_91 = aten_view_copy_default_63 = None\n        getitem_128: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_91[0];  executorch_call_delegate_91 = None\n        alloc_204: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_22: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 0, out = alloc_204);  alloc_204 = None\n        \n        # No stacktrace found for following nodes\n        alloc_205: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_23: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 1, out = alloc_205);  alloc_205 = None\n        \n        # No stacktrace found for following nodes\n        alloc_206: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_24: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 2, out = alloc_206);  getitem_128 = alloc_206 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_92 = self.lowered_module_92\n        executorch_call_delegate_92 = torch.ops.higher_order.executorch_call_delegate(lowered_module_92, aten_select_copy_int_22, _lifted_tensor_constant39, aten_select_copy_int_23);  lowered_module_92 = aten_select_copy_int_22 = _lifted_tensor_constant39 = aten_select_copy_int_23 = None\n        alloc_207: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_24, [1, 6, 257, 64], out = alloc_207);  aten_select_copy_int_24 = alloc_207 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_129: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_92[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_130: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_92[1];  executorch_call_delegate_92 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_64: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_29, [6, 257, 64]);  aten_expand_copy_default_29 = None\n        \n        # No stacktrace found for following nodes\n        alloc_208: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_30: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_129, [1, 6, 257, 64], out = alloc_208);  getitem_129 = alloc_208 = None\n        \n        # No stacktrace found for following nodes\n        alloc_209: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_31: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_130, [1, 6, 64, 257], out = alloc_209);  getitem_130 = alloc_209 = None\n        aten_view_copy_default_65: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_30, [6, 257, 64]);  aten_expand_copy_default_30 = None\n        aten_view_copy_default_66: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_31, [6, 64, 257]);  aten_expand_copy_default_31 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_93 = self.lowered_module_93\n        executorch_call_delegate_93 = torch.ops.higher_order.executorch_call_delegate(lowered_module_93, aten_view_copy_default_65, aten_view_copy_default_66);  lowered_module_93 = aten_view_copy_default_65 = aten_view_copy_default_66 = None\n        getitem_131: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_93[0];  executorch_call_delegate_93 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_67: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_131, [1, 6, 257, 257]);  getitem_131 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_94 = self.lowered_module_94\n        executorch_call_delegate_94 = torch.ops.higher_order.executorch_call_delegate(lowered_module_94, aten_view_copy_default_67);  lowered_module_94 = aten_view_copy_default_67 = None\n        getitem_132: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_94[0];  executorch_call_delegate_94 = None\n        alloc_210: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_36: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_132, out = alloc_210);  getitem_132 = alloc_210 = None\n        \n        # No stacktrace found for following nodes\n        alloc_211: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_32: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_36, [1, 6, 257, 257], out = alloc_211);  aten_clone_default_36 = alloc_211 = None\n        aten_view_copy_default_68: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_32, [6, 257, 257]);  aten_expand_copy_default_32 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_95 = self.lowered_module_95\n        executorch_call_delegate_95 = torch.ops.higher_order.executorch_call_delegate(lowered_module_95, aten_view_copy_default_68, aten_view_copy_default_64);  lowered_module_95 = aten_view_copy_default_68 = aten_view_copy_default_64 = None\n        getitem_133: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_95[0];  executorch_call_delegate_95 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_69: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_133, [1, 6, 257, 64]);  getitem_133 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_96 = self.lowered_module_96\n        executorch_call_delegate_96 = torch.ops.higher_order.executorch_call_delegate(lowered_module_96, aten_view_copy_default_69);  lowered_module_96 = aten_view_copy_default_69 = None\n        getitem_134: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_96[0];  executorch_call_delegate_96 = None\n        alloc_212: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_37: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_134, memory_format = torch.contiguous_format, out = alloc_212);  getitem_134 = alloc_212 = None\n        aten_view_copy_default_70: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_37, [1, 257, 384]);  aten_clone_default_37 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_97 = self.lowered_module_97\n        executorch_call_delegate_97 = torch.ops.higher_order.executorch_call_delegate(lowered_module_97, aten_view_copy_default_70);  lowered_module_97 = aten_view_copy_default_70 = None\n        getitem_135: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_97[0];  executorch_call_delegate_97 = None\n        alloc_213: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_38: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_135, out = alloc_213);  getitem_135 = alloc_213 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_98 = self.lowered_module_98\n        executorch_call_delegate_98 = torch.ops.higher_order.executorch_call_delegate(lowered_module_98, aten_clone_default_38, p_blocks_7_ls1_gamma, getitem_125);  lowered_module_98 = aten_clone_default_38 = p_blocks_7_ls1_gamma = getitem_125 = None\n        getitem_136: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_98[0];  executorch_call_delegate_98 = None\n        alloc_214: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_215: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_216: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_15 = torch.ops.aten.native_layer_norm.out(getitem_136, [384], p_blocks_7_norm2_weight, p_blocks_7_norm2_bias, 1e-06, out0 = alloc_214, out1 = alloc_215, out2 = alloc_216);  p_blocks_7_norm2_weight = p_blocks_7_norm2_bias = alloc_214 = alloc_215 = alloc_216 = None\n        getitem_137: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_15[0];  aten_native_layer_norm_default_15 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_99 = self.lowered_module_99\n        executorch_call_delegate_99 = torch.ops.higher_order.executorch_call_delegate(lowered_module_99, getitem_137);  lowered_module_99 = getitem_137 = None\n        getitem_138: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_99[0];  executorch_call_delegate_99 = None\n        alloc_217: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_39: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_138, out = alloc_217);  getitem_138 = alloc_217 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_100 = self.lowered_module_100\n        executorch_call_delegate_100 = torch.ops.higher_order.executorch_call_delegate(lowered_module_100, aten_clone_default_39);  lowered_module_100 = aten_clone_default_39 = None\n        getitem_139: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_100[0];  executorch_call_delegate_100 = None\n        alloc_218: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_40: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_139, out = alloc_218);  getitem_139 = alloc_218 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_101 = self.lowered_module_101\n        executorch_call_delegate_101 = torch.ops.higher_order.executorch_call_delegate(lowered_module_101, aten_clone_default_40, p_blocks_7_ls2_gamma, getitem_136);  lowered_module_101 = aten_clone_default_40 = p_blocks_7_ls2_gamma = getitem_136 = None\n        getitem_140: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_101[0];  executorch_call_delegate_101 = None\n        alloc_219: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_220: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_221: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_16 = torch.ops.aten.native_layer_norm.out(getitem_140, [384], p_blocks_8_norm1_weight, p_blocks_8_norm1_bias, 1e-06, out0 = alloc_219, out1 = alloc_220, out2 = alloc_221);  p_blocks_8_norm1_weight = p_blocks_8_norm1_bias = alloc_219 = alloc_220 = alloc_221 = None\n        getitem_141: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_16[0];  aten_native_layer_norm_default_16 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_102 = self.lowered_module_102\n        executorch_call_delegate_102 = torch.ops.higher_order.executorch_call_delegate(lowered_module_102, getitem_141);  lowered_module_102 = getitem_141 = None\n        getitem_142: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_102[0];  executorch_call_delegate_102 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_71: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_142, [1, 257, 3, 6, 64]);  getitem_142 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_103 = self.lowered_module_103\n        executorch_call_delegate_103 = torch.ops.higher_order.executorch_call_delegate(lowered_module_103, aten_view_copy_default_71);  lowered_module_103 = aten_view_copy_default_71 = None\n        getitem_143: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_103[0];  executorch_call_delegate_103 = None\n        alloc_222: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_25: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 0, out = alloc_222);  alloc_222 = None\n        \n        # No stacktrace found for following nodes\n        alloc_223: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_26: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 1, out = alloc_223);  alloc_223 = None\n        \n        # No stacktrace found for following nodes\n        alloc_224: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_27: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 2, out = alloc_224);  getitem_143 = alloc_224 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_104 = self.lowered_module_104\n        executorch_call_delegate_104 = torch.ops.higher_order.executorch_call_delegate(lowered_module_104, aten_select_copy_int_25, _lifted_tensor_constant40, aten_select_copy_int_26);  lowered_module_104 = aten_select_copy_int_25 = _lifted_tensor_constant40 = aten_select_copy_int_26 = None\n        alloc_225: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_33: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_27, [1, 6, 257, 64], out = alloc_225);  aten_select_copy_int_27 = alloc_225 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_144: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_104[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_145: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_104[1];  executorch_call_delegate_104 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_72: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_33, [6, 257, 64]);  aten_expand_copy_default_33 = None\n        \n        # No stacktrace found for following nodes\n        alloc_226: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_34: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_144, [1, 6, 257, 64], out = alloc_226);  getitem_144 = alloc_226 = None\n        \n        # No stacktrace found for following nodes\n        alloc_227: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_35: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_145, [1, 6, 64, 257], out = alloc_227);  getitem_145 = alloc_227 = None\n        aten_view_copy_default_73: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_34, [6, 257, 64]);  aten_expand_copy_default_34 = None\n        aten_view_copy_default_74: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_35, [6, 64, 257]);  aten_expand_copy_default_35 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_105 = self.lowered_module_105\n        executorch_call_delegate_105 = torch.ops.higher_order.executorch_call_delegate(lowered_module_105, aten_view_copy_default_73, aten_view_copy_default_74);  lowered_module_105 = aten_view_copy_default_73 = aten_view_copy_default_74 = None\n        getitem_146: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_105[0];  executorch_call_delegate_105 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_75: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_146, [1, 6, 257, 257]);  getitem_146 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_106 = self.lowered_module_106\n        executorch_call_delegate_106 = torch.ops.higher_order.executorch_call_delegate(lowered_module_106, aten_view_copy_default_75);  lowered_module_106 = aten_view_copy_default_75 = None\n        getitem_147: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_106[0];  executorch_call_delegate_106 = None\n        alloc_228: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_41: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_147, out = alloc_228);  getitem_147 = alloc_228 = None\n        \n        # No stacktrace found for following nodes\n        alloc_229: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_36: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_41, [1, 6, 257, 257], out = alloc_229);  aten_clone_default_41 = alloc_229 = None\n        aten_view_copy_default_76: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_36, [6, 257, 257]);  aten_expand_copy_default_36 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_107 = self.lowered_module_107\n        executorch_call_delegate_107 = torch.ops.higher_order.executorch_call_delegate(lowered_module_107, aten_view_copy_default_76, aten_view_copy_default_72);  lowered_module_107 = aten_view_copy_default_76 = aten_view_copy_default_72 = None\n        getitem_148: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_107[0];  executorch_call_delegate_107 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_77: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_148, [1, 6, 257, 64]);  getitem_148 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_108 = self.lowered_module_108\n        executorch_call_delegate_108 = torch.ops.higher_order.executorch_call_delegate(lowered_module_108, aten_view_copy_default_77);  lowered_module_108 = aten_view_copy_default_77 = None\n        getitem_149: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_108[0];  executorch_call_delegate_108 = None\n        alloc_230: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_42: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_149, memory_format = torch.contiguous_format, out = alloc_230);  getitem_149 = alloc_230 = None\n        aten_view_copy_default_78: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_42, [1, 257, 384]);  aten_clone_default_42 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_109 = self.lowered_module_109\n        executorch_call_delegate_109 = torch.ops.higher_order.executorch_call_delegate(lowered_module_109, aten_view_copy_default_78);  lowered_module_109 = aten_view_copy_default_78 = None\n        getitem_150: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_109[0];  executorch_call_delegate_109 = None\n        alloc_231: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_43: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_150, out = alloc_231);  getitem_150 = alloc_231 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_110 = self.lowered_module_110\n        executorch_call_delegate_110 = torch.ops.higher_order.executorch_call_delegate(lowered_module_110, aten_clone_default_43, p_blocks_8_ls1_gamma, getitem_140);  lowered_module_110 = aten_clone_default_43 = p_blocks_8_ls1_gamma = getitem_140 = None\n        getitem_151: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_110[0];  executorch_call_delegate_110 = None\n        alloc_232: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_233: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_234: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_17 = torch.ops.aten.native_layer_norm.out(getitem_151, [384], p_blocks_8_norm2_weight, p_blocks_8_norm2_bias, 1e-06, out0 = alloc_232, out1 = alloc_233, out2 = alloc_234);  p_blocks_8_norm2_weight = p_blocks_8_norm2_bias = alloc_232 = alloc_233 = alloc_234 = None\n        getitem_152: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_17[0];  aten_native_layer_norm_default_17 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_111 = self.lowered_module_111\n        executorch_call_delegate_111 = torch.ops.higher_order.executorch_call_delegate(lowered_module_111, getitem_152);  lowered_module_111 = getitem_152 = None\n        getitem_153: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_111[0];  executorch_call_delegate_111 = None\n        alloc_235: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_44: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_153, out = alloc_235);  getitem_153 = alloc_235 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_112 = self.lowered_module_112\n        executorch_call_delegate_112 = torch.ops.higher_order.executorch_call_delegate(lowered_module_112, aten_clone_default_44);  lowered_module_112 = aten_clone_default_44 = None\n        getitem_154: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_112[0];  executorch_call_delegate_112 = None\n        alloc_236: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_45: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_154, out = alloc_236);  getitem_154 = alloc_236 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_113 = self.lowered_module_113\n        executorch_call_delegate_113 = torch.ops.higher_order.executorch_call_delegate(lowered_module_113, aten_clone_default_45, p_blocks_8_ls2_gamma, getitem_151);  lowered_module_113 = aten_clone_default_45 = p_blocks_8_ls2_gamma = getitem_151 = None\n        getitem_155: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_113[0];  executorch_call_delegate_113 = None\n        alloc_237: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_238: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_239: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_18 = torch.ops.aten.native_layer_norm.out(getitem_155, [384], p_blocks_9_norm1_weight, p_blocks_9_norm1_bias, 1e-06, out0 = alloc_237, out1 = alloc_238, out2 = alloc_239);  p_blocks_9_norm1_weight = p_blocks_9_norm1_bias = alloc_237 = alloc_238 = alloc_239 = None\n        getitem_156: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_18[0];  aten_native_layer_norm_default_18 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_114 = self.lowered_module_114\n        executorch_call_delegate_114 = torch.ops.higher_order.executorch_call_delegate(lowered_module_114, getitem_156);  lowered_module_114 = getitem_156 = None\n        getitem_157: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_114[0];  executorch_call_delegate_114 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_79: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_157, [1, 257, 3, 6, 64]);  getitem_157 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_115 = self.lowered_module_115\n        executorch_call_delegate_115 = torch.ops.higher_order.executorch_call_delegate(lowered_module_115, aten_view_copy_default_79);  lowered_module_115 = aten_view_copy_default_79 = None\n        getitem_158: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_115[0];  executorch_call_delegate_115 = None\n        alloc_240: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_28: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 0, out = alloc_240);  alloc_240 = None\n        \n        # No stacktrace found for following nodes\n        alloc_241: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 1, out = alloc_241);  alloc_241 = None\n        \n        # No stacktrace found for following nodes\n        alloc_242: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_30: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 2, out = alloc_242);  getitem_158 = alloc_242 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_116 = self.lowered_module_116\n        executorch_call_delegate_116 = torch.ops.higher_order.executorch_call_delegate(lowered_module_116, aten_select_copy_int_28, _lifted_tensor_constant41, aten_select_copy_int_29);  lowered_module_116 = aten_select_copy_int_28 = _lifted_tensor_constant41 = aten_select_copy_int_29 = None\n        alloc_243: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_37: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_30, [1, 6, 257, 64], out = alloc_243);  aten_select_copy_int_30 = alloc_243 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_159: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_116[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_160: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_116[1];  executorch_call_delegate_116 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_80: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_37, [6, 257, 64]);  aten_expand_copy_default_37 = None\n        \n        # No stacktrace found for following nodes\n        alloc_244: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_38: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_159, [1, 6, 257, 64], out = alloc_244);  getitem_159 = alloc_244 = None\n        \n        # No stacktrace found for following nodes\n        alloc_245: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_39: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_160, [1, 6, 64, 257], out = alloc_245);  getitem_160 = alloc_245 = None\n        aten_view_copy_default_81: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_38, [6, 257, 64]);  aten_expand_copy_default_38 = None\n        aten_view_copy_default_82: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_39, [6, 64, 257]);  aten_expand_copy_default_39 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_117 = self.lowered_module_117\n        executorch_call_delegate_117 = torch.ops.higher_order.executorch_call_delegate(lowered_module_117, aten_view_copy_default_81, aten_view_copy_default_82);  lowered_module_117 = aten_view_copy_default_81 = aten_view_copy_default_82 = None\n        getitem_161: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_117[0];  executorch_call_delegate_117 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_83: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_161, [1, 6, 257, 257]);  getitem_161 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_118 = self.lowered_module_118\n        executorch_call_delegate_118 = torch.ops.higher_order.executorch_call_delegate(lowered_module_118, aten_view_copy_default_83);  lowered_module_118 = aten_view_copy_default_83 = None\n        getitem_162: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_118[0];  executorch_call_delegate_118 = None\n        alloc_246: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_46: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_162, out = alloc_246);  getitem_162 = alloc_246 = None\n        \n        # No stacktrace found for following nodes\n        alloc_247: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_40: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_46, [1, 6, 257, 257], out = alloc_247);  aten_clone_default_46 = alloc_247 = None\n        aten_view_copy_default_84: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_40, [6, 257, 257]);  aten_expand_copy_default_40 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_119 = self.lowered_module_119\n        executorch_call_delegate_119 = torch.ops.higher_order.executorch_call_delegate(lowered_module_119, aten_view_copy_default_84, aten_view_copy_default_80);  lowered_module_119 = aten_view_copy_default_84 = aten_view_copy_default_80 = None\n        getitem_163: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_119[0];  executorch_call_delegate_119 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_85: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_163, [1, 6, 257, 64]);  getitem_163 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_120 = self.lowered_module_120\n        executorch_call_delegate_120 = torch.ops.higher_order.executorch_call_delegate(lowered_module_120, aten_view_copy_default_85);  lowered_module_120 = aten_view_copy_default_85 = None\n        getitem_164: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_120[0];  executorch_call_delegate_120 = None\n        alloc_248: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_47: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_164, memory_format = torch.contiguous_format, out = alloc_248);  getitem_164 = alloc_248 = None\n        aten_view_copy_default_86: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_47, [1, 257, 384]);  aten_clone_default_47 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_121 = self.lowered_module_121\n        executorch_call_delegate_121 = torch.ops.higher_order.executorch_call_delegate(lowered_module_121, aten_view_copy_default_86);  lowered_module_121 = aten_view_copy_default_86 = None\n        getitem_165: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_121[0];  executorch_call_delegate_121 = None\n        alloc_249: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_48: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_165, out = alloc_249);  getitem_165 = alloc_249 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_122 = self.lowered_module_122\n        executorch_call_delegate_122 = torch.ops.higher_order.executorch_call_delegate(lowered_module_122, aten_clone_default_48, p_blocks_9_ls1_gamma, getitem_155);  lowered_module_122 = aten_clone_default_48 = p_blocks_9_ls1_gamma = getitem_155 = None\n        getitem_166: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_122[0];  executorch_call_delegate_122 = None\n        alloc_250: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_251: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_252: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_19 = torch.ops.aten.native_layer_norm.out(getitem_166, [384], p_blocks_9_norm2_weight, p_blocks_9_norm2_bias, 1e-06, out0 = alloc_250, out1 = alloc_251, out2 = alloc_252);  p_blocks_9_norm2_weight = p_blocks_9_norm2_bias = alloc_250 = alloc_251 = alloc_252 = None\n        getitem_167: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_19[0];  aten_native_layer_norm_default_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_123 = self.lowered_module_123\n        executorch_call_delegate_123 = torch.ops.higher_order.executorch_call_delegate(lowered_module_123, getitem_167);  lowered_module_123 = getitem_167 = None\n        getitem_168: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_123[0];  executorch_call_delegate_123 = None\n        alloc_253: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_49: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_168, out = alloc_253);  getitem_168 = alloc_253 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_124 = self.lowered_module_124\n        executorch_call_delegate_124 = torch.ops.higher_order.executorch_call_delegate(lowered_module_124, aten_clone_default_49);  lowered_module_124 = aten_clone_default_49 = None\n        getitem_169: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_124[0];  executorch_call_delegate_124 = None\n        alloc_254: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_50: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_169, out = alloc_254);  getitem_169 = alloc_254 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_125 = self.lowered_module_125\n        executorch_call_delegate_125 = torch.ops.higher_order.executorch_call_delegate(lowered_module_125, aten_clone_default_50, p_blocks_9_ls2_gamma, getitem_166);  lowered_module_125 = aten_clone_default_50 = p_blocks_9_ls2_gamma = getitem_166 = None\n        getitem_170: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_125[0];  executorch_call_delegate_125 = None\n        alloc_255: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_256: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_257: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_20 = torch.ops.aten.native_layer_norm.out(getitem_170, [384], p_blocks_10_norm1_weight, p_blocks_10_norm1_bias, 1e-06, out0 = alloc_255, out1 = alloc_256, out2 = alloc_257);  p_blocks_10_norm1_weight = p_blocks_10_norm1_bias = alloc_255 = alloc_256 = alloc_257 = None\n        getitem_171: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_20[0];  aten_native_layer_norm_default_20 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_126 = self.lowered_module_126\n        executorch_call_delegate_126 = torch.ops.higher_order.executorch_call_delegate(lowered_module_126, getitem_171);  lowered_module_126 = getitem_171 = None\n        getitem_172: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_126[0];  executorch_call_delegate_126 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_87: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_172, [1, 257, 3, 6, 64]);  getitem_172 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_127 = self.lowered_module_127\n        executorch_call_delegate_127 = torch.ops.higher_order.executorch_call_delegate(lowered_module_127, aten_view_copy_default_87);  lowered_module_127 = aten_view_copy_default_87 = None\n        getitem_173: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_127[0];  executorch_call_delegate_127 = None\n        alloc_258: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_31: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 0, out = alloc_258);  alloc_258 = None\n        \n        # No stacktrace found for following nodes\n        alloc_259: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_32: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 1, out = alloc_259);  alloc_259 = None\n        \n        # No stacktrace found for following nodes\n        alloc_260: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_33: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 2, out = alloc_260);  getitem_173 = alloc_260 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_128 = self.lowered_module_128\n        executorch_call_delegate_128 = torch.ops.higher_order.executorch_call_delegate(lowered_module_128, aten_select_copy_int_31, _lifted_tensor_constant42, aten_select_copy_int_32);  lowered_module_128 = aten_select_copy_int_31 = _lifted_tensor_constant42 = aten_select_copy_int_32 = None\n        alloc_261: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_41: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_33, [1, 6, 257, 64], out = alloc_261);  aten_select_copy_int_33 = alloc_261 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_174: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_128[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_175: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_128[1];  executorch_call_delegate_128 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_88: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_41, [6, 257, 64]);  aten_expand_copy_default_41 = None\n        \n        # No stacktrace found for following nodes\n        alloc_262: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_42: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_174, [1, 6, 257, 64], out = alloc_262);  getitem_174 = alloc_262 = None\n        \n        # No stacktrace found for following nodes\n        alloc_263: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_43: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_175, [1, 6, 64, 257], out = alloc_263);  getitem_175 = alloc_263 = None\n        aten_view_copy_default_89: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_42, [6, 257, 64]);  aten_expand_copy_default_42 = None\n        aten_view_copy_default_90: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_43, [6, 64, 257]);  aten_expand_copy_default_43 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_129 = self.lowered_module_129\n        executorch_call_delegate_129 = torch.ops.higher_order.executorch_call_delegate(lowered_module_129, aten_view_copy_default_89, aten_view_copy_default_90);  lowered_module_129 = aten_view_copy_default_89 = aten_view_copy_default_90 = None\n        getitem_176: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_129[0];  executorch_call_delegate_129 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_91: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_176, [1, 6, 257, 257]);  getitem_176 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_130 = self.lowered_module_130\n        executorch_call_delegate_130 = torch.ops.higher_order.executorch_call_delegate(lowered_module_130, aten_view_copy_default_91);  lowered_module_130 = aten_view_copy_default_91 = None\n        getitem_177: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_130[0];  executorch_call_delegate_130 = None\n        alloc_264: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_51: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_177, out = alloc_264);  getitem_177 = alloc_264 = None\n        \n        # No stacktrace found for following nodes\n        alloc_265: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_44: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_51, [1, 6, 257, 257], out = alloc_265);  aten_clone_default_51 = alloc_265 = None\n        aten_view_copy_default_92: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_44, [6, 257, 257]);  aten_expand_copy_default_44 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_131 = self.lowered_module_131\n        executorch_call_delegate_131 = torch.ops.higher_order.executorch_call_delegate(lowered_module_131, aten_view_copy_default_92, aten_view_copy_default_88);  lowered_module_131 = aten_view_copy_default_92 = aten_view_copy_default_88 = None\n        getitem_178: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_131[0];  executorch_call_delegate_131 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_93: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_178, [1, 6, 257, 64]);  getitem_178 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_132 = self.lowered_module_132\n        executorch_call_delegate_132 = torch.ops.higher_order.executorch_call_delegate(lowered_module_132, aten_view_copy_default_93);  lowered_module_132 = aten_view_copy_default_93 = None\n        getitem_179: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_132[0];  executorch_call_delegate_132 = None\n        alloc_266: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_52: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_179, memory_format = torch.contiguous_format, out = alloc_266);  getitem_179 = alloc_266 = None\n        aten_view_copy_default_94: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_52, [1, 257, 384]);  aten_clone_default_52 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_133 = self.lowered_module_133\n        executorch_call_delegate_133 = torch.ops.higher_order.executorch_call_delegate(lowered_module_133, aten_view_copy_default_94);  lowered_module_133 = aten_view_copy_default_94 = None\n        getitem_180: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_133[0];  executorch_call_delegate_133 = None\n        alloc_267: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_53: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_180, out = alloc_267);  getitem_180 = alloc_267 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_134 = self.lowered_module_134\n        executorch_call_delegate_134 = torch.ops.higher_order.executorch_call_delegate(lowered_module_134, aten_clone_default_53, p_blocks_10_ls1_gamma, getitem_170);  lowered_module_134 = aten_clone_default_53 = p_blocks_10_ls1_gamma = getitem_170 = None\n        getitem_181: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_134[0];  executorch_call_delegate_134 = None\n        alloc_268: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_269: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_270: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_21 = torch.ops.aten.native_layer_norm.out(getitem_181, [384], p_blocks_10_norm2_weight, p_blocks_10_norm2_bias, 1e-06, out0 = alloc_268, out1 = alloc_269, out2 = alloc_270);  p_blocks_10_norm2_weight = p_blocks_10_norm2_bias = alloc_268 = alloc_269 = alloc_270 = None\n        getitem_182: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_21[0];  aten_native_layer_norm_default_21 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_135 = self.lowered_module_135\n        executorch_call_delegate_135 = torch.ops.higher_order.executorch_call_delegate(lowered_module_135, getitem_182);  lowered_module_135 = getitem_182 = None\n        getitem_183: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_135[0];  executorch_call_delegate_135 = None\n        alloc_271: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_54: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_183, out = alloc_271);  getitem_183 = alloc_271 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_136 = self.lowered_module_136\n        executorch_call_delegate_136 = torch.ops.higher_order.executorch_call_delegate(lowered_module_136, aten_clone_default_54);  lowered_module_136 = aten_clone_default_54 = None\n        getitem_184: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_136[0];  executorch_call_delegate_136 = None\n        alloc_272: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_55: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_184, out = alloc_272);  getitem_184 = alloc_272 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_137 = self.lowered_module_137\n        executorch_call_delegate_137 = torch.ops.higher_order.executorch_call_delegate(lowered_module_137, aten_clone_default_55, p_blocks_10_ls2_gamma, getitem_181);  lowered_module_137 = aten_clone_default_55 = p_blocks_10_ls2_gamma = getitem_181 = None\n        getitem_185: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_137[0];  executorch_call_delegate_137 = None\n        alloc_273: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_274: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_275: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_22 = torch.ops.aten.native_layer_norm.out(getitem_185, [384], p_blocks_11_norm1_weight, p_blocks_11_norm1_bias, 1e-06, out0 = alloc_273, out1 = alloc_274, out2 = alloc_275);  p_blocks_11_norm1_weight = p_blocks_11_norm1_bias = alloc_273 = alloc_274 = alloc_275 = None\n        getitem_186: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_22[0];  aten_native_layer_norm_default_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_138 = self.lowered_module_138\n        executorch_call_delegate_138 = torch.ops.higher_order.executorch_call_delegate(lowered_module_138, getitem_186);  lowered_module_138 = getitem_186 = None\n        getitem_187: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_138[0];  executorch_call_delegate_138 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_95: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_187, [1, 257, 3, 6, 64]);  getitem_187 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_139 = self.lowered_module_139\n        executorch_call_delegate_139 = torch.ops.higher_order.executorch_call_delegate(lowered_module_139, aten_view_copy_default_95);  lowered_module_139 = aten_view_copy_default_95 = None\n        getitem_188: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_139[0];  executorch_call_delegate_139 = None\n        alloc_276: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_34: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 0, out = alloc_276);  alloc_276 = None\n        \n        # No stacktrace found for following nodes\n        alloc_277: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_35: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 1, out = alloc_277);  alloc_277 = None\n        \n        # No stacktrace found for following nodes\n        alloc_278: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_36: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 2, out = alloc_278);  getitem_188 = alloc_278 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_140 = self.lowered_module_140\n        executorch_call_delegate_140 = torch.ops.higher_order.executorch_call_delegate(lowered_module_140, aten_select_copy_int_34, _lifted_tensor_constant43, aten_select_copy_int_35);  lowered_module_140 = aten_select_copy_int_34 = _lifted_tensor_constant43 = aten_select_copy_int_35 = None\n        alloc_279: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_45: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_36, [1, 6, 257, 64], out = alloc_279);  aten_select_copy_int_36 = alloc_279 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_189: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_140[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_190: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_140[1];  executorch_call_delegate_140 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_96: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_45, [6, 257, 64]);  aten_expand_copy_default_45 = None\n        \n        # No stacktrace found for following nodes\n        alloc_280: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_46: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_189, [1, 6, 257, 64], out = alloc_280);  getitem_189 = alloc_280 = None\n        \n        # No stacktrace found for following nodes\n        alloc_281: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_47: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_190, [1, 6, 64, 257], out = alloc_281);  getitem_190 = alloc_281 = None\n        aten_view_copy_default_97: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_46, [6, 257, 64]);  aten_expand_copy_default_46 = None\n        aten_view_copy_default_98: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_47, [6, 64, 257]);  aten_expand_copy_default_47 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_141 = self.lowered_module_141\n        executorch_call_delegate_141 = torch.ops.higher_order.executorch_call_delegate(lowered_module_141, aten_view_copy_default_97, aten_view_copy_default_98);  lowered_module_141 = aten_view_copy_default_97 = aten_view_copy_default_98 = None\n        getitem_191: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_141[0];  executorch_call_delegate_141 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_99: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_191, [1, 6, 257, 257]);  getitem_191 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_142 = self.lowered_module_142\n        executorch_call_delegate_142 = torch.ops.higher_order.executorch_call_delegate(lowered_module_142, aten_view_copy_default_99);  lowered_module_142 = aten_view_copy_default_99 = None\n        getitem_192: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_142[0];  executorch_call_delegate_142 = None\n        alloc_282: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_56: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_192, out = alloc_282);  getitem_192 = alloc_282 = None\n        \n        # No stacktrace found for following nodes\n        alloc_283: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_48: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_56, [1, 6, 257, 257], out = alloc_283);  aten_clone_default_56 = alloc_283 = None\n        aten_view_copy_default_100: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_48, [6, 257, 257]);  aten_expand_copy_default_48 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_143 = self.lowered_module_143\n        executorch_call_delegate_143 = torch.ops.higher_order.executorch_call_delegate(lowered_module_143, aten_view_copy_default_100, aten_view_copy_default_96);  lowered_module_143 = aten_view_copy_default_100 = aten_view_copy_default_96 = None\n        getitem_193: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_143[0];  executorch_call_delegate_143 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_101: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_193, [1, 6, 257, 64]);  getitem_193 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_144 = self.lowered_module_144\n        executorch_call_delegate_144 = torch.ops.higher_order.executorch_call_delegate(lowered_module_144, aten_view_copy_default_101);  lowered_module_144 = aten_view_copy_default_101 = None\n        getitem_194: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_144[0];  executorch_call_delegate_144 = None\n        alloc_284: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_57: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_194, memory_format = torch.contiguous_format, out = alloc_284);  getitem_194 = alloc_284 = None\n        aten_view_copy_default_102: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_57, [1, 257, 384]);  aten_clone_default_57 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_145 = self.lowered_module_145\n        executorch_call_delegate_145 = torch.ops.higher_order.executorch_call_delegate(lowered_module_145, aten_view_copy_default_102);  lowered_module_145 = aten_view_copy_default_102 = None\n        getitem_195: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_145[0];  executorch_call_delegate_145 = None\n        alloc_285: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_58: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_195, out = alloc_285);  getitem_195 = alloc_285 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_146 = self.lowered_module_146\n        executorch_call_delegate_146 = torch.ops.higher_order.executorch_call_delegate(lowered_module_146, aten_clone_default_58, p_blocks_11_ls1_gamma, getitem_185);  lowered_module_146 = aten_clone_default_58 = p_blocks_11_ls1_gamma = getitem_185 = None\n        getitem_196: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_146[0];  executorch_call_delegate_146 = None\n        alloc_286: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_287: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_288: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_23 = torch.ops.aten.native_layer_norm.out(getitem_196, [384], p_blocks_11_norm2_weight, p_blocks_11_norm2_bias, 1e-06, out0 = alloc_286, out1 = alloc_287, out2 = alloc_288);  p_blocks_11_norm2_weight = p_blocks_11_norm2_bias = alloc_286 = alloc_287 = alloc_288 = None\n        getitem_197: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_23[0];  aten_native_layer_norm_default_23 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_147 = self.lowered_module_147\n        executorch_call_delegate_147 = torch.ops.higher_order.executorch_call_delegate(lowered_module_147, getitem_197);  lowered_module_147 = getitem_197 = None\n        getitem_198: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_147[0];  executorch_call_delegate_147 = None\n        alloc_289: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_59: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_198, out = alloc_289);  getitem_198 = alloc_289 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_148 = self.lowered_module_148\n        executorch_call_delegate_148 = torch.ops.higher_order.executorch_call_delegate(lowered_module_148, aten_clone_default_59);  lowered_module_148 = aten_clone_default_59 = None\n        getitem_199: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_148[0];  executorch_call_delegate_148 = None\n        alloc_290: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_60: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_199, out = alloc_290);  getitem_199 = alloc_290 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_149 = self.lowered_module_149\n        executorch_call_delegate_149 = torch.ops.higher_order.executorch_call_delegate(lowered_module_149, aten_clone_default_60, p_blocks_11_ls2_gamma, getitem_196);  lowered_module_149 = aten_clone_default_60 = p_blocks_11_ls2_gamma = getitem_196 = None\n        getitem_200: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_149[0];  executorch_call_delegate_149 = None\n        alloc_291: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_292: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_293: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_24 = torch.ops.aten.native_layer_norm.out(getitem_200, [384], p_norm_weight, p_norm_bias, 1e-06, out0 = alloc_291, out1 = alloc_292, out2 = alloc_293);  getitem_200 = p_norm_weight = p_norm_bias = alloc_291 = alloc_292 = alloc_293 = None\n        getitem_201: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_24[0];  aten_native_layer_norm_default_24 = None\n        \n        # No stacktrace found for following nodes\n        alloc_294: \"f32[1, 384][384, 1]\" = executorch_exir_memory_alloc(((1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_select_copy_int_37: \"f32[1, 384][384, 1]\" = torch.ops.aten.select_copy.int_out(getitem_201, 1, 0, out = alloc_294);  getitem_201 = alloc_294 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_150 = self.lowered_module_150\n        executorch_call_delegate_150 = torch.ops.higher_order.executorch_call_delegate(lowered_module_150, aten_select_copy_int_37);  lowered_module_150 = aten_select_copy_int_37 = None\n        getitem_202: \"f32[1, 3][3, 1]\" = executorch_call_delegate_150[0];  executorch_call_delegate_150 = None\n        return (getitem_202,)\n        \n\nOriginal traceback:\nFile \"/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 325, in forward\n    ret = self.forward_features(*args, **kwargs)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:564\u001b[39m, in \u001b[36m_Emitter._constant_to_evalue\u001b[39m\u001b[34m(self, val, val_type)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EValue(Int(\u001b[43mmemory_format_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/tensor.py:267\u001b[39m, in \u001b[36mmemory_format_enum\u001b[39m\u001b[34m(memory_format)\u001b[39m\n\u001b[32m    263\u001b[39m table = {\n\u001b[32m    264\u001b[39m     torch.contiguous_format: \u001b[32m0\u001b[39m,\n\u001b[32m    265\u001b[39m     torch.preserve_format: \u001b[32m1\u001b[39m,\n\u001b[32m    266\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: torch.channels_last",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      8\u001b[39m sample_inputs = (torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m), )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# model = models.mobilenet_v2(pretrained=True)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# # ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# model.load_state_dict(torch.load('/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/AI/MobileNet/best_mobilenet_cucumber.pth', weights_only=False, map_location=device))\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[32m     18\u001b[39m et_program = \u001b[43mto_edge_transform_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartitioner\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mXnnpackPartitioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_executorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mvit_model.pte\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     24\u001b[39m     f.write(et_program.buffer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/program/_program.py:114\u001b[39m, in \u001b[36met_logger.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/program/_program.py:1620\u001b[39m, in \u001b[36mEdgeProgramManager.to_executorch\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1617\u001b[39m     _copy_module(program.graph_module, new_gm)\n\u001b[32m   1618\u001b[39m     execution_programs[name] = program\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecutorchProgramManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_programs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_named_data_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_named_data_store_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/program/_program.py:1674\u001b[39m, in \u001b[36mExecutorchProgramManager.__init__\u001b[39m\u001b[34m(self, execution_programs, config_methods, backend_config, named_data)\u001b[39m\n\u001b[32m   1671\u001b[39m backend_config = backend_config \u001b[38;5;129;01mor\u001b[39;00m ExecutorchBackendConfig()\n\u001b[32m   1673\u001b[39m \u001b[38;5;66;03m# Emit methods\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1674\u001b[39m \u001b[38;5;28mself\u001b[39m._emitter_output: EmitterOutput = \u001b[43memit_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execution_programs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43memit_stacktrace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43memit_mutable_buffer_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[38;5;66;03m# Serialize emitter output, ready to be written to a file.\u001b[39;00m\n\u001b[32m   1682\u001b[39m \u001b[38;5;28mself\u001b[39m._data_serializer = FlatTensorSerializer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emit_program.py:176\u001b[39m, in \u001b[36memit_program\u001b[39m\u001b[34m(methods, emit_stacktrace, prim_getters, emit_mutable_buffer_names)\u001b[39m\n\u001b[32m    170\u001b[39m gm = _remove_non_user_outputs(exported_program)\n\u001b[32m    172\u001b[39m emitter = _TopLevelEmitter(\n\u001b[32m    173\u001b[39m     name, exported_program, gm, program_state, emitter_state\n\u001b[32m    174\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[43memitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m plans.append(emitter.plan())\n\u001b[32m    179\u001b[39m debug_handle_map[name] = emitter.debug_handle_map\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1516\u001b[39m, in \u001b[36m_Emitter.run\u001b[39m\u001b[34m(self, initial_env, *args)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(  \u001b[38;5;66;03m# pyre-fixme[14]\u001b[39;00m\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1512\u001b[39m     *args: _Argument,\n\u001b[32m   1513\u001b[39m     initial_env: Optional[Dict[torch.fx.Node, _Argument]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1514\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1515\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Traverses all nodes in the graph module and emits each one appropriately.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_io_processing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/fx/interpreter.py:173\u001b[39m, in \u001b[36mInterpreter.run\u001b[39m\u001b[34m(self, initial_env, enable_io_processing, *args)\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28mself\u001b[39m.env[node] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extra_traceback:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1529\u001b[39m, in \u001b[36m_Emitter.run_node\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1527\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (InternalError, ExportError)):\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1531\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InternalError(\n\u001b[32m   1532\u001b[39m             \u001b[38;5;28mself\u001b[39m._emit_node_specific_error(\u001b[38;5;28mself\u001b[39m.node, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m   1533\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1526\u001b[39m, in \u001b[36m_Emitter.run_node\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28mself\u001b[39m.node = n\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m     ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (InternalError, ExportError)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/fx/interpreter.py:242\u001b[39m, in \u001b[36mInterpreter.run_node\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1501\u001b[39m, in \u001b[36m_Emitter.call_function\u001b[39m\u001b[34m(self, target, args, kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1499\u001b[39m     target, (torch._ops.OpOverload, EdgeOpOverload, BackendOpOverload)\n\u001b[32m   1500\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_emit_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1504\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InternalError(\n\u001b[32m   1505\u001b[39m         \u001b[38;5;28mself\u001b[39m._emit_node_specific_error(\n\u001b[32m   1506\u001b[39m             \u001b[38;5;28mself\u001b[39m.node, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid target for call_function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1507\u001b[39m         )\n\u001b[32m   1508\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1229\u001b[39m, in \u001b[36m_Emitter._emit_operator\u001b[39m\u001b[34m(self, target, args, kwargs)\u001b[39m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kernel_arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema_arg.type, torch.TensorType):\n\u001b[32m   1227\u001b[39m     kernel_arg = \u001b[38;5;28mself\u001b[39m._emit_evalue(_get_empty_tensor_evalue())\n\u001b[32m-> \u001b[39m\u001b[32m1229\u001b[39m kernel_args.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_emit_argument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_arg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m.id)\n\u001b[32m   1231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema_arg.is_out:\n\u001b[32m   1232\u001b[39m     out_args.append((schema_arg.name, kernel_arg))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1048\u001b[39m, in \u001b[36m_Emitter._emit_argument\u001b[39m\u001b[34m(self, arg, arg_type)\u001b[39m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, _AbstractValue):\n\u001b[32m   1047\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[32m-> \u001b[39m\u001b[32m1048\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._emit_evalue(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_constant_to_evalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_type\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:566\u001b[39m, in \u001b[36m_Emitter._constant_to_evalue\u001b[39m\u001b[34m(self, val, val_type)\u001b[39m\n\u001b[32m    564\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m EValue(Int(memory_format_enum(val)))\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InternalError(\n\u001b[32m    567\u001b[39m             \u001b[38;5;28mself\u001b[39m._emit_node_specific_error(\n\u001b[32m    568\u001b[39m                 \u001b[38;5;28mself\u001b[39m.node,\n\u001b[32m    569\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTensor has a memory_format that is unsupported in ExecuTorch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    570\u001b[39m             )\n\u001b[32m    571\u001b[39m         )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, torch.Tensor):\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ExportError(\n\u001b[32m    575\u001b[39m         ExportErrorType.NOT_SUPPORTED,\n\u001b[32m    576\u001b[39m         \u001b[38;5;28mself\u001b[39m._emit_node_specific_error(\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         ),\n\u001b[32m    580\u001b[39m     )\n",
      "\u001b[31mInternalError\u001b[39m: Failed with error: Tensor has a memory_format that is unsupported in ExecuTorch: torch.channels_last\nHere is the node in the graph module:\ngraph():\n    %p_cls_token : [num_users=1] = placeholder[target=p_cls_token]\n    %p_pos_embed : [num_users=2] = placeholder[target=p_pos_embed]\n    %p_blocks_0_norm1_weight : [num_users=1] = placeholder[target=p_blocks_0_norm1_weight]\n    %p_blocks_0_norm1_bias : [num_users=1] = placeholder[target=p_blocks_0_norm1_bias]\n    %p_blocks_0_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_0_ls1_gamma]\n    %p_blocks_0_norm2_weight : [num_users=1] = placeholder[target=p_blocks_0_norm2_weight]\n    %p_blocks_0_norm2_bias : [num_users=1] = placeholder[target=p_blocks_0_norm2_bias]\n    %p_blocks_0_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_0_ls2_gamma]\n    %p_blocks_1_norm1_weight : [num_users=1] = placeholder[target=p_blocks_1_norm1_weight]\n    %p_blocks_1_norm1_bias : [num_users=1] = placeholder[target=p_blocks_1_norm1_bias]\n    %p_blocks_1_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_1_ls1_gamma]\n    %p_blocks_1_norm2_weight : [num_users=1] = placeholder[target=p_blocks_1_norm2_weight]\n    %p_blocks_1_norm2_bias : [num_users=1] = placeholder[target=p_blocks_1_norm2_bias]\n    %p_blocks_1_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_1_ls2_gamma]\n    %p_blocks_2_norm1_weight : [num_users=1] = placeholder[target=p_blocks_2_norm1_weight]\n    %p_blocks_2_norm1_bias : [num_users=1] = placeholder[target=p_blocks_2_norm1_bias]\n    %p_blocks_2_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_2_ls1_gamma]\n    %p_blocks_2_norm2_weight : [num_users=1] = placeholder[target=p_blocks_2_norm2_weight]\n    %p_blocks_2_norm2_bias : [num_users=1] = placeholder[target=p_blocks_2_norm2_bias]\n    %p_blocks_2_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_2_ls2_gamma]\n    %p_blocks_3_norm1_weight : [num_users=1] = placeholder[target=p_blocks_3_norm1_weight]\n    %p_blocks_3_norm1_bias : [num_users=1] = placeholder[target=p_blocks_3_norm1_bias]\n    %p_blocks_3_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_3_ls1_gamma]\n    %p_blocks_3_norm2_weight : [num_users=1] = placeholder[target=p_blocks_3_norm2_weight]\n    %p_blocks_3_norm2_bias : [num_users=1] = placeholder[target=p_blocks_3_norm2_bias]\n    %p_blocks_3_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_3_ls2_gamma]\n    %p_blocks_4_norm1_weight : [num_users=1] = placeholder[target=p_blocks_4_norm1_weight]\n    %p_blocks_4_norm1_bias : [num_users=1] = placeholder[target=p_blocks_4_norm1_bias]\n    %p_blocks_4_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_4_ls1_gamma]\n    %p_blocks_4_norm2_weight : [num_users=1] = placeholder[target=p_blocks_4_norm2_weight]\n    %p_blocks_4_norm2_bias : [num_users=1] = placeholder[target=p_blocks_4_norm2_bias]\n    %p_blocks_4_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_4_ls2_gamma]\n    %p_blocks_5_norm1_weight : [num_users=1] = placeholder[target=p_blocks_5_norm1_weight]\n    %p_blocks_5_norm1_bias : [num_users=1] = placeholder[target=p_blocks_5_norm1_bias]\n    %p_blocks_5_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_5_ls1_gamma]\n    %p_blocks_5_norm2_weight : [num_users=1] = placeholder[target=p_blocks_5_norm2_weight]\n    %p_blocks_5_norm2_bias : [num_users=1] = placeholder[target=p_blocks_5_norm2_bias]\n    %p_blocks_5_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_5_ls2_gamma]\n    %p_blocks_6_norm1_weight : [num_users=1] = placeholder[target=p_blocks_6_norm1_weight]\n    %p_blocks_6_norm1_bias : [num_users=1] = placeholder[target=p_blocks_6_norm1_bias]\n    %p_blocks_6_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_6_ls1_gamma]\n    %p_blocks_6_norm2_weight : [num_users=1] = placeholder[target=p_blocks_6_norm2_weight]\n    %p_blocks_6_norm2_bias : [num_users=1] = placeholder[target=p_blocks_6_norm2_bias]\n    %p_blocks_6_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_6_ls2_gamma]\n    %p_blocks_7_norm1_weight : [num_users=1] = placeholder[target=p_blocks_7_norm1_weight]\n    %p_blocks_7_norm1_bias : [num_users=1] = placeholder[target=p_blocks_7_norm1_bias]\n    %p_blocks_7_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_7_ls1_gamma]\n    %p_blocks_7_norm2_weight : [num_users=1] = placeholder[target=p_blocks_7_norm2_weight]\n    %p_blocks_7_norm2_bias : [num_users=1] = placeholder[target=p_blocks_7_norm2_bias]\n    %p_blocks_7_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_7_ls2_gamma]\n    %p_blocks_8_norm1_weight : [num_users=1] = placeholder[target=p_blocks_8_norm1_weight]\n    %p_blocks_8_norm1_bias : [num_users=1] = placeholder[target=p_blocks_8_norm1_bias]\n    %p_blocks_8_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_8_ls1_gamma]\n    %p_blocks_8_norm2_weight : [num_users=1] = placeholder[target=p_blocks_8_norm2_weight]\n    %p_blocks_8_norm2_bias : [num_users=1] = placeholder[target=p_blocks_8_norm2_bias]\n    %p_blocks_8_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_8_ls2_gamma]\n    %p_blocks_9_norm1_weight : [num_users=1] = placeholder[target=p_blocks_9_norm1_weight]\n    %p_blocks_9_norm1_bias : [num_users=1] = placeholder[target=p_blocks_9_norm1_bias]\n    %p_blocks_9_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_9_ls1_gamma]\n    %p_blocks_9_norm2_weight : [num_users=1] = placeholder[target=p_blocks_9_norm2_weight]\n    %p_blocks_9_norm2_bias : [num_users=1] = placeholder[target=p_blocks_9_norm2_bias]\n    %p_blocks_9_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_9_ls2_gamma]\n    %p_blocks_10_norm1_weight : [num_users=1] = placeholder[target=p_blocks_10_norm1_weight]\n    %p_blocks_10_norm1_bias : [num_users=1] = placeholder[target=p_blocks_10_norm1_bias]\n    %p_blocks_10_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_10_ls1_gamma]\n    %p_blocks_10_norm2_weight : [num_users=1] = placeholder[target=p_blocks_10_norm2_weight]\n    %p_blocks_10_norm2_bias : [num_users=1] = placeholder[target=p_blocks_10_norm2_bias]\n    %p_blocks_10_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_10_ls2_gamma]\n    %p_blocks_11_norm1_weight : [num_users=1] = placeholder[target=p_blocks_11_norm1_weight]\n    %p_blocks_11_norm1_bias : [num_users=1] = placeholder[target=p_blocks_11_norm1_bias]\n    %p_blocks_11_ls1_gamma : [num_users=1] = placeholder[target=p_blocks_11_ls1_gamma]\n    %p_blocks_11_norm2_weight : [num_users=1] = placeholder[target=p_blocks_11_norm2_weight]\n    %p_blocks_11_norm2_bias : [num_users=1] = placeholder[target=p_blocks_11_norm2_bias]\n    %p_blocks_11_ls2_gamma : [num_users=1] = placeholder[target=p_blocks_11_ls2_gamma]\n    %p_norm_weight : [num_users=1] = placeholder[target=p_norm_weight]\n    %p_norm_bias : [num_users=1] = placeholder[target=p_norm_bias]\n    %_lifted_tensor_constant0 : [num_users=1] = placeholder[target=_lifted_tensor_constant0]\n    %_lifted_tensor_constant1 : [num_users=1] = placeholder[target=_lifted_tensor_constant1]\n    %_lifted_tensor_constant2 : [num_users=1] = placeholder[target=_lifted_tensor_constant2]\n    %_lifted_tensor_constant3 : [num_users=1] = placeholder[target=_lifted_tensor_constant3]\n    %_lifted_tensor_constant4 : [num_users=1] = placeholder[target=_lifted_tensor_constant4]\n    %_lifted_tensor_constant5 : [num_users=1] = placeholder[target=_lifted_tensor_constant5]\n    %_lifted_tensor_constant6 : [num_users=1] = placeholder[target=_lifted_tensor_constant6]\n    %_lifted_tensor_constant7 : [num_users=1] = placeholder[target=_lifted_tensor_constant7]\n    %_lifted_tensor_constant8 : [num_users=1] = placeholder[target=_lifted_tensor_constant8]\n    %_lifted_tensor_constant9 : [num_users=1] = placeholder[target=_lifted_tensor_constant9]\n    %_lifted_tensor_constant10 : [num_users=1] = placeholder[target=_lifted_tensor_constant10]\n    %_lifted_tensor_constant11 : [num_users=1] = placeholder[target=_lifted_tensor_constant11]\n    %_lifted_tensor_constant12 : [num_users=1] = placeholder[target=_lifted_tensor_constant12]\n    %_lifted_tensor_constant13 : [num_users=1] = placeholder[target=_lifted_tensor_constant13]\n    %_lifted_tensor_constant14 : [num_users=1] = placeholder[target=_lifted_tensor_constant14]\n    %_lifted_tensor_constant15 : [num_users=1] = placeholder[target=_lifted_tensor_constant15]\n    %_lifted_tensor_constant16 : [num_users=1] = placeholder[target=_lifted_tensor_constant16]\n    %_lifted_tensor_constant17 : [num_users=1] = placeholder[target=_lifted_tensor_constant17]\n    %_lifted_tensor_constant18 : [num_users=1] = placeholder[target=_lifted_tensor_constant18]\n    %_lifted_tensor_constant19 : [num_users=1] = placeholder[target=_lifted_tensor_constant19]\n    %_lifted_tensor_constant20 : [num_users=1] = placeholder[target=_lifted_tensor_constant20]\n    %_lifted_tensor_constant21 : [num_users=1] = placeholder[target=_lifted_tensor_constant21]\n    %_lifted_tensor_constant22 : [num_users=1] = placeholder[target=_lifted_tensor_constant22]\n    %_lifted_tensor_constant23 : [num_users=1] = placeholder[target=_lifted_tensor_constant23]\n    %_lifted_tensor_constant24 : [num_users=1] = placeholder[target=_lifted_tensor_constant24]\n    %_lifted_tensor_constant25 : [num_users=1] = placeholder[target=_lifted_tensor_constant25]\n    %_lifted_tensor_constant26 : [num_users=1] = placeholder[target=_lifted_tensor_constant26]\n    %_lifted_tensor_constant27 : [num_users=1] = placeholder[target=_lifted_tensor_constant27]\n    %_lifted_tensor_constant28 : [num_users=1] = placeholder[target=_lifted_tensor_constant28]\n    %_lifted_tensor_constant29 : [num_users=1] = placeholder[target=_lifted_tensor_constant29]\n    %_lifted_tensor_constant30 : [num_users=1] = placeholder[target=_lifted_tensor_constant30]\n    %_lifted_tensor_constant31 : [num_users=1] = placeholder[target=_lifted_tensor_constant31]\n    %_lifted_tensor_constant32 : [num_users=1] = placeholder[target=_lifted_tensor_constant32]\n    %_lifted_tensor_constant33 : [num_users=1] = placeholder[target=_lifted_tensor_constant33]\n    %_lifted_tensor_constant34 : [num_users=1] = placeholder[target=_lifted_tensor_constant34]\n    %_lifted_tensor_constant35 : [num_users=1] = placeholder[target=_lifted_tensor_constant35]\n    %_lifted_tensor_constant36 : [num_users=1] = placeholder[target=_lifted_tensor_constant36]\n    %_lifted_tensor_constant37 : [num_users=1] = placeholder[target=_lifted_tensor_constant37]\n    %_lifted_tensor_constant38 : [num_users=1] = placeholder[target=_lifted_tensor_constant38]\n    %_lifted_tensor_constant39 : [num_users=1] = placeholder[target=_lifted_tensor_constant39]\n    %_lifted_tensor_constant40 : [num_users=1] = placeholder[target=_lifted_tensor_constant40]\n    %_lifted_tensor_constant41 : [num_users=1] = placeholder[target=_lifted_tensor_constant41]\n    %_lifted_tensor_constant42 : [num_users=1] = placeholder[target=_lifted_tensor_constant42]\n    %_lifted_tensor_constant43 : [num_users=1] = placeholder[target=_lifted_tensor_constant43]\n    %args_0 : [num_users=1] = placeholder[target=args_0]\n    %alloc : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_arange_start_step : [num_users=1] = call_function[target=torch.ops.aten.arange.start_out](args = (0, 16), kwargs = {out: %alloc})\n    %alloc_1 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_arange_start_step_1 : [num_users=1] = call_function[target=torch.ops.aten.arange.start_out](args = (0, 16), kwargs = {out: %alloc_1})\n    %alloc_2 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 384), torch.float32),), kwargs = {})\n    %aten_expand_copy_default : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%p_cls_token, [1, -1, -1]), kwargs = {out: %alloc_2})\n    %alloc_3 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384), torch.float32),), kwargs = {})\n    %aten_select_copy_int : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%p_pos_embed, 1, 0), kwargs = {out: %alloc_3})\n    %alloc_4 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%_lifted_tensor_constant21,), kwargs = {dim_order: [], out: %alloc_4})\n    %alloc_5 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_1 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%_lifted_tensor_constant31,), kwargs = {dim_order: [], out: %alloc_5})\n    %alloc_6 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_2 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%aten_arange_start_step,), kwargs = {dim_order: [0], out: %alloc_6})\n    %alloc_7 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_3 : [num_users=1] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%aten_arange_start_step_1,), kwargs = {dim_order: [0], out: %alloc_7})\n    %alloc_8 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 384), torch.float32),), kwargs = {})\n    %aten_unsqueeze_copy_default : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze_copy.out](args = (%aten_select_copy_int, 0), kwargs = {out: %alloc_8})\n    %lowered_module_0 : [num_users=1] = get_attr[target=lowered_module_0]\n    %executorch_call_delegate : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_0, %dim_order_ops__to_dim_order_copy_default_2, %_lifted_tensor_constant3, %_lifted_tensor_constant4, %_lifted_tensor_constant5), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 0), kwargs = {})\n    %alloc_9 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_unsqueeze_copy_default_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze_copy.out](args = (%getitem, -1), kwargs = {out: %alloc_9})\n    %lowered_module_1 : [num_users=1] = get_attr[target=lowered_module_1]\n    %executorch_call_delegate_1 : [num_users=7] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_1, %p_pos_embed, %dim_order_ops__to_dim_order_copy_default_3, %_lifted_tensor_constant0, %aten_unsqueeze_copy_default_1, %_lifted_tensor_constant1, %_lifted_tensor_constant2, %_lifted_tensor_constant22, %_lifted_tensor_constant23, %_lifted_tensor_constant24, %_lifted_tensor_constant12, %_lifted_tensor_constant13, %_lifted_tensor_constant14), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 0), kwargs = {})\n    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 1), kwargs = {})\n    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 2), kwargs = {})\n    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 3), kwargs = {})\n    %getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 4), kwargs = {})\n    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 5), kwargs = {})\n    %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 6), kwargs = {})\n    %aten_view_copy_default : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_1, [1, 37, 37, 384]), kwargs = {})\n    %alloc_10 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_4 : [num_users=7] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%getitem_2,), kwargs = {dim_order: [0, 1], out: %alloc_10})\n    %alloc_11 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %dim_order_ops__to_dim_order_copy_default_5 : [num_users=7] = call_function[target=torch.ops.dim_order_ops._to_dim_order_copy.out](args = (%getitem_3,), kwargs = {dim_order: [0], out: %alloc_11})\n    %aten_view_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_4, [2, 16, 1]), kwargs = {})\n    %aten_view_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_5, [2, 16, 1]), kwargs = {})\n    %aten_view_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_6, [2, 16]), kwargs = {})\n    %aten_view_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_7, [2, 16]), kwargs = {})\n    %alloc_12 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_sub_tensor : [num_users=4] = call_function[target=torch.ops.aten.sub.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant6), kwargs = {out: %alloc_12})\n    %alloc_13 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_add_tensor : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant7), kwargs = {out: %alloc_13})\n    %alloc_14 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_add_tensor_1 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_4, %_lifted_tensor_constant8), kwargs = {out: %alloc_14})\n    %alloc_15 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_15})\n    %alloc_16 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_16})\n    %alloc_17 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_17})\n    %alloc_18 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_4, 0, 36), kwargs = {out: %alloc_18})\n    %alloc_19 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_sub_tensor_1 : [num_users=4] = call_function[target=torch.ops.aten.sub.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant9), kwargs = {out: %alloc_19})\n    %alloc_20 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_add_tensor_2 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant10), kwargs = {out: %alloc_20})\n    %alloc_21 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_add_tensor_3 : [num_users=4] = call_function[target=torch.ops.aten.add.out](args = (%dim_order_ops__to_dim_order_copy_default_5, %_lifted_tensor_constant11), kwargs = {out: %alloc_21})\n    %alloc_22 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_22})\n    %alloc_23 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_5 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_23})\n    %alloc_24 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_6 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_24})\n    %alloc_25 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_7 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%dim_order_ops__to_dim_order_copy_default_5, 0, 36), kwargs = {out: %alloc_25})\n    %lowered_module_2 : [num_users=1] = get_attr[target=lowered_module_2]\n    %executorch_call_delegate_2 : [num_users=9] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_2, %aten_view_copy_default, %aten_view_copy_default_3, %_lifted_tensor_constant19, %aten_view_copy_default_4, %_lifted_tensor_constant15, %aten_view_copy_default_1, %_lifted_tensor_constant29, %aten_view_copy_default_2, %_lifted_tensor_constant25, %_lifted_tensor_constant20, %_lifted_tensor_constant16, %_lifted_tensor_constant30, %_lifted_tensor_constant26, %_lifted_tensor_constant17, %_lifted_tensor_constant27, %dim_order_ops__to_dim_order_copy_default, %dim_order_ops__to_dim_order_copy_default_1, %_lifted_tensor_constant18, %_lifted_tensor_constant28), kwargs = {})\n    %alloc_26 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_8 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_26})\n    %alloc_27 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_9 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_27})\n    %alloc_28 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_10 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_28})\n    %alloc_29 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_11 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor, 0, 36), kwargs = {out: %alloc_29})\n    %alloc_30 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_12 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_30})\n    %alloc_31 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_13 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_31})\n    %alloc_32 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_14 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_32})\n    %alloc_33 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_15 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor, 0, 36), kwargs = {out: %alloc_33})\n    %alloc_34 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_16 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_34})\n    %alloc_35 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_17 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_35})\n    %alloc_36 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_18 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_36})\n    %alloc_37 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.int64),), kwargs = {})\n    %aten_clamp_default_19 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_1, 0, 36), kwargs = {out: %alloc_37})\n    %alloc_38 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_20 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_38})\n    %alloc_39 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_21 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_39})\n    %alloc_40 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_22 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_40})\n    %alloc_41 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_23 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_sub_tensor_1, 0, 36), kwargs = {out: %alloc_41})\n    %alloc_42 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_24 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_42})\n    %alloc_43 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_25 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_43})\n    %alloc_44 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_26 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_44})\n    %alloc_45 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_27 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_2, 0, 36), kwargs = {out: %alloc_45})\n    %alloc_46 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_28 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_46})\n    %alloc_47 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_29 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_47})\n    %alloc_48 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_30 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_48})\n    %alloc_49 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.int64),), kwargs = {})\n    %aten_clamp_default_31 : [num_users=1] = call_function[target=torch.ops.aten.clamp.out](args = (%aten_add_tensor_3, 0, 36), kwargs = {out: %alloc_49})\n    %getitem_8 : [num_users=16] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 0), kwargs = {})\n    %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 1), kwargs = {})\n    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 2), kwargs = {})\n    %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 3), kwargs = {})\n    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 4), kwargs = {})\n    %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 5), kwargs = {})\n    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 6), kwargs = {})\n    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 7), kwargs = {})\n    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 8), kwargs = {})\n    %alloc_50 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_1, %aten_clamp_default_5]), kwargs = {out: %alloc_50})\n    %alloc_51 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_1 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default, %aten_clamp_default_21]), kwargs = {out: %alloc_51})\n    %alloc_52 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_2 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_2, %aten_clamp_default_25]), kwargs = {out: %alloc_52})\n    %alloc_53 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_3 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_3, %aten_clamp_default_29]), kwargs = {out: %alloc_53})\n    %alloc_54 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_4 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_8, %aten_clamp_default_20]), kwargs = {out: %alloc_54})\n    %alloc_55 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_5 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_9, %aten_clamp_default_4]), kwargs = {out: %alloc_55})\n    %alloc_56 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_10, %aten_clamp_default_24]), kwargs = {out: %alloc_56})\n    %alloc_57 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_7 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_11, %aten_clamp_default_28]), kwargs = {out: %alloc_57})\n    %alloc_58 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_8 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_12, %aten_clamp_default_22]), kwargs = {out: %alloc_58})\n    %alloc_59 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_9 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_13, %aten_clamp_default_6]), kwargs = {out: %alloc_59})\n    %alloc_60 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_10 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_14, %aten_clamp_default_26]), kwargs = {out: %alloc_60})\n    %alloc_61 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_11 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_15, %aten_clamp_default_30]), kwargs = {out: %alloc_61})\n    %alloc_62 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_12 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_16, %aten_clamp_default_23]), kwargs = {out: %alloc_62})\n    %alloc_63 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_13 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_17, %aten_clamp_default_7]), kwargs = {out: %alloc_63})\n    %alloc_64 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_14 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_18, %aten_clamp_default_27]), kwargs = {out: %alloc_64})\n    %alloc_65 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n    %aten_index_tensor_15 : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor_out](args = (%getitem_8, [None, None, %aten_clamp_default_19, %aten_clamp_default_31]), kwargs = {out: %alloc_65})\n    %alloc_66 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_9, [0]), kwargs = {out: %alloc_66})\n    %alloc_67 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_10, [0]), kwargs = {out: %alloc_67})\n    %alloc_68 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_2 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_11, [0]), kwargs = {out: %alloc_68})\n    %alloc_69 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_3 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_12, [0]), kwargs = {out: %alloc_69})\n    %alloc_70 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_4 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_13, [0]), kwargs = {out: %alloc_70})\n    %alloc_71 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16,), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_5 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_14, [0]), kwargs = {out: %alloc_71})\n    %alloc_72 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_6 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_15, [0]), kwargs = {out: %alloc_72})\n    %alloc_73 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((16, 1), torch.float32),), kwargs = {})\n    %aten_squeeze_copy_dims_7 : [num_users=1] = call_function[target=torch.ops.aten.squeeze_copy.dims_out](args = (%getitem_16, [0]), kwargs = {out: %alloc_73})\n    %lowered_module_3 : [num_users=1] = get_attr[target=lowered_module_3]\n    %executorch_call_delegate_3 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_3, %aten_index_tensor_5, %aten_squeeze_copy_dims, %aten_index_tensor, %aten_index_tensor_9, %aten_index_tensor_13, %aten_index_tensor_6, %aten_squeeze_copy_dims_1, %aten_index_tensor_2, %aten_index_tensor_10, %aten_index_tensor_14, %aten_index_tensor_4, %aten_squeeze_copy_dims_4, %aten_index_tensor_1, %aten_index_tensor_8, %aten_index_tensor_12, %aten_index_tensor_7, %aten_squeeze_copy_dims_5, %aten_index_tensor_3, %aten_index_tensor_11, %aten_index_tensor_15, %aten_squeeze_copy_dims_6, %aten_squeeze_copy_dims_2, %aten_squeeze_copy_dims_3, %aten_squeeze_copy_dims_7), kwargs = {})\n    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 0), kwargs = {})\n    %alloc_74 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384, 16, 16), torch.float32),), kwargs = {})\n--> %aten_clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_17,), kwargs = {memory_format: torch.channels_last, out: %alloc_74})\n    %lowered_module_4 : [num_users=1] = get_attr[target=lowered_module_4]\n    %executorch_call_delegate_4 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_4, %aten_clone_default, %args_0), kwargs = {})\n    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 0), kwargs = {})\n    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 1), kwargs = {})\n    %aten_view_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_18, [1, -1, 384]), kwargs = {})\n    %aten_view_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_19, [1, 384, 256]), kwargs = {})\n    %lowered_module_5 : [num_users=1] = get_attr[target=lowered_module_5]\n    %executorch_call_delegate_5 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_5, %aten_view_copy_default_6, %aten_unsqueeze_copy_default, %aten_view_copy_default_5, %aten_expand_copy_default), kwargs = {})\n    %getitem_20 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 0), kwargs = {})\n    %alloc_75 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_76 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_77 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_20, [384], %p_blocks_0_norm1_weight, %p_blocks_0_norm1_bias, 1e-06), kwargs = {out0: %alloc_75, out1: %alloc_76, out2: %alloc_77})\n    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default, 0), kwargs = {})\n    %lowered_module_6 : [num_users=1] = get_attr[target=lowered_module_6]\n    %executorch_call_delegate_6 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_6, %getitem_21), kwargs = {})\n    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 0), kwargs = {})\n    %aten_view_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_22, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_7 : [num_users=1] = get_attr[target=lowered_module_7]\n    %executorch_call_delegate_7 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_7, %aten_view_copy_default_7), kwargs = {})\n    %getitem_23 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_7, 0), kwargs = {})\n    %alloc_78 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_1 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 0), kwargs = {out: %alloc_78})\n    %alloc_79 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_2 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 1), kwargs = {out: %alloc_79})\n    %alloc_80 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_3 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_23, 0, 2), kwargs = {out: %alloc_80})\n    %lowered_module_8 : [num_users=1] = get_attr[target=lowered_module_8]\n    %executorch_call_delegate_8 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_8, %aten_select_copy_int_1, %_lifted_tensor_constant32, %aten_select_copy_int_2), kwargs = {})\n    %alloc_81 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_1 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_3, [1, 6, 257, 64]), kwargs = {out: %alloc_81})\n    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 0), kwargs = {})\n    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 1), kwargs = {})\n    %aten_view_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_1, [6, 257, 64]), kwargs = {})\n    %alloc_82 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_2 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_24, [1, 6, 257, 64]), kwargs = {out: %alloc_82})\n    %alloc_83 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_3 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_25, [1, 6, 64, 257]), kwargs = {out: %alloc_83})\n    %aten_view_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_2, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_3, [6, 64, 257]), kwargs = {})\n    %lowered_module_9 : [num_users=1] = get_attr[target=lowered_module_9]\n    %executorch_call_delegate_9 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_9, %aten_view_copy_default_9, %aten_view_copy_default_10), kwargs = {})\n    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 0), kwargs = {})\n    %aten_view_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_26, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_10 : [num_users=1] = get_attr[target=lowered_module_10]\n    %executorch_call_delegate_10 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_10, %aten_view_copy_default_11), kwargs = {})\n    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 0), kwargs = {})\n    %alloc_84 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_27,), kwargs = {out: %alloc_84})\n    %alloc_85 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_4 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_1, [1, 6, 257, 257]), kwargs = {out: %alloc_85})\n    %aten_view_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_4, [6, 257, 257]), kwargs = {})\n    %lowered_module_11 : [num_users=1] = get_attr[target=lowered_module_11]\n    %executorch_call_delegate_11 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_11, %aten_view_copy_default_12, %aten_view_copy_default_8), kwargs = {})\n    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_11, 0), kwargs = {})\n    %aten_view_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_28, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_12 : [num_users=1] = get_attr[target=lowered_module_12]\n    %executorch_call_delegate_12 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_12, %aten_view_copy_default_13), kwargs = {})\n    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 0), kwargs = {})\n    %alloc_86 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_2 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_29,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_86})\n    %aten_view_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_2, [1, 257, 384]), kwargs = {})\n    %lowered_module_13 : [num_users=1] = get_attr[target=lowered_module_13]\n    %executorch_call_delegate_13 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_13, %aten_view_copy_default_14), kwargs = {})\n    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 0), kwargs = {})\n    %alloc_87 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_3 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_30,), kwargs = {out: %alloc_87})\n    %lowered_module_14 : [num_users=1] = get_attr[target=lowered_module_14]\n    %executorch_call_delegate_14 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_14, %aten_clone_default_3, %p_blocks_0_ls1_gamma, %getitem_20), kwargs = {})\n    %getitem_31 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 0), kwargs = {})\n    %alloc_88 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_89 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_90 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_1 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_31, [384], %p_blocks_0_norm2_weight, %p_blocks_0_norm2_bias, 1e-06), kwargs = {out0: %alloc_88, out1: %alloc_89, out2: %alloc_90})\n    %getitem_32 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_1, 0), kwargs = {})\n    %lowered_module_15 : [num_users=1] = get_attr[target=lowered_module_15]\n    %executorch_call_delegate_15 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_15, %getitem_32), kwargs = {})\n    %getitem_33 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_15, 0), kwargs = {})\n    %alloc_91 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_4 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_33,), kwargs = {out: %alloc_91})\n    %lowered_module_16 : [num_users=1] = get_attr[target=lowered_module_16]\n    %executorch_call_delegate_16 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_16, %aten_clone_default_4), kwargs = {})\n    %getitem_34 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 0), kwargs = {})\n    %alloc_92 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_5 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_34,), kwargs = {out: %alloc_92})\n    %lowered_module_17 : [num_users=1] = get_attr[target=lowered_module_17]\n    %executorch_call_delegate_17 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_17, %aten_clone_default_5, %p_blocks_0_ls2_gamma, %getitem_31), kwargs = {})\n    %getitem_35 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 0), kwargs = {})\n    %alloc_93 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_94 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_95 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_2 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_35, [384], %p_blocks_1_norm1_weight, %p_blocks_1_norm1_bias, 1e-06), kwargs = {out0: %alloc_93, out1: %alloc_94, out2: %alloc_95})\n    %getitem_36 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_2, 0), kwargs = {})\n    %lowered_module_18 : [num_users=1] = get_attr[target=lowered_module_18]\n    %executorch_call_delegate_18 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_18, %getitem_36), kwargs = {})\n    %getitem_37 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 0), kwargs = {})\n    %aten_view_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_37, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_19 : [num_users=1] = get_attr[target=lowered_module_19]\n    %executorch_call_delegate_19 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_19, %aten_view_copy_default_15), kwargs = {})\n    %getitem_38 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_19, 0), kwargs = {})\n    %alloc_96 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_4 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 0), kwargs = {out: %alloc_96})\n    %alloc_97 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_5 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 1), kwargs = {out: %alloc_97})\n    %alloc_98 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_6 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_38, 0, 2), kwargs = {out: %alloc_98})\n    %lowered_module_20 : [num_users=1] = get_attr[target=lowered_module_20]\n    %executorch_call_delegate_20 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_20, %aten_select_copy_int_4, %_lifted_tensor_constant33, %aten_select_copy_int_5), kwargs = {})\n    %alloc_99 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_5 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_6, [1, 6, 257, 64]), kwargs = {out: %alloc_99})\n    %getitem_39 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 0), kwargs = {})\n    %getitem_40 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 1), kwargs = {})\n    %aten_view_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_5, [6, 257, 64]), kwargs = {})\n    %alloc_100 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_6 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_39, [1, 6, 257, 64]), kwargs = {out: %alloc_100})\n    %alloc_101 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_7 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_40, [1, 6, 64, 257]), kwargs = {out: %alloc_101})\n    %aten_view_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_6, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_7, [6, 64, 257]), kwargs = {})\n    %lowered_module_21 : [num_users=1] = get_attr[target=lowered_module_21]\n    %executorch_call_delegate_21 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_21, %aten_view_copy_default_17, %aten_view_copy_default_18), kwargs = {})\n    %getitem_41 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 0), kwargs = {})\n    %aten_view_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_41, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_22 : [num_users=1] = get_attr[target=lowered_module_22]\n    %executorch_call_delegate_22 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_22, %aten_view_copy_default_19), kwargs = {})\n    %getitem_42 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 0), kwargs = {})\n    %alloc_102 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_6 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_42,), kwargs = {out: %alloc_102})\n    %alloc_103 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_8 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_6, [1, 6, 257, 257]), kwargs = {out: %alloc_103})\n    %aten_view_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_8, [6, 257, 257]), kwargs = {})\n    %lowered_module_23 : [num_users=1] = get_attr[target=lowered_module_23]\n    %executorch_call_delegate_23 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_23, %aten_view_copy_default_20, %aten_view_copy_default_16), kwargs = {})\n    %getitem_43 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_23, 0), kwargs = {})\n    %aten_view_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_43, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_24 : [num_users=1] = get_attr[target=lowered_module_24]\n    %executorch_call_delegate_24 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_24, %aten_view_copy_default_21), kwargs = {})\n    %getitem_44 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 0), kwargs = {})\n    %alloc_104 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_7 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_44,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_104})\n    %aten_view_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_7, [1, 257, 384]), kwargs = {})\n    %lowered_module_25 : [num_users=1] = get_attr[target=lowered_module_25]\n    %executorch_call_delegate_25 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_25, %aten_view_copy_default_22), kwargs = {})\n    %getitem_45 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_25, 0), kwargs = {})\n    %alloc_105 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_8 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_45,), kwargs = {out: %alloc_105})\n    %lowered_module_26 : [num_users=1] = get_attr[target=lowered_module_26]\n    %executorch_call_delegate_26 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_26, %aten_clone_default_8, %p_blocks_1_ls1_gamma, %getitem_35), kwargs = {})\n    %getitem_46 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_26, 0), kwargs = {})\n    %alloc_106 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_107 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_108 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_3 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_46, [384], %p_blocks_1_norm2_weight, %p_blocks_1_norm2_bias, 1e-06), kwargs = {out0: %alloc_106, out1: %alloc_107, out2: %alloc_108})\n    %getitem_47 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_3, 0), kwargs = {})\n    %lowered_module_27 : [num_users=1] = get_attr[target=lowered_module_27]\n    %executorch_call_delegate_27 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_27, %getitem_47), kwargs = {})\n    %getitem_48 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_27, 0), kwargs = {})\n    %alloc_109 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_9 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_48,), kwargs = {out: %alloc_109})\n    %lowered_module_28 : [num_users=1] = get_attr[target=lowered_module_28]\n    %executorch_call_delegate_28 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_28, %aten_clone_default_9), kwargs = {})\n    %getitem_49 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 0), kwargs = {})\n    %alloc_110 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_10 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_49,), kwargs = {out: %alloc_110})\n    %lowered_module_29 : [num_users=1] = get_attr[target=lowered_module_29]\n    %executorch_call_delegate_29 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_29, %aten_clone_default_10, %p_blocks_1_ls2_gamma, %getitem_46), kwargs = {})\n    %getitem_50 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_29, 0), kwargs = {})\n    %alloc_111 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_112 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_113 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_4 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_50, [384], %p_blocks_2_norm1_weight, %p_blocks_2_norm1_bias, 1e-06), kwargs = {out0: %alloc_111, out1: %alloc_112, out2: %alloc_113})\n    %getitem_51 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_4, 0), kwargs = {})\n    %lowered_module_30 : [num_users=1] = get_attr[target=lowered_module_30]\n    %executorch_call_delegate_30 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_30, %getitem_51), kwargs = {})\n    %getitem_52 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_30, 0), kwargs = {})\n    %aten_view_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_52, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_31 : [num_users=1] = get_attr[target=lowered_module_31]\n    %executorch_call_delegate_31 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_31, %aten_view_copy_default_23), kwargs = {})\n    %getitem_53 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_31, 0), kwargs = {})\n    %alloc_114 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_7 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 0), kwargs = {out: %alloc_114})\n    %alloc_115 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_8 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 1), kwargs = {out: %alloc_115})\n    %alloc_116 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_9 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_53, 0, 2), kwargs = {out: %alloc_116})\n    %lowered_module_32 : [num_users=1] = get_attr[target=lowered_module_32]\n    %executorch_call_delegate_32 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_32, %aten_select_copy_int_7, %_lifted_tensor_constant34, %aten_select_copy_int_8), kwargs = {})\n    %alloc_117 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_9 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_9, [1, 6, 257, 64]), kwargs = {out: %alloc_117})\n    %getitem_54 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 0), kwargs = {})\n    %getitem_55 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 1), kwargs = {})\n    %aten_view_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_9, [6, 257, 64]), kwargs = {})\n    %alloc_118 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_10 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_54, [1, 6, 257, 64]), kwargs = {out: %alloc_118})\n    %alloc_119 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_11 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_55, [1, 6, 64, 257]), kwargs = {out: %alloc_119})\n    %aten_view_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_10, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_11, [6, 64, 257]), kwargs = {})\n    %lowered_module_33 : [num_users=1] = get_attr[target=lowered_module_33]\n    %executorch_call_delegate_33 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_33, %aten_view_copy_default_25, %aten_view_copy_default_26), kwargs = {})\n    %getitem_56 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_33, 0), kwargs = {})\n    %aten_view_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_56, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_34 : [num_users=1] = get_attr[target=lowered_module_34]\n    %executorch_call_delegate_34 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_34, %aten_view_copy_default_27), kwargs = {})\n    %getitem_57 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_34, 0), kwargs = {})\n    %alloc_120 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_11 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_57,), kwargs = {out: %alloc_120})\n    %alloc_121 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_12 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_11, [1, 6, 257, 257]), kwargs = {out: %alloc_121})\n    %aten_view_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_12, [6, 257, 257]), kwargs = {})\n    %lowered_module_35 : [num_users=1] = get_attr[target=lowered_module_35]\n    %executorch_call_delegate_35 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_35, %aten_view_copy_default_28, %aten_view_copy_default_24), kwargs = {})\n    %getitem_58 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_35, 0), kwargs = {})\n    %aten_view_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_58, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_36 : [num_users=1] = get_attr[target=lowered_module_36]\n    %executorch_call_delegate_36 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_36, %aten_view_copy_default_29), kwargs = {})\n    %getitem_59 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 0), kwargs = {})\n    %alloc_122 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_12 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_59,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_122})\n    %aten_view_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_12, [1, 257, 384]), kwargs = {})\n    %lowered_module_37 : [num_users=1] = get_attr[target=lowered_module_37]\n    %executorch_call_delegate_37 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_37, %aten_view_copy_default_30), kwargs = {})\n    %getitem_60 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_37, 0), kwargs = {})\n    %alloc_123 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_13 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_60,), kwargs = {out: %alloc_123})\n    %lowered_module_38 : [num_users=1] = get_attr[target=lowered_module_38]\n    %executorch_call_delegate_38 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_38, %aten_clone_default_13, %p_blocks_2_ls1_gamma, %getitem_50), kwargs = {})\n    %getitem_61 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_38, 0), kwargs = {})\n    %alloc_124 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_125 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_126 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_5 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_61, [384], %p_blocks_2_norm2_weight, %p_blocks_2_norm2_bias, 1e-06), kwargs = {out0: %alloc_124, out1: %alloc_125, out2: %alloc_126})\n    %getitem_62 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_5, 0), kwargs = {})\n    %lowered_module_39 : [num_users=1] = get_attr[target=lowered_module_39]\n    %executorch_call_delegate_39 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_39, %getitem_62), kwargs = {})\n    %getitem_63 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_39, 0), kwargs = {})\n    %alloc_127 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_14 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_63,), kwargs = {out: %alloc_127})\n    %lowered_module_40 : [num_users=1] = get_attr[target=lowered_module_40]\n    %executorch_call_delegate_40 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_40, %aten_clone_default_14), kwargs = {})\n    %getitem_64 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 0), kwargs = {})\n    %alloc_128 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_15 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_64,), kwargs = {out: %alloc_128})\n    %lowered_module_41 : [num_users=1] = get_attr[target=lowered_module_41]\n    %executorch_call_delegate_41 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_41, %aten_clone_default_15, %p_blocks_2_ls2_gamma, %getitem_61), kwargs = {})\n    %getitem_65 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_41, 0), kwargs = {})\n    %alloc_129 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_130 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_131 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_6 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_65, [384], %p_blocks_3_norm1_weight, %p_blocks_3_norm1_bias, 1e-06), kwargs = {out0: %alloc_129, out1: %alloc_130, out2: %alloc_131})\n    %getitem_66 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_6, 0), kwargs = {})\n    %lowered_module_42 : [num_users=1] = get_attr[target=lowered_module_42]\n    %executorch_call_delegate_42 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_42, %getitem_66), kwargs = {})\n    %getitem_67 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_42, 0), kwargs = {})\n    %aten_view_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_67, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_43 : [num_users=1] = get_attr[target=lowered_module_43]\n    %executorch_call_delegate_43 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_43, %aten_view_copy_default_31), kwargs = {})\n    %getitem_68 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_43, 0), kwargs = {})\n    %alloc_132 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_10 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 0), kwargs = {out: %alloc_132})\n    %alloc_133 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_11 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 1), kwargs = {out: %alloc_133})\n    %alloc_134 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_12 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_68, 0, 2), kwargs = {out: %alloc_134})\n    %lowered_module_44 : [num_users=1] = get_attr[target=lowered_module_44]\n    %executorch_call_delegate_44 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_44, %aten_select_copy_int_10, %_lifted_tensor_constant35, %aten_select_copy_int_11), kwargs = {})\n    %alloc_135 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_13 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_12, [1, 6, 257, 64]), kwargs = {out: %alloc_135})\n    %getitem_69 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 0), kwargs = {})\n    %getitem_70 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 1), kwargs = {})\n    %aten_view_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_13, [6, 257, 64]), kwargs = {})\n    %alloc_136 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_14 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_69, [1, 6, 257, 64]), kwargs = {out: %alloc_136})\n    %alloc_137 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_15 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_70, [1, 6, 64, 257]), kwargs = {out: %alloc_137})\n    %aten_view_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_14, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_15, [6, 64, 257]), kwargs = {})\n    %lowered_module_45 : [num_users=1] = get_attr[target=lowered_module_45]\n    %executorch_call_delegate_45 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_45, %aten_view_copy_default_33, %aten_view_copy_default_34), kwargs = {})\n    %getitem_71 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_45, 0), kwargs = {})\n    %aten_view_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_71, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_46 : [num_users=1] = get_attr[target=lowered_module_46]\n    %executorch_call_delegate_46 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_46, %aten_view_copy_default_35), kwargs = {})\n    %getitem_72 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_46, 0), kwargs = {})\n    %alloc_138 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_16 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_72,), kwargs = {out: %alloc_138})\n    %alloc_139 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_16 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_16, [1, 6, 257, 257]), kwargs = {out: %alloc_139})\n    %aten_view_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_16, [6, 257, 257]), kwargs = {})\n    %lowered_module_47 : [num_users=1] = get_attr[target=lowered_module_47]\n    %executorch_call_delegate_47 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_47, %aten_view_copy_default_36, %aten_view_copy_default_32), kwargs = {})\n    %getitem_73 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_47, 0), kwargs = {})\n    %aten_view_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_73, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_48 : [num_users=1] = get_attr[target=lowered_module_48]\n    %executorch_call_delegate_48 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_48, %aten_view_copy_default_37), kwargs = {})\n    %getitem_74 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_48, 0), kwargs = {})\n    %alloc_140 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_17 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_74,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_140})\n    %aten_view_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_17, [1, 257, 384]), kwargs = {})\n    %lowered_module_49 : [num_users=1] = get_attr[target=lowered_module_49]\n    %executorch_call_delegate_49 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_49, %aten_view_copy_default_38), kwargs = {})\n    %getitem_75 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_49, 0), kwargs = {})\n    %alloc_141 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_18 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_75,), kwargs = {out: %alloc_141})\n    %lowered_module_50 : [num_users=1] = get_attr[target=lowered_module_50]\n    %executorch_call_delegate_50 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_50, %aten_clone_default_18, %p_blocks_3_ls1_gamma, %getitem_65), kwargs = {})\n    %getitem_76 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_50, 0), kwargs = {})\n    %alloc_142 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_143 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_144 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_7 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_76, [384], %p_blocks_3_norm2_weight, %p_blocks_3_norm2_bias, 1e-06), kwargs = {out0: %alloc_142, out1: %alloc_143, out2: %alloc_144})\n    %getitem_77 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_7, 0), kwargs = {})\n    %lowered_module_51 : [num_users=1] = get_attr[target=lowered_module_51]\n    %executorch_call_delegate_51 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_51, %getitem_77), kwargs = {})\n    %getitem_78 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_51, 0), kwargs = {})\n    %alloc_145 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_19 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_78,), kwargs = {out: %alloc_145})\n    %lowered_module_52 : [num_users=1] = get_attr[target=lowered_module_52]\n    %executorch_call_delegate_52 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_52, %aten_clone_default_19), kwargs = {})\n    %getitem_79 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_52, 0), kwargs = {})\n    %alloc_146 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_20 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_79,), kwargs = {out: %alloc_146})\n    %lowered_module_53 : [num_users=1] = get_attr[target=lowered_module_53]\n    %executorch_call_delegate_53 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_53, %aten_clone_default_20, %p_blocks_3_ls2_gamma, %getitem_76), kwargs = {})\n    %getitem_80 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_53, 0), kwargs = {})\n    %alloc_147 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_148 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_149 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_8 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_80, [384], %p_blocks_4_norm1_weight, %p_blocks_4_norm1_bias, 1e-06), kwargs = {out0: %alloc_147, out1: %alloc_148, out2: %alloc_149})\n    %getitem_81 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_8, 0), kwargs = {})\n    %lowered_module_54 : [num_users=1] = get_attr[target=lowered_module_54]\n    %executorch_call_delegate_54 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_54, %getitem_81), kwargs = {})\n    %getitem_82 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_54, 0), kwargs = {})\n    %aten_view_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_82, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_55 : [num_users=1] = get_attr[target=lowered_module_55]\n    %executorch_call_delegate_55 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_55, %aten_view_copy_default_39), kwargs = {})\n    %getitem_83 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_55, 0), kwargs = {})\n    %alloc_150 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_13 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 0), kwargs = {out: %alloc_150})\n    %alloc_151 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_14 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 1), kwargs = {out: %alloc_151})\n    %alloc_152 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_15 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_83, 0, 2), kwargs = {out: %alloc_152})\n    %lowered_module_56 : [num_users=1] = get_attr[target=lowered_module_56]\n    %executorch_call_delegate_56 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_56, %aten_select_copy_int_13, %_lifted_tensor_constant36, %aten_select_copy_int_14), kwargs = {})\n    %alloc_153 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_17 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_15, [1, 6, 257, 64]), kwargs = {out: %alloc_153})\n    %getitem_84 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_56, 0), kwargs = {})\n    %getitem_85 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_56, 1), kwargs = {})\n    %aten_view_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_17, [6, 257, 64]), kwargs = {})\n    %alloc_154 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_18 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_84, [1, 6, 257, 64]), kwargs = {out: %alloc_154})\n    %alloc_155 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_19 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_85, [1, 6, 64, 257]), kwargs = {out: %alloc_155})\n    %aten_view_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_18, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_19, [6, 64, 257]), kwargs = {})\n    %lowered_module_57 : [num_users=1] = get_attr[target=lowered_module_57]\n    %executorch_call_delegate_57 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_57, %aten_view_copy_default_41, %aten_view_copy_default_42), kwargs = {})\n    %getitem_86 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_57, 0), kwargs = {})\n    %aten_view_copy_default_43 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_86, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_58 : [num_users=1] = get_attr[target=lowered_module_58]\n    %executorch_call_delegate_58 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_58, %aten_view_copy_default_43), kwargs = {})\n    %getitem_87 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_58, 0), kwargs = {})\n    %alloc_156 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_21 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_87,), kwargs = {out: %alloc_156})\n    %alloc_157 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_20 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_21, [1, 6, 257, 257]), kwargs = {out: %alloc_157})\n    %aten_view_copy_default_44 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_20, [6, 257, 257]), kwargs = {})\n    %lowered_module_59 : [num_users=1] = get_attr[target=lowered_module_59]\n    %executorch_call_delegate_59 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_59, %aten_view_copy_default_44, %aten_view_copy_default_40), kwargs = {})\n    %getitem_88 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_59, 0), kwargs = {})\n    %aten_view_copy_default_45 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_88, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_60 : [num_users=1] = get_attr[target=lowered_module_60]\n    %executorch_call_delegate_60 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_60, %aten_view_copy_default_45), kwargs = {})\n    %getitem_89 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_60, 0), kwargs = {})\n    %alloc_158 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_22 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_89,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_158})\n    %aten_view_copy_default_46 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_22, [1, 257, 384]), kwargs = {})\n    %lowered_module_61 : [num_users=1] = get_attr[target=lowered_module_61]\n    %executorch_call_delegate_61 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_61, %aten_view_copy_default_46), kwargs = {})\n    %getitem_90 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_61, 0), kwargs = {})\n    %alloc_159 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_23 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_90,), kwargs = {out: %alloc_159})\n    %lowered_module_62 : [num_users=1] = get_attr[target=lowered_module_62]\n    %executorch_call_delegate_62 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_62, %aten_clone_default_23, %p_blocks_4_ls1_gamma, %getitem_80), kwargs = {})\n    %getitem_91 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_62, 0), kwargs = {})\n    %alloc_160 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_161 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_162 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_9 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_91, [384], %p_blocks_4_norm2_weight, %p_blocks_4_norm2_bias, 1e-06), kwargs = {out0: %alloc_160, out1: %alloc_161, out2: %alloc_162})\n    %getitem_92 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_9, 0), kwargs = {})\n    %lowered_module_63 : [num_users=1] = get_attr[target=lowered_module_63]\n    %executorch_call_delegate_63 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_63, %getitem_92), kwargs = {})\n    %getitem_93 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_63, 0), kwargs = {})\n    %alloc_163 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_24 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_93,), kwargs = {out: %alloc_163})\n    %lowered_module_64 : [num_users=1] = get_attr[target=lowered_module_64]\n    %executorch_call_delegate_64 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_64, %aten_clone_default_24), kwargs = {})\n    %getitem_94 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_64, 0), kwargs = {})\n    %alloc_164 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_25 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_94,), kwargs = {out: %alloc_164})\n    %lowered_module_65 : [num_users=1] = get_attr[target=lowered_module_65]\n    %executorch_call_delegate_65 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_65, %aten_clone_default_25, %p_blocks_4_ls2_gamma, %getitem_91), kwargs = {})\n    %getitem_95 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_65, 0), kwargs = {})\n    %alloc_165 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_166 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_167 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_10 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_95, [384], %p_blocks_5_norm1_weight, %p_blocks_5_norm1_bias, 1e-06), kwargs = {out0: %alloc_165, out1: %alloc_166, out2: %alloc_167})\n    %getitem_96 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_10, 0), kwargs = {})\n    %lowered_module_66 : [num_users=1] = get_attr[target=lowered_module_66]\n    %executorch_call_delegate_66 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_66, %getitem_96), kwargs = {})\n    %getitem_97 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_66, 0), kwargs = {})\n    %aten_view_copy_default_47 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_97, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_67 : [num_users=1] = get_attr[target=lowered_module_67]\n    %executorch_call_delegate_67 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_67, %aten_view_copy_default_47), kwargs = {})\n    %getitem_98 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_67, 0), kwargs = {})\n    %alloc_168 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_16 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 0), kwargs = {out: %alloc_168})\n    %alloc_169 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_17 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 1), kwargs = {out: %alloc_169})\n    %alloc_170 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_18 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_98, 0, 2), kwargs = {out: %alloc_170})\n    %lowered_module_68 : [num_users=1] = get_attr[target=lowered_module_68]\n    %executorch_call_delegate_68 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_68, %aten_select_copy_int_16, %_lifted_tensor_constant37, %aten_select_copy_int_17), kwargs = {})\n    %alloc_171 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_21 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_18, [1, 6, 257, 64]), kwargs = {out: %alloc_171})\n    %getitem_99 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_68, 0), kwargs = {})\n    %getitem_100 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_68, 1), kwargs = {})\n    %aten_view_copy_default_48 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_21, [6, 257, 64]), kwargs = {})\n    %alloc_172 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_22 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_99, [1, 6, 257, 64]), kwargs = {out: %alloc_172})\n    %alloc_173 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_23 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_100, [1, 6, 64, 257]), kwargs = {out: %alloc_173})\n    %aten_view_copy_default_49 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_22, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_50 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_23, [6, 64, 257]), kwargs = {})\n    %lowered_module_69 : [num_users=1] = get_attr[target=lowered_module_69]\n    %executorch_call_delegate_69 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_69, %aten_view_copy_default_49, %aten_view_copy_default_50), kwargs = {})\n    %getitem_101 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_69, 0), kwargs = {})\n    %aten_view_copy_default_51 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_101, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_70 : [num_users=1] = get_attr[target=lowered_module_70]\n    %executorch_call_delegate_70 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_70, %aten_view_copy_default_51), kwargs = {})\n    %getitem_102 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_70, 0), kwargs = {})\n    %alloc_174 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_26 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_102,), kwargs = {out: %alloc_174})\n    %alloc_175 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_24 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_26, [1, 6, 257, 257]), kwargs = {out: %alloc_175})\n    %aten_view_copy_default_52 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_24, [6, 257, 257]), kwargs = {})\n    %lowered_module_71 : [num_users=1] = get_attr[target=lowered_module_71]\n    %executorch_call_delegate_71 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_71, %aten_view_copy_default_52, %aten_view_copy_default_48), kwargs = {})\n    %getitem_103 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_71, 0), kwargs = {})\n    %aten_view_copy_default_53 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_103, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_72 : [num_users=1] = get_attr[target=lowered_module_72]\n    %executorch_call_delegate_72 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_72, %aten_view_copy_default_53), kwargs = {})\n    %getitem_104 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_72, 0), kwargs = {})\n    %alloc_176 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_27 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_104,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_176})\n    %aten_view_copy_default_54 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_27, [1, 257, 384]), kwargs = {})\n    %lowered_module_73 : [num_users=1] = get_attr[target=lowered_module_73]\n    %executorch_call_delegate_73 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_73, %aten_view_copy_default_54), kwargs = {})\n    %getitem_105 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_73, 0), kwargs = {})\n    %alloc_177 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_28 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_105,), kwargs = {out: %alloc_177})\n    %lowered_module_74 : [num_users=1] = get_attr[target=lowered_module_74]\n    %executorch_call_delegate_74 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_74, %aten_clone_default_28, %p_blocks_5_ls1_gamma, %getitem_95), kwargs = {})\n    %getitem_106 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_74, 0), kwargs = {})\n    %alloc_178 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_179 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_180 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_11 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_106, [384], %p_blocks_5_norm2_weight, %p_blocks_5_norm2_bias, 1e-06), kwargs = {out0: %alloc_178, out1: %alloc_179, out2: %alloc_180})\n    %getitem_107 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_11, 0), kwargs = {})\n    %lowered_module_75 : [num_users=1] = get_attr[target=lowered_module_75]\n    %executorch_call_delegate_75 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_75, %getitem_107), kwargs = {})\n    %getitem_108 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_75, 0), kwargs = {})\n    %alloc_181 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_29 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_108,), kwargs = {out: %alloc_181})\n    %lowered_module_76 : [num_users=1] = get_attr[target=lowered_module_76]\n    %executorch_call_delegate_76 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_76, %aten_clone_default_29), kwargs = {})\n    %getitem_109 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_76, 0), kwargs = {})\n    %alloc_182 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_30 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_109,), kwargs = {out: %alloc_182})\n    %lowered_module_77 : [num_users=1] = get_attr[target=lowered_module_77]\n    %executorch_call_delegate_77 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_77, %aten_clone_default_30, %p_blocks_5_ls2_gamma, %getitem_106), kwargs = {})\n    %getitem_110 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_77, 0), kwargs = {})\n    %alloc_183 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_184 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_185 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_12 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_110, [384], %p_blocks_6_norm1_weight, %p_blocks_6_norm1_bias, 1e-06), kwargs = {out0: %alloc_183, out1: %alloc_184, out2: %alloc_185})\n    %getitem_111 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_12, 0), kwargs = {})\n    %lowered_module_78 : [num_users=1] = get_attr[target=lowered_module_78]\n    %executorch_call_delegate_78 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_78, %getitem_111), kwargs = {})\n    %getitem_112 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_78, 0), kwargs = {})\n    %aten_view_copy_default_55 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_112, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_79 : [num_users=1] = get_attr[target=lowered_module_79]\n    %executorch_call_delegate_79 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_79, %aten_view_copy_default_55), kwargs = {})\n    %getitem_113 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_79, 0), kwargs = {})\n    %alloc_186 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_19 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 0), kwargs = {out: %alloc_186})\n    %alloc_187 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_20 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 1), kwargs = {out: %alloc_187})\n    %alloc_188 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_21 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_113, 0, 2), kwargs = {out: %alloc_188})\n    %lowered_module_80 : [num_users=1] = get_attr[target=lowered_module_80]\n    %executorch_call_delegate_80 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_80, %aten_select_copy_int_19, %_lifted_tensor_constant38, %aten_select_copy_int_20), kwargs = {})\n    %alloc_189 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_25 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_21, [1, 6, 257, 64]), kwargs = {out: %alloc_189})\n    %getitem_114 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_80, 0), kwargs = {})\n    %getitem_115 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_80, 1), kwargs = {})\n    %aten_view_copy_default_56 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_25, [6, 257, 64]), kwargs = {})\n    %alloc_190 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_26 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_114, [1, 6, 257, 64]), kwargs = {out: %alloc_190})\n    %alloc_191 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_27 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_115, [1, 6, 64, 257]), kwargs = {out: %alloc_191})\n    %aten_view_copy_default_57 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_26, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_58 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_27, [6, 64, 257]), kwargs = {})\n    %lowered_module_81 : [num_users=1] = get_attr[target=lowered_module_81]\n    %executorch_call_delegate_81 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_81, %aten_view_copy_default_57, %aten_view_copy_default_58), kwargs = {})\n    %getitem_116 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_81, 0), kwargs = {})\n    %aten_view_copy_default_59 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_116, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_82 : [num_users=1] = get_attr[target=lowered_module_82]\n    %executorch_call_delegate_82 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_82, %aten_view_copy_default_59), kwargs = {})\n    %getitem_117 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_82, 0), kwargs = {})\n    %alloc_192 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_31 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_117,), kwargs = {out: %alloc_192})\n    %alloc_193 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_28 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_31, [1, 6, 257, 257]), kwargs = {out: %alloc_193})\n    %aten_view_copy_default_60 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_28, [6, 257, 257]), kwargs = {})\n    %lowered_module_83 : [num_users=1] = get_attr[target=lowered_module_83]\n    %executorch_call_delegate_83 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_83, %aten_view_copy_default_60, %aten_view_copy_default_56), kwargs = {})\n    %getitem_118 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_83, 0), kwargs = {})\n    %aten_view_copy_default_61 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_118, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_84 : [num_users=1] = get_attr[target=lowered_module_84]\n    %executorch_call_delegate_84 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_84, %aten_view_copy_default_61), kwargs = {})\n    %getitem_119 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_84, 0), kwargs = {})\n    %alloc_194 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_32 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_119,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_194})\n    %aten_view_copy_default_62 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_32, [1, 257, 384]), kwargs = {})\n    %lowered_module_85 : [num_users=1] = get_attr[target=lowered_module_85]\n    %executorch_call_delegate_85 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_85, %aten_view_copy_default_62), kwargs = {})\n    %getitem_120 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_85, 0), kwargs = {})\n    %alloc_195 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_33 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_120,), kwargs = {out: %alloc_195})\n    %lowered_module_86 : [num_users=1] = get_attr[target=lowered_module_86]\n    %executorch_call_delegate_86 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_86, %aten_clone_default_33, %p_blocks_6_ls1_gamma, %getitem_110), kwargs = {})\n    %getitem_121 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_86, 0), kwargs = {})\n    %alloc_196 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_197 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_198 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_13 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_121, [384], %p_blocks_6_norm2_weight, %p_blocks_6_norm2_bias, 1e-06), kwargs = {out0: %alloc_196, out1: %alloc_197, out2: %alloc_198})\n    %getitem_122 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_13, 0), kwargs = {})\n    %lowered_module_87 : [num_users=1] = get_attr[target=lowered_module_87]\n    %executorch_call_delegate_87 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_87, %getitem_122), kwargs = {})\n    %getitem_123 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_87, 0), kwargs = {})\n    %alloc_199 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_34 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_123,), kwargs = {out: %alloc_199})\n    %lowered_module_88 : [num_users=1] = get_attr[target=lowered_module_88]\n    %executorch_call_delegate_88 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_88, %aten_clone_default_34), kwargs = {})\n    %getitem_124 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_88, 0), kwargs = {})\n    %alloc_200 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_35 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_124,), kwargs = {out: %alloc_200})\n    %lowered_module_89 : [num_users=1] = get_attr[target=lowered_module_89]\n    %executorch_call_delegate_89 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_89, %aten_clone_default_35, %p_blocks_6_ls2_gamma, %getitem_121), kwargs = {})\n    %getitem_125 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_89, 0), kwargs = {})\n    %alloc_201 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_202 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_203 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_14 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_125, [384], %p_blocks_7_norm1_weight, %p_blocks_7_norm1_bias, 1e-06), kwargs = {out0: %alloc_201, out1: %alloc_202, out2: %alloc_203})\n    %getitem_126 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_14, 0), kwargs = {})\n    %lowered_module_90 : [num_users=1] = get_attr[target=lowered_module_90]\n    %executorch_call_delegate_90 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_90, %getitem_126), kwargs = {})\n    %getitem_127 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_90, 0), kwargs = {})\n    %aten_view_copy_default_63 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_127, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_91 : [num_users=1] = get_attr[target=lowered_module_91]\n    %executorch_call_delegate_91 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_91, %aten_view_copy_default_63), kwargs = {})\n    %getitem_128 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_91, 0), kwargs = {})\n    %alloc_204 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_22 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 0), kwargs = {out: %alloc_204})\n    %alloc_205 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_23 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 1), kwargs = {out: %alloc_205})\n    %alloc_206 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_24 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_128, 0, 2), kwargs = {out: %alloc_206})\n    %lowered_module_92 : [num_users=1] = get_attr[target=lowered_module_92]\n    %executorch_call_delegate_92 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_92, %aten_select_copy_int_22, %_lifted_tensor_constant39, %aten_select_copy_int_23), kwargs = {})\n    %alloc_207 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_29 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_24, [1, 6, 257, 64]), kwargs = {out: %alloc_207})\n    %getitem_129 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_92, 0), kwargs = {})\n    %getitem_130 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_92, 1), kwargs = {})\n    %aten_view_copy_default_64 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_29, [6, 257, 64]), kwargs = {})\n    %alloc_208 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_30 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_129, [1, 6, 257, 64]), kwargs = {out: %alloc_208})\n    %alloc_209 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_31 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_130, [1, 6, 64, 257]), kwargs = {out: %alloc_209})\n    %aten_view_copy_default_65 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_30, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_66 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_31, [6, 64, 257]), kwargs = {})\n    %lowered_module_93 : [num_users=1] = get_attr[target=lowered_module_93]\n    %executorch_call_delegate_93 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_93, %aten_view_copy_default_65, %aten_view_copy_default_66), kwargs = {})\n    %getitem_131 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_93, 0), kwargs = {})\n    %aten_view_copy_default_67 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_131, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_94 : [num_users=1] = get_attr[target=lowered_module_94]\n    %executorch_call_delegate_94 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_94, %aten_view_copy_default_67), kwargs = {})\n    %getitem_132 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_94, 0), kwargs = {})\n    %alloc_210 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_36 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_132,), kwargs = {out: %alloc_210})\n    %alloc_211 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_32 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_36, [1, 6, 257, 257]), kwargs = {out: %alloc_211})\n    %aten_view_copy_default_68 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_32, [6, 257, 257]), kwargs = {})\n    %lowered_module_95 : [num_users=1] = get_attr[target=lowered_module_95]\n    %executorch_call_delegate_95 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_95, %aten_view_copy_default_68, %aten_view_copy_default_64), kwargs = {})\n    %getitem_133 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_95, 0), kwargs = {})\n    %aten_view_copy_default_69 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_133, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_96 : [num_users=1] = get_attr[target=lowered_module_96]\n    %executorch_call_delegate_96 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_96, %aten_view_copy_default_69), kwargs = {})\n    %getitem_134 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_96, 0), kwargs = {})\n    %alloc_212 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_37 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_134,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_212})\n    %aten_view_copy_default_70 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_37, [1, 257, 384]), kwargs = {})\n    %lowered_module_97 : [num_users=1] = get_attr[target=lowered_module_97]\n    %executorch_call_delegate_97 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_97, %aten_view_copy_default_70), kwargs = {})\n    %getitem_135 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_97, 0), kwargs = {})\n    %alloc_213 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_38 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_135,), kwargs = {out: %alloc_213})\n    %lowered_module_98 : [num_users=1] = get_attr[target=lowered_module_98]\n    %executorch_call_delegate_98 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_98, %aten_clone_default_38, %p_blocks_7_ls1_gamma, %getitem_125), kwargs = {})\n    %getitem_136 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_98, 0), kwargs = {})\n    %alloc_214 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_215 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_216 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_15 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_136, [384], %p_blocks_7_norm2_weight, %p_blocks_7_norm2_bias, 1e-06), kwargs = {out0: %alloc_214, out1: %alloc_215, out2: %alloc_216})\n    %getitem_137 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_15, 0), kwargs = {})\n    %lowered_module_99 : [num_users=1] = get_attr[target=lowered_module_99]\n    %executorch_call_delegate_99 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_99, %getitem_137), kwargs = {})\n    %getitem_138 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_99, 0), kwargs = {})\n    %alloc_217 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_39 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_138,), kwargs = {out: %alloc_217})\n    %lowered_module_100 : [num_users=1] = get_attr[target=lowered_module_100]\n    %executorch_call_delegate_100 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_100, %aten_clone_default_39), kwargs = {})\n    %getitem_139 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_100, 0), kwargs = {})\n    %alloc_218 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_40 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_139,), kwargs = {out: %alloc_218})\n    %lowered_module_101 : [num_users=1] = get_attr[target=lowered_module_101]\n    %executorch_call_delegate_101 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_101, %aten_clone_default_40, %p_blocks_7_ls2_gamma, %getitem_136), kwargs = {})\n    %getitem_140 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_101, 0), kwargs = {})\n    %alloc_219 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_220 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_221 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_16 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_140, [384], %p_blocks_8_norm1_weight, %p_blocks_8_norm1_bias, 1e-06), kwargs = {out0: %alloc_219, out1: %alloc_220, out2: %alloc_221})\n    %getitem_141 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_16, 0), kwargs = {})\n    %lowered_module_102 : [num_users=1] = get_attr[target=lowered_module_102]\n    %executorch_call_delegate_102 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_102, %getitem_141), kwargs = {})\n    %getitem_142 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_102, 0), kwargs = {})\n    %aten_view_copy_default_71 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_142, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_103 : [num_users=1] = get_attr[target=lowered_module_103]\n    %executorch_call_delegate_103 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_103, %aten_view_copy_default_71), kwargs = {})\n    %getitem_143 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_103, 0), kwargs = {})\n    %alloc_222 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_25 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 0), kwargs = {out: %alloc_222})\n    %alloc_223 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_26 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 1), kwargs = {out: %alloc_223})\n    %alloc_224 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_27 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_143, 0, 2), kwargs = {out: %alloc_224})\n    %lowered_module_104 : [num_users=1] = get_attr[target=lowered_module_104]\n    %executorch_call_delegate_104 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_104, %aten_select_copy_int_25, %_lifted_tensor_constant40, %aten_select_copy_int_26), kwargs = {})\n    %alloc_225 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_33 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_27, [1, 6, 257, 64]), kwargs = {out: %alloc_225})\n    %getitem_144 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_104, 0), kwargs = {})\n    %getitem_145 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_104, 1), kwargs = {})\n    %aten_view_copy_default_72 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_33, [6, 257, 64]), kwargs = {})\n    %alloc_226 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_34 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_144, [1, 6, 257, 64]), kwargs = {out: %alloc_226})\n    %alloc_227 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_35 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_145, [1, 6, 64, 257]), kwargs = {out: %alloc_227})\n    %aten_view_copy_default_73 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_34, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_74 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_35, [6, 64, 257]), kwargs = {})\n    %lowered_module_105 : [num_users=1] = get_attr[target=lowered_module_105]\n    %executorch_call_delegate_105 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_105, %aten_view_copy_default_73, %aten_view_copy_default_74), kwargs = {})\n    %getitem_146 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_105, 0), kwargs = {})\n    %aten_view_copy_default_75 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_146, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_106 : [num_users=1] = get_attr[target=lowered_module_106]\n    %executorch_call_delegate_106 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_106, %aten_view_copy_default_75), kwargs = {})\n    %getitem_147 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_106, 0), kwargs = {})\n    %alloc_228 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_41 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_147,), kwargs = {out: %alloc_228})\n    %alloc_229 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_36 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_41, [1, 6, 257, 257]), kwargs = {out: %alloc_229})\n    %aten_view_copy_default_76 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_36, [6, 257, 257]), kwargs = {})\n    %lowered_module_107 : [num_users=1] = get_attr[target=lowered_module_107]\n    %executorch_call_delegate_107 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_107, %aten_view_copy_default_76, %aten_view_copy_default_72), kwargs = {})\n    %getitem_148 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_107, 0), kwargs = {})\n    %aten_view_copy_default_77 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_148, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_108 : [num_users=1] = get_attr[target=lowered_module_108]\n    %executorch_call_delegate_108 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_108, %aten_view_copy_default_77), kwargs = {})\n    %getitem_149 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_108, 0), kwargs = {})\n    %alloc_230 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_42 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_149,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_230})\n    %aten_view_copy_default_78 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_42, [1, 257, 384]), kwargs = {})\n    %lowered_module_109 : [num_users=1] = get_attr[target=lowered_module_109]\n    %executorch_call_delegate_109 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_109, %aten_view_copy_default_78), kwargs = {})\n    %getitem_150 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_109, 0), kwargs = {})\n    %alloc_231 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_43 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_150,), kwargs = {out: %alloc_231})\n    %lowered_module_110 : [num_users=1] = get_attr[target=lowered_module_110]\n    %executorch_call_delegate_110 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_110, %aten_clone_default_43, %p_blocks_8_ls1_gamma, %getitem_140), kwargs = {})\n    %getitem_151 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_110, 0), kwargs = {})\n    %alloc_232 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_233 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_234 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_17 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_151, [384], %p_blocks_8_norm2_weight, %p_blocks_8_norm2_bias, 1e-06), kwargs = {out0: %alloc_232, out1: %alloc_233, out2: %alloc_234})\n    %getitem_152 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_17, 0), kwargs = {})\n    %lowered_module_111 : [num_users=1] = get_attr[target=lowered_module_111]\n    %executorch_call_delegate_111 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_111, %getitem_152), kwargs = {})\n    %getitem_153 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_111, 0), kwargs = {})\n    %alloc_235 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_44 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_153,), kwargs = {out: %alloc_235})\n    %lowered_module_112 : [num_users=1] = get_attr[target=lowered_module_112]\n    %executorch_call_delegate_112 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_112, %aten_clone_default_44), kwargs = {})\n    %getitem_154 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_112, 0), kwargs = {})\n    %alloc_236 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_45 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_154,), kwargs = {out: %alloc_236})\n    %lowered_module_113 : [num_users=1] = get_attr[target=lowered_module_113]\n    %executorch_call_delegate_113 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_113, %aten_clone_default_45, %p_blocks_8_ls2_gamma, %getitem_151), kwargs = {})\n    %getitem_155 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_113, 0), kwargs = {})\n    %alloc_237 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_238 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_239 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_18 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_155, [384], %p_blocks_9_norm1_weight, %p_blocks_9_norm1_bias, 1e-06), kwargs = {out0: %alloc_237, out1: %alloc_238, out2: %alloc_239})\n    %getitem_156 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_18, 0), kwargs = {})\n    %lowered_module_114 : [num_users=1] = get_attr[target=lowered_module_114]\n    %executorch_call_delegate_114 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_114, %getitem_156), kwargs = {})\n    %getitem_157 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_114, 0), kwargs = {})\n    %aten_view_copy_default_79 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_157, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_115 : [num_users=1] = get_attr[target=lowered_module_115]\n    %executorch_call_delegate_115 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_115, %aten_view_copy_default_79), kwargs = {})\n    %getitem_158 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_115, 0), kwargs = {})\n    %alloc_240 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_28 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 0), kwargs = {out: %alloc_240})\n    %alloc_241 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_29 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 1), kwargs = {out: %alloc_241})\n    %alloc_242 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_30 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_158, 0, 2), kwargs = {out: %alloc_242})\n    %lowered_module_116 : [num_users=1] = get_attr[target=lowered_module_116]\n    %executorch_call_delegate_116 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_116, %aten_select_copy_int_28, %_lifted_tensor_constant41, %aten_select_copy_int_29), kwargs = {})\n    %alloc_243 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_37 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_30, [1, 6, 257, 64]), kwargs = {out: %alloc_243})\n    %getitem_159 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_116, 0), kwargs = {})\n    %getitem_160 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_116, 1), kwargs = {})\n    %aten_view_copy_default_80 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_37, [6, 257, 64]), kwargs = {})\n    %alloc_244 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_38 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_159, [1, 6, 257, 64]), kwargs = {out: %alloc_244})\n    %alloc_245 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_39 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_160, [1, 6, 64, 257]), kwargs = {out: %alloc_245})\n    %aten_view_copy_default_81 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_38, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_82 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_39, [6, 64, 257]), kwargs = {})\n    %lowered_module_117 : [num_users=1] = get_attr[target=lowered_module_117]\n    %executorch_call_delegate_117 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_117, %aten_view_copy_default_81, %aten_view_copy_default_82), kwargs = {})\n    %getitem_161 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_117, 0), kwargs = {})\n    %aten_view_copy_default_83 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_161, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_118 : [num_users=1] = get_attr[target=lowered_module_118]\n    %executorch_call_delegate_118 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_118, %aten_view_copy_default_83), kwargs = {})\n    %getitem_162 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_118, 0), kwargs = {})\n    %alloc_246 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_46 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_162,), kwargs = {out: %alloc_246})\n    %alloc_247 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_40 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_46, [1, 6, 257, 257]), kwargs = {out: %alloc_247})\n    %aten_view_copy_default_84 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_40, [6, 257, 257]), kwargs = {})\n    %lowered_module_119 : [num_users=1] = get_attr[target=lowered_module_119]\n    %executorch_call_delegate_119 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_119, %aten_view_copy_default_84, %aten_view_copy_default_80), kwargs = {})\n    %getitem_163 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_119, 0), kwargs = {})\n    %aten_view_copy_default_85 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_163, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_120 : [num_users=1] = get_attr[target=lowered_module_120]\n    %executorch_call_delegate_120 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_120, %aten_view_copy_default_85), kwargs = {})\n    %getitem_164 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_120, 0), kwargs = {})\n    %alloc_248 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_47 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_164,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_248})\n    %aten_view_copy_default_86 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_47, [1, 257, 384]), kwargs = {})\n    %lowered_module_121 : [num_users=1] = get_attr[target=lowered_module_121]\n    %executorch_call_delegate_121 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_121, %aten_view_copy_default_86), kwargs = {})\n    %getitem_165 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_121, 0), kwargs = {})\n    %alloc_249 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_48 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_165,), kwargs = {out: %alloc_249})\n    %lowered_module_122 : [num_users=1] = get_attr[target=lowered_module_122]\n    %executorch_call_delegate_122 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_122, %aten_clone_default_48, %p_blocks_9_ls1_gamma, %getitem_155), kwargs = {})\n    %getitem_166 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_122, 0), kwargs = {})\n    %alloc_250 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_251 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_252 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_19 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_166, [384], %p_blocks_9_norm2_weight, %p_blocks_9_norm2_bias, 1e-06), kwargs = {out0: %alloc_250, out1: %alloc_251, out2: %alloc_252})\n    %getitem_167 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_19, 0), kwargs = {})\n    %lowered_module_123 : [num_users=1] = get_attr[target=lowered_module_123]\n    %executorch_call_delegate_123 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_123, %getitem_167), kwargs = {})\n    %getitem_168 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_123, 0), kwargs = {})\n    %alloc_253 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_49 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_168,), kwargs = {out: %alloc_253})\n    %lowered_module_124 : [num_users=1] = get_attr[target=lowered_module_124]\n    %executorch_call_delegate_124 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_124, %aten_clone_default_49), kwargs = {})\n    %getitem_169 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_124, 0), kwargs = {})\n    %alloc_254 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_50 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_169,), kwargs = {out: %alloc_254})\n    %lowered_module_125 : [num_users=1] = get_attr[target=lowered_module_125]\n    %executorch_call_delegate_125 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_125, %aten_clone_default_50, %p_blocks_9_ls2_gamma, %getitem_166), kwargs = {})\n    %getitem_170 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_125, 0), kwargs = {})\n    %alloc_255 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_256 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_257 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_20 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_170, [384], %p_blocks_10_norm1_weight, %p_blocks_10_norm1_bias, 1e-06), kwargs = {out0: %alloc_255, out1: %alloc_256, out2: %alloc_257})\n    %getitem_171 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_20, 0), kwargs = {})\n    %lowered_module_126 : [num_users=1] = get_attr[target=lowered_module_126]\n    %executorch_call_delegate_126 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_126, %getitem_171), kwargs = {})\n    %getitem_172 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_126, 0), kwargs = {})\n    %aten_view_copy_default_87 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_172, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_127 : [num_users=1] = get_attr[target=lowered_module_127]\n    %executorch_call_delegate_127 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_127, %aten_view_copy_default_87), kwargs = {})\n    %getitem_173 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_127, 0), kwargs = {})\n    %alloc_258 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_31 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 0), kwargs = {out: %alloc_258})\n    %alloc_259 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_32 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 1), kwargs = {out: %alloc_259})\n    %alloc_260 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_33 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_173, 0, 2), kwargs = {out: %alloc_260})\n    %lowered_module_128 : [num_users=1] = get_attr[target=lowered_module_128]\n    %executorch_call_delegate_128 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_128, %aten_select_copy_int_31, %_lifted_tensor_constant42, %aten_select_copy_int_32), kwargs = {})\n    %alloc_261 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_41 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_33, [1, 6, 257, 64]), kwargs = {out: %alloc_261})\n    %getitem_174 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_128, 0), kwargs = {})\n    %getitem_175 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_128, 1), kwargs = {})\n    %aten_view_copy_default_88 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_41, [6, 257, 64]), kwargs = {})\n    %alloc_262 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_42 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_174, [1, 6, 257, 64]), kwargs = {out: %alloc_262})\n    %alloc_263 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_43 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_175, [1, 6, 64, 257]), kwargs = {out: %alloc_263})\n    %aten_view_copy_default_89 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_42, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_90 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_43, [6, 64, 257]), kwargs = {})\n    %lowered_module_129 : [num_users=1] = get_attr[target=lowered_module_129]\n    %executorch_call_delegate_129 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_129, %aten_view_copy_default_89, %aten_view_copy_default_90), kwargs = {})\n    %getitem_176 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_129, 0), kwargs = {})\n    %aten_view_copy_default_91 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_176, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_130 : [num_users=1] = get_attr[target=lowered_module_130]\n    %executorch_call_delegate_130 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_130, %aten_view_copy_default_91), kwargs = {})\n    %getitem_177 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_130, 0), kwargs = {})\n    %alloc_264 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_51 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_177,), kwargs = {out: %alloc_264})\n    %alloc_265 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_44 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_51, [1, 6, 257, 257]), kwargs = {out: %alloc_265})\n    %aten_view_copy_default_92 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_44, [6, 257, 257]), kwargs = {})\n    %lowered_module_131 : [num_users=1] = get_attr[target=lowered_module_131]\n    %executorch_call_delegate_131 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_131, %aten_view_copy_default_92, %aten_view_copy_default_88), kwargs = {})\n    %getitem_178 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_131, 0), kwargs = {})\n    %aten_view_copy_default_93 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_178, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_132 : [num_users=1] = get_attr[target=lowered_module_132]\n    %executorch_call_delegate_132 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_132, %aten_view_copy_default_93), kwargs = {})\n    %getitem_179 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_132, 0), kwargs = {})\n    %alloc_266 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_52 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_179,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_266})\n    %aten_view_copy_default_94 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_52, [1, 257, 384]), kwargs = {})\n    %lowered_module_133 : [num_users=1] = get_attr[target=lowered_module_133]\n    %executorch_call_delegate_133 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_133, %aten_view_copy_default_94), kwargs = {})\n    %getitem_180 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_133, 0), kwargs = {})\n    %alloc_267 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_53 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_180,), kwargs = {out: %alloc_267})\n    %lowered_module_134 : [num_users=1] = get_attr[target=lowered_module_134]\n    %executorch_call_delegate_134 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_134, %aten_clone_default_53, %p_blocks_10_ls1_gamma, %getitem_170), kwargs = {})\n    %getitem_181 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_134, 0), kwargs = {})\n    %alloc_268 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_269 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_270 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_21 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_181, [384], %p_blocks_10_norm2_weight, %p_blocks_10_norm2_bias, 1e-06), kwargs = {out0: %alloc_268, out1: %alloc_269, out2: %alloc_270})\n    %getitem_182 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_21, 0), kwargs = {})\n    %lowered_module_135 : [num_users=1] = get_attr[target=lowered_module_135]\n    %executorch_call_delegate_135 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_135, %getitem_182), kwargs = {})\n    %getitem_183 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_135, 0), kwargs = {})\n    %alloc_271 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_54 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_183,), kwargs = {out: %alloc_271})\n    %lowered_module_136 : [num_users=1] = get_attr[target=lowered_module_136]\n    %executorch_call_delegate_136 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_136, %aten_clone_default_54), kwargs = {})\n    %getitem_184 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_136, 0), kwargs = {})\n    %alloc_272 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_55 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_184,), kwargs = {out: %alloc_272})\n    %lowered_module_137 : [num_users=1] = get_attr[target=lowered_module_137]\n    %executorch_call_delegate_137 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_137, %aten_clone_default_55, %p_blocks_10_ls2_gamma, %getitem_181), kwargs = {})\n    %getitem_185 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_137, 0), kwargs = {})\n    %alloc_273 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_274 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_275 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_22 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_185, [384], %p_blocks_11_norm1_weight, %p_blocks_11_norm1_bias, 1e-06), kwargs = {out0: %alloc_273, out1: %alloc_274, out2: %alloc_275})\n    %getitem_186 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_22, 0), kwargs = {})\n    %lowered_module_138 : [num_users=1] = get_attr[target=lowered_module_138]\n    %executorch_call_delegate_138 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_138, %getitem_186), kwargs = {})\n    %getitem_187 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_138, 0), kwargs = {})\n    %aten_view_copy_default_95 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_187, [1, 257, 3, 6, 64]), kwargs = {})\n    %lowered_module_139 : [num_users=1] = get_attr[target=lowered_module_139]\n    %executorch_call_delegate_139 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_139, %aten_view_copy_default_95), kwargs = {})\n    %getitem_188 : [num_users=3] = call_function[target=operator.getitem](args = (%executorch_call_delegate_139, 0), kwargs = {})\n    %alloc_276 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_34 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 0), kwargs = {out: %alloc_276})\n    %alloc_277 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_35 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 1), kwargs = {out: %alloc_277})\n    %alloc_278 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_select_copy_int_36 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_188, 0, 2), kwargs = {out: %alloc_278})\n    %lowered_module_140 : [num_users=1] = get_attr[target=lowered_module_140]\n    %executorch_call_delegate_140 : [num_users=2] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_140, %aten_select_copy_int_34, %_lifted_tensor_constant43, %aten_select_copy_int_35), kwargs = {})\n    %alloc_279 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_45 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_select_copy_int_36, [1, 6, 257, 64]), kwargs = {out: %alloc_279})\n    %getitem_189 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_140, 0), kwargs = {})\n    %getitem_190 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_140, 1), kwargs = {})\n    %aten_view_copy_default_96 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_45, [6, 257, 64]), kwargs = {})\n    %alloc_280 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 64), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_46 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_189, [1, 6, 257, 64]), kwargs = {out: %alloc_280})\n    %alloc_281 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 64, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_47 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%getitem_190, [1, 6, 64, 257]), kwargs = {out: %alloc_281})\n    %aten_view_copy_default_97 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_46, [6, 257, 64]), kwargs = {})\n    %aten_view_copy_default_98 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_47, [6, 64, 257]), kwargs = {})\n    %lowered_module_141 : [num_users=1] = get_attr[target=lowered_module_141]\n    %executorch_call_delegate_141 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_141, %aten_view_copy_default_97, %aten_view_copy_default_98), kwargs = {})\n    %getitem_191 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_141, 0), kwargs = {})\n    %aten_view_copy_default_99 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_191, [1, 6, 257, 257]), kwargs = {})\n    %lowered_module_142 : [num_users=1] = get_attr[target=lowered_module_142]\n    %executorch_call_delegate_142 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_142, %aten_view_copy_default_99), kwargs = {})\n    %getitem_192 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_142, 0), kwargs = {})\n    %alloc_282 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_clone_default_56 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_192,), kwargs = {out: %alloc_282})\n    %alloc_283 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 6, 257, 257), torch.float32),), kwargs = {})\n    %aten_expand_copy_default_48 : [num_users=1] = call_function[target=torch.ops.aten.expand_copy.out](args = (%aten_clone_default_56, [1, 6, 257, 257]), kwargs = {out: %alloc_283})\n    %aten_view_copy_default_100 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_expand_copy_default_48, [6, 257, 257]), kwargs = {})\n    %lowered_module_143 : [num_users=1] = get_attr[target=lowered_module_143]\n    %executorch_call_delegate_143 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_143, %aten_view_copy_default_100, %aten_view_copy_default_96), kwargs = {})\n    %getitem_193 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_143, 0), kwargs = {})\n    %aten_view_copy_default_101 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%getitem_193, [1, 6, 257, 64]), kwargs = {})\n    %lowered_module_144 : [num_users=1] = get_attr[target=lowered_module_144]\n    %executorch_call_delegate_144 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_144, %aten_view_copy_default_101), kwargs = {})\n    %getitem_194 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_144, 0), kwargs = {})\n    %alloc_284 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 6, 64), torch.float32),), kwargs = {})\n    %aten_clone_default_57 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_194,), kwargs = {memory_format: torch.contiguous_format, out: %alloc_284})\n    %aten_view_copy_default_102 : [num_users=1] = call_function[target=executorch.exir.memory.view](args = (%aten_clone_default_57, [1, 257, 384]), kwargs = {})\n    %lowered_module_145 : [num_users=1] = get_attr[target=lowered_module_145]\n    %executorch_call_delegate_145 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_145, %aten_view_copy_default_102), kwargs = {})\n    %getitem_195 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_145, 0), kwargs = {})\n    %alloc_285 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_58 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_195,), kwargs = {out: %alloc_285})\n    %lowered_module_146 : [num_users=1] = get_attr[target=lowered_module_146]\n    %executorch_call_delegate_146 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_146, %aten_clone_default_58, %p_blocks_11_ls1_gamma, %getitem_185), kwargs = {})\n    %getitem_196 : [num_users=2] = call_function[target=operator.getitem](args = (%executorch_call_delegate_146, 0), kwargs = {})\n    %alloc_286 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_287 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_288 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_23 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_196, [384], %p_blocks_11_norm2_weight, %p_blocks_11_norm2_bias, 1e-06), kwargs = {out0: %alloc_286, out1: %alloc_287, out2: %alloc_288})\n    %getitem_197 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_23, 0), kwargs = {})\n    %lowered_module_147 : [num_users=1] = get_attr[target=lowered_module_147]\n    %executorch_call_delegate_147 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_147, %getitem_197), kwargs = {})\n    %getitem_198 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_147, 0), kwargs = {})\n    %alloc_289 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1536), torch.float32),), kwargs = {})\n    %aten_clone_default_59 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_198,), kwargs = {out: %alloc_289})\n    %lowered_module_148 : [num_users=1] = get_attr[target=lowered_module_148]\n    %executorch_call_delegate_148 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_148, %aten_clone_default_59), kwargs = {})\n    %getitem_199 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_148, 0), kwargs = {})\n    %alloc_290 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %aten_clone_default_60 : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_199,), kwargs = {out: %alloc_290})\n    %lowered_module_149 : [num_users=1] = get_attr[target=lowered_module_149]\n    %executorch_call_delegate_149 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_149, %aten_clone_default_60, %p_blocks_11_ls2_gamma, %getitem_196), kwargs = {})\n    %getitem_200 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_149, 0), kwargs = {})\n    %alloc_291 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 384), torch.float32),), kwargs = {})\n    %alloc_292 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %alloc_293 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 257, 1), torch.float32),), kwargs = {})\n    %aten_native_layer_norm_default_24 : [num_users=1] = call_function[target=torch.ops.aten.native_layer_norm.out](args = (%getitem_200, [384], %p_norm_weight, %p_norm_bias, 1e-06), kwargs = {out0: %alloc_291, out1: %alloc_292, out2: %alloc_293})\n    %getitem_201 : [num_users=1] = call_function[target=operator.getitem](args = (%aten_native_layer_norm_default_24, 0), kwargs = {})\n    %alloc_294 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 384), torch.float32),), kwargs = {})\n    %aten_select_copy_int_37 : [num_users=1] = call_function[target=torch.ops.aten.select_copy.int_out](args = (%getitem_201, 1, 0), kwargs = {out: %alloc_294})\n    %lowered_module_150 : [num_users=1] = get_attr[target=lowered_module_150]\n    %executorch_call_delegate_150 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_150, %aten_select_copy_int_37), kwargs = {})\n    %getitem_202 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_150, 0), kwargs = {})\n    return (getitem_202,)\nThis node aten_clone_default has metadata of:\nThe node stacktrace:\nTraceback (most recent call last): \n    File \"/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 325, in forward\nret = self.forward_features(*args, **kwargs) \n\n\n\n\nWhile executing %aten_clone_default : [num_users=1] = call_function[target=torch.ops.aten.clone.out](args = (%getitem_17,), kwargs = {memory_format: torch.channels_last, out: %alloc_74})\nGraphModule: class GraphModule(torch.nn.Module):\n    def forward(self, p_cls_token: \"f32[1, 1, 384][384, 384, 1]\", p_pos_embed: \"f32[1, 1370, 384][526080, 384, 1]\", p_blocks_0_norm1_weight: \"f32[384][1]\", p_blocks_0_norm1_bias: \"f32[384][1]\", p_blocks_0_ls1_gamma: \"f32[384][1]\", p_blocks_0_norm2_weight: \"f32[384][1]\", p_blocks_0_norm2_bias: \"f32[384][1]\", p_blocks_0_ls2_gamma: \"f32[384][1]\", p_blocks_1_norm1_weight: \"f32[384][1]\", p_blocks_1_norm1_bias: \"f32[384][1]\", p_blocks_1_ls1_gamma: \"f32[384][1]\", p_blocks_1_norm2_weight: \"f32[384][1]\", p_blocks_1_norm2_bias: \"f32[384][1]\", p_blocks_1_ls2_gamma: \"f32[384][1]\", p_blocks_2_norm1_weight: \"f32[384][1]\", p_blocks_2_norm1_bias: \"f32[384][1]\", p_blocks_2_ls1_gamma: \"f32[384][1]\", p_blocks_2_norm2_weight: \"f32[384][1]\", p_blocks_2_norm2_bias: \"f32[384][1]\", p_blocks_2_ls2_gamma: \"f32[384][1]\", p_blocks_3_norm1_weight: \"f32[384][1]\", p_blocks_3_norm1_bias: \"f32[384][1]\", p_blocks_3_ls1_gamma: \"f32[384][1]\", p_blocks_3_norm2_weight: \"f32[384][1]\", p_blocks_3_norm2_bias: \"f32[384][1]\", p_blocks_3_ls2_gamma: \"f32[384][1]\", p_blocks_4_norm1_weight: \"f32[384][1]\", p_blocks_4_norm1_bias: \"f32[384][1]\", p_blocks_4_ls1_gamma: \"f32[384][1]\", p_blocks_4_norm2_weight: \"f32[384][1]\", p_blocks_4_norm2_bias: \"f32[384][1]\", p_blocks_4_ls2_gamma: \"f32[384][1]\", p_blocks_5_norm1_weight: \"f32[384][1]\", p_blocks_5_norm1_bias: \"f32[384][1]\", p_blocks_5_ls1_gamma: \"f32[384][1]\", p_blocks_5_norm2_weight: \"f32[384][1]\", p_blocks_5_norm2_bias: \"f32[384][1]\", p_blocks_5_ls2_gamma: \"f32[384][1]\", p_blocks_6_norm1_weight: \"f32[384][1]\", p_blocks_6_norm1_bias: \"f32[384][1]\", p_blocks_6_ls1_gamma: \"f32[384][1]\", p_blocks_6_norm2_weight: \"f32[384][1]\", p_blocks_6_norm2_bias: \"f32[384][1]\", p_blocks_6_ls2_gamma: \"f32[384][1]\", p_blocks_7_norm1_weight: \"f32[384][1]\", p_blocks_7_norm1_bias: \"f32[384][1]\", p_blocks_7_ls1_gamma: \"f32[384][1]\", p_blocks_7_norm2_weight: \"f32[384][1]\", p_blocks_7_norm2_bias: \"f32[384][1]\", p_blocks_7_ls2_gamma: \"f32[384][1]\", p_blocks_8_norm1_weight: \"f32[384][1]\", p_blocks_8_norm1_bias: \"f32[384][1]\", p_blocks_8_ls1_gamma: \"f32[384][1]\", p_blocks_8_norm2_weight: \"f32[384][1]\", p_blocks_8_norm2_bias: \"f32[384][1]\", p_blocks_8_ls2_gamma: \"f32[384][1]\", p_blocks_9_norm1_weight: \"f32[384][1]\", p_blocks_9_norm1_bias: \"f32[384][1]\", p_blocks_9_ls1_gamma: \"f32[384][1]\", p_blocks_9_norm2_weight: \"f32[384][1]\", p_blocks_9_norm2_bias: \"f32[384][1]\", p_blocks_9_ls2_gamma: \"f32[384][1]\", p_blocks_10_norm1_weight: \"f32[384][1]\", p_blocks_10_norm1_bias: \"f32[384][1]\", p_blocks_10_ls1_gamma: \"f32[384][1]\", p_blocks_10_norm2_weight: \"f32[384][1]\", p_blocks_10_norm2_bias: \"f32[384][1]\", p_blocks_10_ls2_gamma: \"f32[384][1]\", p_blocks_11_norm1_weight: \"f32[384][1]\", p_blocks_11_norm1_bias: \"f32[384][1]\", p_blocks_11_ls1_gamma: \"f32[384][1]\", p_blocks_11_norm2_weight: \"f32[384][1]\", p_blocks_11_norm2_bias: \"f32[384][1]\", p_blocks_11_ls2_gamma: \"f32[384][1]\", p_norm_weight: \"f32[384][1]\", p_norm_bias: \"f32[384][1]\", _lifted_tensor_constant0: \"f32[][]\", _lifted_tensor_constant1: \"f32[][]\", _lifted_tensor_constant2: \"f32[][]\", _lifted_tensor_constant3: \"f32[][]\", _lifted_tensor_constant4: \"f32[][]\", _lifted_tensor_constant5: \"f32[][]\", _lifted_tensor_constant6: \"i64[][]\", _lifted_tensor_constant7: \"i64[][]\", _lifted_tensor_constant8: \"i64[][]\", _lifted_tensor_constant9: \"i64[][]\", _lifted_tensor_constant10: \"i64[][]\", _lifted_tensor_constant11: \"i64[][]\", _lifted_tensor_constant12: \"f32[][]\", _lifted_tensor_constant13: \"f32[][]\", _lifted_tensor_constant14: \"f32[][]\", _lifted_tensor_constant15: \"f32[][]\", _lifted_tensor_constant16: \"f32[][]\", _lifted_tensor_constant17: \"f32[][]\", _lifted_tensor_constant18: \"f32[][]\", _lifted_tensor_constant19: \"f32[][]\", _lifted_tensor_constant20: \"f32[][]\", _lifted_tensor_constant21: \"i64[][]\", _lifted_tensor_constant22: \"f32[][]\", _lifted_tensor_constant23: \"f32[][]\", _lifted_tensor_constant24: \"f32[][]\", _lifted_tensor_constant25: \"f32[][]\", _lifted_tensor_constant26: \"f32[][]\", _lifted_tensor_constant27: \"f32[][]\", _lifted_tensor_constant28: \"f32[][]\", _lifted_tensor_constant29: \"f32[][]\", _lifted_tensor_constant30: \"f32[][]\", _lifted_tensor_constant31: \"i64[][]\", _lifted_tensor_constant32: \"f32[][]\", _lifted_tensor_constant33: \"f32[][]\", _lifted_tensor_constant34: \"f32[][]\", _lifted_tensor_constant35: \"f32[][]\", _lifted_tensor_constant36: \"f32[][]\", _lifted_tensor_constant37: \"f32[][]\", _lifted_tensor_constant38: \"f32[][]\", _lifted_tensor_constant39: \"f32[][]\", _lifted_tensor_constant40: \"f32[][]\", _lifted_tensor_constant41: \"f32[][]\", _lifted_tensor_constant42: \"f32[][]\", _lifted_tensor_constant43: \"f32[][]\", args_0: \"f32[1, 3, 224, 224][150528, 50176, 224, 1]\"):\n        # No stacktrace found for following nodes\n        alloc: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_arange_start_step: \"i64[16][1]\" = torch.ops.aten.arange.start_out(0, 16, out = alloc);  alloc = None\n        \n        # No stacktrace found for following nodes\n        alloc_1: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_arange_start_step_1: \"i64[16][1]\" = torch.ops.aten.arange.start_out(0, 16, out = alloc_1);  alloc_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_2: \"f32[1, 1, 384][384, 384, 1]\" = executorch_exir_memory_alloc(((1, 1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_expand_copy_default: \"f32[1, 1, 384][384, 384, 1]\" = torch.ops.aten.expand_copy.out(p_cls_token, [1, -1, -1], out = alloc_2);  p_cls_token = alloc_2 = None\n        \n        # No stacktrace found for following nodes\n        alloc_3: \"f32[1, 384][384, 1]\" = executorch_exir_memory_alloc(((1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_select_copy_int: \"f32[1, 384][384, 1]\" = torch.ops.aten.select_copy.int_out(p_pos_embed, 1, 0, out = alloc_3);  alloc_3 = None\n        \n        # No stacktrace found for following nodes\n        alloc_4: \"f32[][]\" = executorch_exir_memory_alloc(((), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default: \"f32[][]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(_lifted_tensor_constant21, dim_order = [], out = alloc_4);  _lifted_tensor_constant21 = alloc_4 = None\n        \n        # No stacktrace found for following nodes\n        alloc_5: \"f32[][]\" = executorch_exir_memory_alloc(((), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_1: \"f32[][]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(_lifted_tensor_constant31, dim_order = [], out = alloc_5);  _lifted_tensor_constant31 = alloc_5 = None\n        \n        # No stacktrace found for following nodes\n        alloc_6: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_2: \"f32[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(aten_arange_start_step, dim_order = [0], out = alloc_6);  aten_arange_start_step = alloc_6 = None\n        \n        # No stacktrace found for following nodes\n        alloc_7: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_3: \"f32[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(aten_arange_start_step_1, dim_order = [0], out = alloc_7);  aten_arange_start_step_1 = alloc_7 = None\n        \n        # No stacktrace found for following nodes\n        alloc_8: \"f32[1, 1, 384][384, 384, 1]\" = executorch_exir_memory_alloc(((1, 1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_unsqueeze_copy_default: \"f32[1, 1, 384][384, 384, 1]\" = torch.ops.aten.unsqueeze_copy.out(aten_select_copy_int, 0, out = alloc_8);  aten_select_copy_int = alloc_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_0 = self.lowered_module_0\n        executorch_call_delegate = torch.ops.higher_order.executorch_call_delegate(lowered_module_0, dim_order_ops__to_dim_order_copy_default_2, _lifted_tensor_constant3, _lifted_tensor_constant4, _lifted_tensor_constant5);  lowered_module_0 = dim_order_ops__to_dim_order_copy_default_2 = _lifted_tensor_constant3 = _lifted_tensor_constant4 = _lifted_tensor_constant5 = None\n        getitem: \"f32[16][1]\" = executorch_call_delegate[0];  executorch_call_delegate = None\n        alloc_9: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_unsqueeze_copy_default_1: \"f32[16, 1][1, 1]\" = torch.ops.aten.unsqueeze_copy.out(getitem, -1, out = alloc_9);  getitem = alloc_9 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_1 = self.lowered_module_1\n        executorch_call_delegate_1 = torch.ops.higher_order.executorch_call_delegate(lowered_module_1, p_pos_embed, dim_order_ops__to_dim_order_copy_default_3, _lifted_tensor_constant0, aten_unsqueeze_copy_default_1, _lifted_tensor_constant1, _lifted_tensor_constant2, _lifted_tensor_constant22, _lifted_tensor_constant23, _lifted_tensor_constant24, _lifted_tensor_constant12, _lifted_tensor_constant13, _lifted_tensor_constant14);  lowered_module_1 = p_pos_embed = dim_order_ops__to_dim_order_copy_default_3 = _lifted_tensor_constant0 = aten_unsqueeze_copy_default_1 = _lifted_tensor_constant1 = _lifted_tensor_constant2 = _lifted_tensor_constant22 = _lifted_tensor_constant23 = _lifted_tensor_constant24 = _lifted_tensor_constant12 = _lifted_tensor_constant13 = _lifted_tensor_constant14 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        getitem_1: \"f32[1, 1369, 384][525696, 384, 1]\" = executorch_call_delegate_1[0]\n        getitem_2: \"f32[16, 1][1, 1]\" = executorch_call_delegate_1[1]\n        getitem_3: \"f32[16][1]\" = executorch_call_delegate_1[2]\n        getitem_4: \"f32[32, 1][1, 1]\" = executorch_call_delegate_1[3]\n        getitem_5: \"f32[32, 1][1, 1]\" = executorch_call_delegate_1[4]\n        getitem_6: \"f32[32][1]\" = executorch_call_delegate_1[5]\n        getitem_7: \"f32[32][1]\" = executorch_call_delegate_1[6];  executorch_call_delegate_1 = None\n        aten_view_copy_default: \"f32[1, 37, 37, 384][525696, 14208, 384, 1]\" = executorch_exir_memory_view(getitem_1, [1, 37, 37, 384]);  getitem_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_10: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_4: \"i64[16, 1][1, 1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(getitem_2, dim_order = [0, 1], out = alloc_10);  getitem_2 = alloc_10 = None\n        \n        # No stacktrace found for following nodes\n        alloc_11: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        dim_order_ops__to_dim_order_copy_default_5: \"i64[16][1]\" = torch.ops.dim_order_ops._to_dim_order_copy.out(getitem_3, dim_order = [0], out = alloc_11);  getitem_3 = alloc_11 = None\n        aten_view_copy_default_1: \"f32[2, 16, 1][16, 1, 1]\" = executorch_exir_memory_view(getitem_4, [2, 16, 1]);  getitem_4 = None\n        aten_view_copy_default_2: \"f32[2, 16, 1][16, 1, 1]\" = executorch_exir_memory_view(getitem_5, [2, 16, 1]);  getitem_5 = None\n        aten_view_copy_default_3: \"f32[2, 16][16, 1]\" = executorch_exir_memory_view(getitem_6, [2, 16]);  getitem_6 = None\n        aten_view_copy_default_4: \"f32[2, 16][16, 1]\" = executorch_exir_memory_view(getitem_7, [2, 16]);  getitem_7 = None\n        \n        # No stacktrace found for following nodes\n        alloc_12: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_sub_tensor: \"i64[16, 1][1, 1]\" = torch.ops.aten.sub.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant6, out = alloc_12);  _lifted_tensor_constant6 = alloc_12 = None\n        \n        # No stacktrace found for following nodes\n        alloc_13: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor: \"i64[16, 1][1, 1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant7, out = alloc_13);  _lifted_tensor_constant7 = alloc_13 = None\n        \n        # No stacktrace found for following nodes\n        alloc_14: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_1: \"i64[16, 1][1, 1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_4, _lifted_tensor_constant8, out = alloc_14);  _lifted_tensor_constant8 = alloc_14 = None\n        \n        # No stacktrace found for following nodes\n        alloc_15: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_15);  alloc_15 = None\n        \n        # No stacktrace found for following nodes\n        alloc_16: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_1: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_16);  alloc_16 = None\n        \n        # No stacktrace found for following nodes\n        alloc_17: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_2: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_17);  alloc_17 = None\n        \n        # No stacktrace found for following nodes\n        alloc_18: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_3: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_4, 0, 36, out = alloc_18);  dim_order_ops__to_dim_order_copy_default_4 = alloc_18 = None\n        \n        # No stacktrace found for following nodes\n        alloc_19: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_sub_tensor_1: \"i64[16][1]\" = torch.ops.aten.sub.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant9, out = alloc_19);  _lifted_tensor_constant9 = alloc_19 = None\n        \n        # No stacktrace found for following nodes\n        alloc_20: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_2: \"i64[16][1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant10, out = alloc_20);  _lifted_tensor_constant10 = alloc_20 = None\n        \n        # No stacktrace found for following nodes\n        alloc_21: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_add_tensor_3: \"i64[16][1]\" = torch.ops.aten.add.out(dim_order_ops__to_dim_order_copy_default_5, _lifted_tensor_constant11, out = alloc_21);  _lifted_tensor_constant11 = alloc_21 = None\n        \n        # No stacktrace found for following nodes\n        alloc_22: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_4: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_22);  alloc_22 = None\n        \n        # No stacktrace found for following nodes\n        alloc_23: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_5: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_23);  alloc_23 = None\n        \n        # No stacktrace found for following nodes\n        alloc_24: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_6: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_24);  alloc_24 = None\n        \n        # No stacktrace found for following nodes\n        alloc_25: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_7: \"i64[16][1]\" = torch.ops.aten.clamp.out(dim_order_ops__to_dim_order_copy_default_5, 0, 36, out = alloc_25);  dim_order_ops__to_dim_order_copy_default_5 = alloc_25 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_2 = self.lowered_module_2\n        executorch_call_delegate_2 = torch.ops.higher_order.executorch_call_delegate(lowered_module_2, aten_view_copy_default, aten_view_copy_default_3, _lifted_tensor_constant19, aten_view_copy_default_4, _lifted_tensor_constant15, aten_view_copy_default_1, _lifted_tensor_constant29, aten_view_copy_default_2, _lifted_tensor_constant25, _lifted_tensor_constant20, _lifted_tensor_constant16, _lifted_tensor_constant30, _lifted_tensor_constant26, _lifted_tensor_constant17, _lifted_tensor_constant27, dim_order_ops__to_dim_order_copy_default, dim_order_ops__to_dim_order_copy_default_1, _lifted_tensor_constant18, _lifted_tensor_constant28);  lowered_module_2 = aten_view_copy_default = aten_view_copy_default_3 = _lifted_tensor_constant19 = aten_view_copy_default_4 = _lifted_tensor_constant15 = aten_view_copy_default_1 = _lifted_tensor_constant29 = aten_view_copy_default_2 = _lifted_tensor_constant25 = _lifted_tensor_constant20 = _lifted_tensor_constant16 = _lifted_tensor_constant30 = _lifted_tensor_constant26 = _lifted_tensor_constant17 = _lifted_tensor_constant27 = dim_order_ops__to_dim_order_copy_default = dim_order_ops__to_dim_order_copy_default_1 = _lifted_tensor_constant18 = _lifted_tensor_constant28 = None\n        alloc_26: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_8: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_26);  alloc_26 = None\n        \n        # No stacktrace found for following nodes\n        alloc_27: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_9: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_27);  alloc_27 = None\n        \n        # No stacktrace found for following nodes\n        alloc_28: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_10: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_28);  alloc_28 = None\n        \n        # No stacktrace found for following nodes\n        alloc_29: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_11: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_sub_tensor, 0, 36, out = alloc_29);  aten_sub_tensor = alloc_29 = None\n        \n        # No stacktrace found for following nodes\n        alloc_30: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_12: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_30);  alloc_30 = None\n        \n        # No stacktrace found for following nodes\n        alloc_31: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_13: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_31);  alloc_31 = None\n        \n        # No stacktrace found for following nodes\n        alloc_32: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_14: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_32);  alloc_32 = None\n        \n        # No stacktrace found for following nodes\n        alloc_33: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_15: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor, 0, 36, out = alloc_33);  aten_add_tensor = alloc_33 = None\n        \n        # No stacktrace found for following nodes\n        alloc_34: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_16: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_34);  alloc_34 = None\n        \n        # No stacktrace found for following nodes\n        alloc_35: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_17: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_35);  alloc_35 = None\n        \n        # No stacktrace found for following nodes\n        alloc_36: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_18: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_36);  alloc_36 = None\n        \n        # No stacktrace found for following nodes\n        alloc_37: \"i64[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_19: \"i64[16, 1][1, 1]\" = torch.ops.aten.clamp.out(aten_add_tensor_1, 0, 36, out = alloc_37);  aten_add_tensor_1 = alloc_37 = None\n        \n        # No stacktrace found for following nodes\n        alloc_38: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_20: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_38);  alloc_38 = None\n        \n        # No stacktrace found for following nodes\n        alloc_39: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_21: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_39);  alloc_39 = None\n        \n        # No stacktrace found for following nodes\n        alloc_40: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_22: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_40);  alloc_40 = None\n        \n        # No stacktrace found for following nodes\n        alloc_41: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_23: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_sub_tensor_1, 0, 36, out = alloc_41);  aten_sub_tensor_1 = alloc_41 = None\n        \n        # No stacktrace found for following nodes\n        alloc_42: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_24: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_42);  alloc_42 = None\n        \n        # No stacktrace found for following nodes\n        alloc_43: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_25: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_43);  alloc_43 = None\n        \n        # No stacktrace found for following nodes\n        alloc_44: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_26: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_44);  alloc_44 = None\n        \n        # No stacktrace found for following nodes\n        alloc_45: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_27: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_2, 0, 36, out = alloc_45);  aten_add_tensor_2 = alloc_45 = None\n        \n        # No stacktrace found for following nodes\n        alloc_46: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_28: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_46);  alloc_46 = None\n        \n        # No stacktrace found for following nodes\n        alloc_47: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_29: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_47);  alloc_47 = None\n        \n        # No stacktrace found for following nodes\n        alloc_48: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_30: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_48);  alloc_48 = None\n        \n        # No stacktrace found for following nodes\n        alloc_49: \"i64[16][1]\" = executorch_exir_memory_alloc(((16,), torch.int64))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clamp_default_31: \"i64[16][1]\" = torch.ops.aten.clamp.out(aten_add_tensor_3, 0, 36, out = alloc_49);  aten_add_tensor_3 = alloc_49 = None\n        getitem_8: \"f32[1, 384, 37, 37][525696, 1369, 37, 1]\" = executorch_call_delegate_2[0]\n        getitem_9: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[1]\n        getitem_10: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[2]\n        getitem_11: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[3]\n        getitem_12: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[4]\n        getitem_13: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[5]\n        getitem_14: \"f32[1, 16][16, 1]\" = executorch_call_delegate_2[6]\n        getitem_15: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[7]\n        getitem_16: \"f32[1, 16, 1][16, 1, 1]\" = executorch_call_delegate_2[8];  executorch_call_delegate_2 = None\n        \n        # No stacktrace found for following nodes\n        alloc_50: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_1, aten_clamp_default_5], out = alloc_50);  aten_clamp_default_1 = aten_clamp_default_5 = alloc_50 = None\n        \n        # No stacktrace found for following nodes\n        alloc_51: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_1: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default, aten_clamp_default_21], out = alloc_51);  aten_clamp_default = aten_clamp_default_21 = alloc_51 = None\n        \n        # No stacktrace found for following nodes\n        alloc_52: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_2: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_2, aten_clamp_default_25], out = alloc_52);  aten_clamp_default_2 = aten_clamp_default_25 = alloc_52 = None\n        \n        # No stacktrace found for following nodes\n        alloc_53: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_3: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_3, aten_clamp_default_29], out = alloc_53);  aten_clamp_default_3 = aten_clamp_default_29 = alloc_53 = None\n        \n        # No stacktrace found for following nodes\n        alloc_54: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_4: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_8, aten_clamp_default_20], out = alloc_54);  aten_clamp_default_8 = aten_clamp_default_20 = alloc_54 = None\n        \n        # No stacktrace found for following nodes\n        alloc_55: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_5: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_9, aten_clamp_default_4], out = alloc_55);  aten_clamp_default_9 = aten_clamp_default_4 = alloc_55 = None\n        \n        # No stacktrace found for following nodes\n        alloc_56: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_6: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_10, aten_clamp_default_24], out = alloc_56);  aten_clamp_default_10 = aten_clamp_default_24 = alloc_56 = None\n        \n        # No stacktrace found for following nodes\n        alloc_57: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_7: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_11, aten_clamp_default_28], out = alloc_57);  aten_clamp_default_11 = aten_clamp_default_28 = alloc_57 = None\n        \n        # No stacktrace found for following nodes\n        alloc_58: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_8: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_12, aten_clamp_default_22], out = alloc_58);  aten_clamp_default_12 = aten_clamp_default_22 = alloc_58 = None\n        \n        # No stacktrace found for following nodes\n        alloc_59: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_9: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_13, aten_clamp_default_6], out = alloc_59);  aten_clamp_default_13 = aten_clamp_default_6 = alloc_59 = None\n        \n        # No stacktrace found for following nodes\n        alloc_60: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_10: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_14, aten_clamp_default_26], out = alloc_60);  aten_clamp_default_14 = aten_clamp_default_26 = alloc_60 = None\n        \n        # No stacktrace found for following nodes\n        alloc_61: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_11: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_15, aten_clamp_default_30], out = alloc_61);  aten_clamp_default_15 = aten_clamp_default_30 = alloc_61 = None\n        \n        # No stacktrace found for following nodes\n        alloc_62: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_12: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_16, aten_clamp_default_23], out = alloc_62);  aten_clamp_default_16 = aten_clamp_default_23 = alloc_62 = None\n        \n        # No stacktrace found for following nodes\n        alloc_63: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_13: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_17, aten_clamp_default_7], out = alloc_63);  aten_clamp_default_17 = aten_clamp_default_7 = alloc_63 = None\n        \n        # No stacktrace found for following nodes\n        alloc_64: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_14: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_18, aten_clamp_default_27], out = alloc_64);  aten_clamp_default_18 = aten_clamp_default_27 = alloc_64 = None\n        \n        # No stacktrace found for following nodes\n        alloc_65: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_index_tensor_15: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = torch.ops.aten.index.Tensor_out(getitem_8, [None, None, aten_clamp_default_19, aten_clamp_default_31], out = alloc_65);  getitem_8 = aten_clamp_default_19 = aten_clamp_default_31 = alloc_65 = None\n        \n        # No stacktrace found for following nodes\n        alloc_66: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_9, [0], out = alloc_66);  getitem_9 = alloc_66 = None\n        \n        # No stacktrace found for following nodes\n        alloc_67: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_1: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_10, [0], out = alloc_67);  getitem_10 = alloc_67 = None\n        \n        # No stacktrace found for following nodes\n        alloc_68: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_2: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_11, [0], out = alloc_68);  getitem_11 = alloc_68 = None\n        \n        # No stacktrace found for following nodes\n        alloc_69: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_3: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_12, [0], out = alloc_69);  getitem_12 = alloc_69 = None\n        \n        # No stacktrace found for following nodes\n        alloc_70: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_4: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_13, [0], out = alloc_70);  getitem_13 = alloc_70 = None\n        \n        # No stacktrace found for following nodes\n        alloc_71: \"f32[16][1]\" = executorch_exir_memory_alloc(((16,), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_5: \"f32[16][1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_14, [0], out = alloc_71);  getitem_14 = alloc_71 = None\n        \n        # No stacktrace found for following nodes\n        alloc_72: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_6: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_15, [0], out = alloc_72);  getitem_15 = alloc_72 = None\n        \n        # No stacktrace found for following nodes\n        alloc_73: \"f32[16, 1][1, 1]\" = executorch_exir_memory_alloc(((16, 1), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_squeeze_copy_dims_7: \"f32[16, 1][1, 1]\" = torch.ops.aten.squeeze_copy.dims_out(getitem_16, [0], out = alloc_73);  getitem_16 = alloc_73 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_3 = self.lowered_module_3\n        executorch_call_delegate_3 = torch.ops.higher_order.executorch_call_delegate(lowered_module_3, aten_index_tensor_5, aten_squeeze_copy_dims, aten_index_tensor, aten_index_tensor_9, aten_index_tensor_13, aten_index_tensor_6, aten_squeeze_copy_dims_1, aten_index_tensor_2, aten_index_tensor_10, aten_index_tensor_14, aten_index_tensor_4, aten_squeeze_copy_dims_4, aten_index_tensor_1, aten_index_tensor_8, aten_index_tensor_12, aten_index_tensor_7, aten_squeeze_copy_dims_5, aten_index_tensor_3, aten_index_tensor_11, aten_index_tensor_15, aten_squeeze_copy_dims_6, aten_squeeze_copy_dims_2, aten_squeeze_copy_dims_3, aten_squeeze_copy_dims_7);  lowered_module_3 = aten_index_tensor_5 = aten_squeeze_copy_dims = aten_index_tensor = aten_index_tensor_9 = aten_index_tensor_13 = aten_index_tensor_6 = aten_squeeze_copy_dims_1 = aten_index_tensor_2 = aten_index_tensor_10 = aten_index_tensor_14 = aten_index_tensor_4 = aten_squeeze_copy_dims_4 = aten_index_tensor_1 = aten_index_tensor_8 = aten_index_tensor_12 = aten_index_tensor_7 = aten_squeeze_copy_dims_5 = aten_index_tensor_3 = aten_index_tensor_11 = aten_index_tensor_15 = aten_squeeze_copy_dims_6 = aten_squeeze_copy_dims_2 = aten_squeeze_copy_dims_3 = aten_squeeze_copy_dims_7 = None\n        getitem_17: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_call_delegate_3[0];  executorch_call_delegate_3 = None\n        alloc_74: \"f32[1, 384, 16, 16][98304, 1, 6144, 384]\" = executorch_exir_memory_alloc(((1, 384, 16, 16), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_clone_default: \"f32[1, 384, 16, 16][98304, 1, 6144, 384]\" = torch.ops.aten.clone.out(getitem_17, memory_format = torch.channels_last, out = alloc_74);  getitem_17 = alloc_74 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_4 = self.lowered_module_4\n        executorch_call_delegate_4 = torch.ops.higher_order.executorch_call_delegate(lowered_module_4, aten_clone_default, args_0);  lowered_module_4 = aten_clone_default = args_0 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        getitem_18: \"f32[1, 16, 16, 384][98304, 6144, 384, 1]\" = executorch_call_delegate_4[0]\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        getitem_19: \"f32[1, 384, 16, 16][98304, 256, 16, 1]\" = executorch_call_delegate_4[1];  executorch_call_delegate_4 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_view_copy_default_5: \"f32[1, 256, 384][98304, 384, 1]\" = executorch_exir_memory_view(getitem_18, [1, -1, 384]);  getitem_18 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:77 in forward, code: x = x.flatten(2).transpose(1, 2)  # B HW C\n        aten_view_copy_default_6: \"f32[1, 384, 256][98304, 256, 1]\" = executorch_exir_memory_view(getitem_19, [1, 384, 256]);  getitem_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_5 = self.lowered_module_5\n        executorch_call_delegate_5 = torch.ops.higher_order.executorch_call_delegate(lowered_module_5, aten_view_copy_default_6, aten_unsqueeze_copy_default, aten_view_copy_default_5, aten_expand_copy_default);  lowered_module_5 = aten_view_copy_default_6 = aten_unsqueeze_copy_default = aten_view_copy_default_5 = aten_expand_copy_default = None\n        getitem_20: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_5[0];  executorch_call_delegate_5 = None\n        alloc_75: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_76: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_77: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default = torch.ops.aten.native_layer_norm.out(getitem_20, [384], p_blocks_0_norm1_weight, p_blocks_0_norm1_bias, 1e-06, out0 = alloc_75, out1 = alloc_76, out2 = alloc_77);  p_blocks_0_norm1_weight = p_blocks_0_norm1_bias = alloc_75 = alloc_76 = alloc_77 = None\n        getitem_21: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default[0];  aten_native_layer_norm_default = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_6 = self.lowered_module_6\n        executorch_call_delegate_6 = torch.ops.higher_order.executorch_call_delegate(lowered_module_6, getitem_21);  lowered_module_6 = getitem_21 = None\n        getitem_22: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_6[0];  executorch_call_delegate_6 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_7: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_22, [1, 257, 3, 6, 64]);  getitem_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_7 = self.lowered_module_7\n        executorch_call_delegate_7 = torch.ops.higher_order.executorch_call_delegate(lowered_module_7, aten_view_copy_default_7);  lowered_module_7 = aten_view_copy_default_7 = None\n        getitem_23: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_7[0];  executorch_call_delegate_7 = None\n        alloc_78: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_1: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 0, out = alloc_78);  alloc_78 = None\n        \n        # No stacktrace found for following nodes\n        alloc_79: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_2: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 1, out = alloc_79);  alloc_79 = None\n        \n        # No stacktrace found for following nodes\n        alloc_80: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_3: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_23, 0, 2, out = alloc_80);  getitem_23 = alloc_80 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_8 = self.lowered_module_8\n        executorch_call_delegate_8 = torch.ops.higher_order.executorch_call_delegate(lowered_module_8, aten_select_copy_int_1, _lifted_tensor_constant32, aten_select_copy_int_2);  lowered_module_8 = aten_select_copy_int_1 = _lifted_tensor_constant32 = aten_select_copy_int_2 = None\n        alloc_81: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_1: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_3, [1, 6, 257, 64], out = alloc_81);  aten_select_copy_int_3 = alloc_81 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_24: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_8[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_25: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_8[1];  executorch_call_delegate_8 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_8: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_1, [6, 257, 64]);  aten_expand_copy_default_1 = None\n        \n        # No stacktrace found for following nodes\n        alloc_82: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_2: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_24, [1, 6, 257, 64], out = alloc_82);  getitem_24 = alloc_82 = None\n        \n        # No stacktrace found for following nodes\n        alloc_83: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_3: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_25, [1, 6, 64, 257], out = alloc_83);  getitem_25 = alloc_83 = None\n        aten_view_copy_default_9: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_2, [6, 257, 64]);  aten_expand_copy_default_2 = None\n        aten_view_copy_default_10: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_3, [6, 64, 257]);  aten_expand_copy_default_3 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_9 = self.lowered_module_9\n        executorch_call_delegate_9 = torch.ops.higher_order.executorch_call_delegate(lowered_module_9, aten_view_copy_default_9, aten_view_copy_default_10);  lowered_module_9 = aten_view_copy_default_9 = aten_view_copy_default_10 = None\n        getitem_26: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_9[0];  executorch_call_delegate_9 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_11: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_26, [1, 6, 257, 257]);  getitem_26 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_10 = self.lowered_module_10\n        executorch_call_delegate_10 = torch.ops.higher_order.executorch_call_delegate(lowered_module_10, aten_view_copy_default_11);  lowered_module_10 = aten_view_copy_default_11 = None\n        getitem_27: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_10[0];  executorch_call_delegate_10 = None\n        alloc_84: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_1: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_27, out = alloc_84);  getitem_27 = alloc_84 = None\n        \n        # No stacktrace found for following nodes\n        alloc_85: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_4: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_1, [1, 6, 257, 257], out = alloc_85);  aten_clone_default_1 = alloc_85 = None\n        aten_view_copy_default_12: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_4, [6, 257, 257]);  aten_expand_copy_default_4 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_11 = self.lowered_module_11\n        executorch_call_delegate_11 = torch.ops.higher_order.executorch_call_delegate(lowered_module_11, aten_view_copy_default_12, aten_view_copy_default_8);  lowered_module_11 = aten_view_copy_default_12 = aten_view_copy_default_8 = None\n        getitem_28: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_11[0];  executorch_call_delegate_11 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_28, [1, 6, 257, 64]);  getitem_28 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_12 = self.lowered_module_12\n        executorch_call_delegate_12 = torch.ops.higher_order.executorch_call_delegate(lowered_module_12, aten_view_copy_default_13);  lowered_module_12 = aten_view_copy_default_13 = None\n        getitem_29: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_12[0];  executorch_call_delegate_12 = None\n        alloc_86: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_2: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_29, memory_format = torch.contiguous_format, out = alloc_86);  getitem_29 = alloc_86 = None\n        aten_view_copy_default_14: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_2, [1, 257, 384]);  aten_clone_default_2 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_13 = self.lowered_module_13\n        executorch_call_delegate_13 = torch.ops.higher_order.executorch_call_delegate(lowered_module_13, aten_view_copy_default_14);  lowered_module_13 = aten_view_copy_default_14 = None\n        getitem_30: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_13[0];  executorch_call_delegate_13 = None\n        alloc_87: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_3: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_30, out = alloc_87);  getitem_30 = alloc_87 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_14 = self.lowered_module_14\n        executorch_call_delegate_14 = torch.ops.higher_order.executorch_call_delegate(lowered_module_14, aten_clone_default_3, p_blocks_0_ls1_gamma, getitem_20);  lowered_module_14 = aten_clone_default_3 = p_blocks_0_ls1_gamma = getitem_20 = None\n        getitem_31: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_14[0];  executorch_call_delegate_14 = None\n        alloc_88: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_89: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_90: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_1 = torch.ops.aten.native_layer_norm.out(getitem_31, [384], p_blocks_0_norm2_weight, p_blocks_0_norm2_bias, 1e-06, out0 = alloc_88, out1 = alloc_89, out2 = alloc_90);  p_blocks_0_norm2_weight = p_blocks_0_norm2_bias = alloc_88 = alloc_89 = alloc_90 = None\n        getitem_32: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_1[0];  aten_native_layer_norm_default_1 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_15 = self.lowered_module_15\n        executorch_call_delegate_15 = torch.ops.higher_order.executorch_call_delegate(lowered_module_15, getitem_32);  lowered_module_15 = getitem_32 = None\n        getitem_33: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_15[0];  executorch_call_delegate_15 = None\n        alloc_91: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_4: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_33, out = alloc_91);  getitem_33 = alloc_91 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_16 = self.lowered_module_16\n        executorch_call_delegate_16 = torch.ops.higher_order.executorch_call_delegate(lowered_module_16, aten_clone_default_4);  lowered_module_16 = aten_clone_default_4 = None\n        getitem_34: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_16[0];  executorch_call_delegate_16 = None\n        alloc_92: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_5: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_34, out = alloc_92);  getitem_34 = alloc_92 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_17 = self.lowered_module_17\n        executorch_call_delegate_17 = torch.ops.higher_order.executorch_call_delegate(lowered_module_17, aten_clone_default_5, p_blocks_0_ls2_gamma, getitem_31);  lowered_module_17 = aten_clone_default_5 = p_blocks_0_ls2_gamma = getitem_31 = None\n        getitem_35: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_17[0];  executorch_call_delegate_17 = None\n        alloc_93: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_94: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_95: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_2 = torch.ops.aten.native_layer_norm.out(getitem_35, [384], p_blocks_1_norm1_weight, p_blocks_1_norm1_bias, 1e-06, out0 = alloc_93, out1 = alloc_94, out2 = alloc_95);  p_blocks_1_norm1_weight = p_blocks_1_norm1_bias = alloc_93 = alloc_94 = alloc_95 = None\n        getitem_36: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_2[0];  aten_native_layer_norm_default_2 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_18 = self.lowered_module_18\n        executorch_call_delegate_18 = torch.ops.higher_order.executorch_call_delegate(lowered_module_18, getitem_36);  lowered_module_18 = getitem_36 = None\n        getitem_37: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_18[0];  executorch_call_delegate_18 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_15: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_37, [1, 257, 3, 6, 64]);  getitem_37 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_19 = self.lowered_module_19\n        executorch_call_delegate_19 = torch.ops.higher_order.executorch_call_delegate(lowered_module_19, aten_view_copy_default_15);  lowered_module_19 = aten_view_copy_default_15 = None\n        getitem_38: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_19[0];  executorch_call_delegate_19 = None\n        alloc_96: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_4: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 0, out = alloc_96);  alloc_96 = None\n        \n        # No stacktrace found for following nodes\n        alloc_97: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_5: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 1, out = alloc_97);  alloc_97 = None\n        \n        # No stacktrace found for following nodes\n        alloc_98: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_6: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_38, 0, 2, out = alloc_98);  getitem_38 = alloc_98 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_20 = self.lowered_module_20\n        executorch_call_delegate_20 = torch.ops.higher_order.executorch_call_delegate(lowered_module_20, aten_select_copy_int_4, _lifted_tensor_constant33, aten_select_copy_int_5);  lowered_module_20 = aten_select_copy_int_4 = _lifted_tensor_constant33 = aten_select_copy_int_5 = None\n        alloc_99: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_5: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_6, [1, 6, 257, 64], out = alloc_99);  aten_select_copy_int_6 = alloc_99 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_39: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_20[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_40: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_20[1];  executorch_call_delegate_20 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_16: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_5, [6, 257, 64]);  aten_expand_copy_default_5 = None\n        \n        # No stacktrace found for following nodes\n        alloc_100: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_6: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_39, [1, 6, 257, 64], out = alloc_100);  getitem_39 = alloc_100 = None\n        \n        # No stacktrace found for following nodes\n        alloc_101: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_7: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_40, [1, 6, 64, 257], out = alloc_101);  getitem_40 = alloc_101 = None\n        aten_view_copy_default_17: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_6, [6, 257, 64]);  aten_expand_copy_default_6 = None\n        aten_view_copy_default_18: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_7, [6, 64, 257]);  aten_expand_copy_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_21 = self.lowered_module_21\n        executorch_call_delegate_21 = torch.ops.higher_order.executorch_call_delegate(lowered_module_21, aten_view_copy_default_17, aten_view_copy_default_18);  lowered_module_21 = aten_view_copy_default_17 = aten_view_copy_default_18 = None\n        getitem_41: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_21[0];  executorch_call_delegate_21 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_19: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_41, [1, 6, 257, 257]);  getitem_41 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_22 = self.lowered_module_22\n        executorch_call_delegate_22 = torch.ops.higher_order.executorch_call_delegate(lowered_module_22, aten_view_copy_default_19);  lowered_module_22 = aten_view_copy_default_19 = None\n        getitem_42: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_22[0];  executorch_call_delegate_22 = None\n        alloc_102: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_6: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_42, out = alloc_102);  getitem_42 = alloc_102 = None\n        \n        # No stacktrace found for following nodes\n        alloc_103: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_8: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_6, [1, 6, 257, 257], out = alloc_103);  aten_clone_default_6 = alloc_103 = None\n        aten_view_copy_default_20: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_8, [6, 257, 257]);  aten_expand_copy_default_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_23 = self.lowered_module_23\n        executorch_call_delegate_23 = torch.ops.higher_order.executorch_call_delegate(lowered_module_23, aten_view_copy_default_20, aten_view_copy_default_16);  lowered_module_23 = aten_view_copy_default_20 = aten_view_copy_default_16 = None\n        getitem_43: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_23[0];  executorch_call_delegate_23 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_43, [1, 6, 257, 64]);  getitem_43 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_24 = self.lowered_module_24\n        executorch_call_delegate_24 = torch.ops.higher_order.executorch_call_delegate(lowered_module_24, aten_view_copy_default_21);  lowered_module_24 = aten_view_copy_default_21 = None\n        getitem_44: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_24[0];  executorch_call_delegate_24 = None\n        alloc_104: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_7: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_44, memory_format = torch.contiguous_format, out = alloc_104);  getitem_44 = alloc_104 = None\n        aten_view_copy_default_22: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_7, [1, 257, 384]);  aten_clone_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_25 = self.lowered_module_25\n        executorch_call_delegate_25 = torch.ops.higher_order.executorch_call_delegate(lowered_module_25, aten_view_copy_default_22);  lowered_module_25 = aten_view_copy_default_22 = None\n        getitem_45: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_25[0];  executorch_call_delegate_25 = None\n        alloc_105: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_8: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_45, out = alloc_105);  getitem_45 = alloc_105 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_26 = self.lowered_module_26\n        executorch_call_delegate_26 = torch.ops.higher_order.executorch_call_delegate(lowered_module_26, aten_clone_default_8, p_blocks_1_ls1_gamma, getitem_35);  lowered_module_26 = aten_clone_default_8 = p_blocks_1_ls1_gamma = getitem_35 = None\n        getitem_46: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_26[0];  executorch_call_delegate_26 = None\n        alloc_106: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_107: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_108: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_3 = torch.ops.aten.native_layer_norm.out(getitem_46, [384], p_blocks_1_norm2_weight, p_blocks_1_norm2_bias, 1e-06, out0 = alloc_106, out1 = alloc_107, out2 = alloc_108);  p_blocks_1_norm2_weight = p_blocks_1_norm2_bias = alloc_106 = alloc_107 = alloc_108 = None\n        getitem_47: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_3[0];  aten_native_layer_norm_default_3 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_27 = self.lowered_module_27\n        executorch_call_delegate_27 = torch.ops.higher_order.executorch_call_delegate(lowered_module_27, getitem_47);  lowered_module_27 = getitem_47 = None\n        getitem_48: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_27[0];  executorch_call_delegate_27 = None\n        alloc_109: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_9: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_48, out = alloc_109);  getitem_48 = alloc_109 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_28 = self.lowered_module_28\n        executorch_call_delegate_28 = torch.ops.higher_order.executorch_call_delegate(lowered_module_28, aten_clone_default_9);  lowered_module_28 = aten_clone_default_9 = None\n        getitem_49: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_28[0];  executorch_call_delegate_28 = None\n        alloc_110: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_10: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_49, out = alloc_110);  getitem_49 = alloc_110 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_29 = self.lowered_module_29\n        executorch_call_delegate_29 = torch.ops.higher_order.executorch_call_delegate(lowered_module_29, aten_clone_default_10, p_blocks_1_ls2_gamma, getitem_46);  lowered_module_29 = aten_clone_default_10 = p_blocks_1_ls2_gamma = getitem_46 = None\n        getitem_50: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_29[0];  executorch_call_delegate_29 = None\n        alloc_111: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_112: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_113: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_4 = torch.ops.aten.native_layer_norm.out(getitem_50, [384], p_blocks_2_norm1_weight, p_blocks_2_norm1_bias, 1e-06, out0 = alloc_111, out1 = alloc_112, out2 = alloc_113);  p_blocks_2_norm1_weight = p_blocks_2_norm1_bias = alloc_111 = alloc_112 = alloc_113 = None\n        getitem_51: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_4[0];  aten_native_layer_norm_default_4 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_30 = self.lowered_module_30\n        executorch_call_delegate_30 = torch.ops.higher_order.executorch_call_delegate(lowered_module_30, getitem_51);  lowered_module_30 = getitem_51 = None\n        getitem_52: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_30[0];  executorch_call_delegate_30 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_23: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_52, [1, 257, 3, 6, 64]);  getitem_52 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_31 = self.lowered_module_31\n        executorch_call_delegate_31 = torch.ops.higher_order.executorch_call_delegate(lowered_module_31, aten_view_copy_default_23);  lowered_module_31 = aten_view_copy_default_23 = None\n        getitem_53: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_31[0];  executorch_call_delegate_31 = None\n        alloc_114: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_7: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 0, out = alloc_114);  alloc_114 = None\n        \n        # No stacktrace found for following nodes\n        alloc_115: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_8: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 1, out = alloc_115);  alloc_115 = None\n        \n        # No stacktrace found for following nodes\n        alloc_116: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_9: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_53, 0, 2, out = alloc_116);  getitem_53 = alloc_116 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_32 = self.lowered_module_32\n        executorch_call_delegate_32 = torch.ops.higher_order.executorch_call_delegate(lowered_module_32, aten_select_copy_int_7, _lifted_tensor_constant34, aten_select_copy_int_8);  lowered_module_32 = aten_select_copy_int_7 = _lifted_tensor_constant34 = aten_select_copy_int_8 = None\n        alloc_117: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_9: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_9, [1, 6, 257, 64], out = alloc_117);  aten_select_copy_int_9 = alloc_117 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_54: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_32[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_55: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_32[1];  executorch_call_delegate_32 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_24: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_9, [6, 257, 64]);  aten_expand_copy_default_9 = None\n        \n        # No stacktrace found for following nodes\n        alloc_118: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_10: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_54, [1, 6, 257, 64], out = alloc_118);  getitem_54 = alloc_118 = None\n        \n        # No stacktrace found for following nodes\n        alloc_119: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_11: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_55, [1, 6, 64, 257], out = alloc_119);  getitem_55 = alloc_119 = None\n        aten_view_copy_default_25: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_10, [6, 257, 64]);  aten_expand_copy_default_10 = None\n        aten_view_copy_default_26: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_11, [6, 64, 257]);  aten_expand_copy_default_11 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_33 = self.lowered_module_33\n        executorch_call_delegate_33 = torch.ops.higher_order.executorch_call_delegate(lowered_module_33, aten_view_copy_default_25, aten_view_copy_default_26);  lowered_module_33 = aten_view_copy_default_25 = aten_view_copy_default_26 = None\n        getitem_56: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_33[0];  executorch_call_delegate_33 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_27: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_56, [1, 6, 257, 257]);  getitem_56 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_34 = self.lowered_module_34\n        executorch_call_delegate_34 = torch.ops.higher_order.executorch_call_delegate(lowered_module_34, aten_view_copy_default_27);  lowered_module_34 = aten_view_copy_default_27 = None\n        getitem_57: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_34[0];  executorch_call_delegate_34 = None\n        alloc_120: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_11: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_57, out = alloc_120);  getitem_57 = alloc_120 = None\n        \n        # No stacktrace found for following nodes\n        alloc_121: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_12: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_11, [1, 6, 257, 257], out = alloc_121);  aten_clone_default_11 = alloc_121 = None\n        aten_view_copy_default_28: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_12, [6, 257, 257]);  aten_expand_copy_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_35 = self.lowered_module_35\n        executorch_call_delegate_35 = torch.ops.higher_order.executorch_call_delegate(lowered_module_35, aten_view_copy_default_28, aten_view_copy_default_24);  lowered_module_35 = aten_view_copy_default_28 = aten_view_copy_default_24 = None\n        getitem_58: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_35[0];  executorch_call_delegate_35 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_58, [1, 6, 257, 64]);  getitem_58 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_36 = self.lowered_module_36\n        executorch_call_delegate_36 = torch.ops.higher_order.executorch_call_delegate(lowered_module_36, aten_view_copy_default_29);  lowered_module_36 = aten_view_copy_default_29 = None\n        getitem_59: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_36[0];  executorch_call_delegate_36 = None\n        alloc_122: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_12: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_59, memory_format = torch.contiguous_format, out = alloc_122);  getitem_59 = alloc_122 = None\n        aten_view_copy_default_30: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_12, [1, 257, 384]);  aten_clone_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_37 = self.lowered_module_37\n        executorch_call_delegate_37 = torch.ops.higher_order.executorch_call_delegate(lowered_module_37, aten_view_copy_default_30);  lowered_module_37 = aten_view_copy_default_30 = None\n        getitem_60: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_37[0];  executorch_call_delegate_37 = None\n        alloc_123: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_13: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_60, out = alloc_123);  getitem_60 = alloc_123 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_38 = self.lowered_module_38\n        executorch_call_delegate_38 = torch.ops.higher_order.executorch_call_delegate(lowered_module_38, aten_clone_default_13, p_blocks_2_ls1_gamma, getitem_50);  lowered_module_38 = aten_clone_default_13 = p_blocks_2_ls1_gamma = getitem_50 = None\n        getitem_61: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_38[0];  executorch_call_delegate_38 = None\n        alloc_124: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_125: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_126: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_5 = torch.ops.aten.native_layer_norm.out(getitem_61, [384], p_blocks_2_norm2_weight, p_blocks_2_norm2_bias, 1e-06, out0 = alloc_124, out1 = alloc_125, out2 = alloc_126);  p_blocks_2_norm2_weight = p_blocks_2_norm2_bias = alloc_124 = alloc_125 = alloc_126 = None\n        getitem_62: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_5[0];  aten_native_layer_norm_default_5 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_39 = self.lowered_module_39\n        executorch_call_delegate_39 = torch.ops.higher_order.executorch_call_delegate(lowered_module_39, getitem_62);  lowered_module_39 = getitem_62 = None\n        getitem_63: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_39[0];  executorch_call_delegate_39 = None\n        alloc_127: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_14: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_63, out = alloc_127);  getitem_63 = alloc_127 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_40 = self.lowered_module_40\n        executorch_call_delegate_40 = torch.ops.higher_order.executorch_call_delegate(lowered_module_40, aten_clone_default_14);  lowered_module_40 = aten_clone_default_14 = None\n        getitem_64: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_40[0];  executorch_call_delegate_40 = None\n        alloc_128: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_15: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_64, out = alloc_128);  getitem_64 = alloc_128 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_41 = self.lowered_module_41\n        executorch_call_delegate_41 = torch.ops.higher_order.executorch_call_delegate(lowered_module_41, aten_clone_default_15, p_blocks_2_ls2_gamma, getitem_61);  lowered_module_41 = aten_clone_default_15 = p_blocks_2_ls2_gamma = getitem_61 = None\n        getitem_65: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_41[0];  executorch_call_delegate_41 = None\n        alloc_129: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_130: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_131: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_6 = torch.ops.aten.native_layer_norm.out(getitem_65, [384], p_blocks_3_norm1_weight, p_blocks_3_norm1_bias, 1e-06, out0 = alloc_129, out1 = alloc_130, out2 = alloc_131);  p_blocks_3_norm1_weight = p_blocks_3_norm1_bias = alloc_129 = alloc_130 = alloc_131 = None\n        getitem_66: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_6[0];  aten_native_layer_norm_default_6 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_42 = self.lowered_module_42\n        executorch_call_delegate_42 = torch.ops.higher_order.executorch_call_delegate(lowered_module_42, getitem_66);  lowered_module_42 = getitem_66 = None\n        getitem_67: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_42[0];  executorch_call_delegate_42 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_31: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_67, [1, 257, 3, 6, 64]);  getitem_67 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_43 = self.lowered_module_43\n        executorch_call_delegate_43 = torch.ops.higher_order.executorch_call_delegate(lowered_module_43, aten_view_copy_default_31);  lowered_module_43 = aten_view_copy_default_31 = None\n        getitem_68: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_43[0];  executorch_call_delegate_43 = None\n        alloc_132: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_10: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 0, out = alloc_132);  alloc_132 = None\n        \n        # No stacktrace found for following nodes\n        alloc_133: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_11: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 1, out = alloc_133);  alloc_133 = None\n        \n        # No stacktrace found for following nodes\n        alloc_134: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_12: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_68, 0, 2, out = alloc_134);  getitem_68 = alloc_134 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_44 = self.lowered_module_44\n        executorch_call_delegate_44 = torch.ops.higher_order.executorch_call_delegate(lowered_module_44, aten_select_copy_int_10, _lifted_tensor_constant35, aten_select_copy_int_11);  lowered_module_44 = aten_select_copy_int_10 = _lifted_tensor_constant35 = aten_select_copy_int_11 = None\n        alloc_135: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_12, [1, 6, 257, 64], out = alloc_135);  aten_select_copy_int_12 = alloc_135 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_69: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_44[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_70: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_44[1];  executorch_call_delegate_44 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_32: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_13, [6, 257, 64]);  aten_expand_copy_default_13 = None\n        \n        # No stacktrace found for following nodes\n        alloc_136: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_14: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_69, [1, 6, 257, 64], out = alloc_136);  getitem_69 = alloc_136 = None\n        \n        # No stacktrace found for following nodes\n        alloc_137: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_15: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_70, [1, 6, 64, 257], out = alloc_137);  getitem_70 = alloc_137 = None\n        aten_view_copy_default_33: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_14, [6, 257, 64]);  aten_expand_copy_default_14 = None\n        aten_view_copy_default_34: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_15, [6, 64, 257]);  aten_expand_copy_default_15 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_45 = self.lowered_module_45\n        executorch_call_delegate_45 = torch.ops.higher_order.executorch_call_delegate(lowered_module_45, aten_view_copy_default_33, aten_view_copy_default_34);  lowered_module_45 = aten_view_copy_default_33 = aten_view_copy_default_34 = None\n        getitem_71: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_45[0];  executorch_call_delegate_45 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_35: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_71, [1, 6, 257, 257]);  getitem_71 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_46 = self.lowered_module_46\n        executorch_call_delegate_46 = torch.ops.higher_order.executorch_call_delegate(lowered_module_46, aten_view_copy_default_35);  lowered_module_46 = aten_view_copy_default_35 = None\n        getitem_72: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_46[0];  executorch_call_delegate_46 = None\n        alloc_138: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_16: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_72, out = alloc_138);  getitem_72 = alloc_138 = None\n        \n        # No stacktrace found for following nodes\n        alloc_139: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_16: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_16, [1, 6, 257, 257], out = alloc_139);  aten_clone_default_16 = alloc_139 = None\n        aten_view_copy_default_36: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_16, [6, 257, 257]);  aten_expand_copy_default_16 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_47 = self.lowered_module_47\n        executorch_call_delegate_47 = torch.ops.higher_order.executorch_call_delegate(lowered_module_47, aten_view_copy_default_36, aten_view_copy_default_32);  lowered_module_47 = aten_view_copy_default_36 = aten_view_copy_default_32 = None\n        getitem_73: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_47[0];  executorch_call_delegate_47 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_37: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_73, [1, 6, 257, 64]);  getitem_73 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_48 = self.lowered_module_48\n        executorch_call_delegate_48 = torch.ops.higher_order.executorch_call_delegate(lowered_module_48, aten_view_copy_default_37);  lowered_module_48 = aten_view_copy_default_37 = None\n        getitem_74: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_48[0];  executorch_call_delegate_48 = None\n        alloc_140: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_17: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_74, memory_format = torch.contiguous_format, out = alloc_140);  getitem_74 = alloc_140 = None\n        aten_view_copy_default_38: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_17, [1, 257, 384]);  aten_clone_default_17 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_49 = self.lowered_module_49\n        executorch_call_delegate_49 = torch.ops.higher_order.executorch_call_delegate(lowered_module_49, aten_view_copy_default_38);  lowered_module_49 = aten_view_copy_default_38 = None\n        getitem_75: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_49[0];  executorch_call_delegate_49 = None\n        alloc_141: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_18: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_75, out = alloc_141);  getitem_75 = alloc_141 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_50 = self.lowered_module_50\n        executorch_call_delegate_50 = torch.ops.higher_order.executorch_call_delegate(lowered_module_50, aten_clone_default_18, p_blocks_3_ls1_gamma, getitem_65);  lowered_module_50 = aten_clone_default_18 = p_blocks_3_ls1_gamma = getitem_65 = None\n        getitem_76: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_50[0];  executorch_call_delegate_50 = None\n        alloc_142: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_143: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_144: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_7 = torch.ops.aten.native_layer_norm.out(getitem_76, [384], p_blocks_3_norm2_weight, p_blocks_3_norm2_bias, 1e-06, out0 = alloc_142, out1 = alloc_143, out2 = alloc_144);  p_blocks_3_norm2_weight = p_blocks_3_norm2_bias = alloc_142 = alloc_143 = alloc_144 = None\n        getitem_77: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_7[0];  aten_native_layer_norm_default_7 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_51 = self.lowered_module_51\n        executorch_call_delegate_51 = torch.ops.higher_order.executorch_call_delegate(lowered_module_51, getitem_77);  lowered_module_51 = getitem_77 = None\n        getitem_78: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_51[0];  executorch_call_delegate_51 = None\n        alloc_145: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_19: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_78, out = alloc_145);  getitem_78 = alloc_145 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_52 = self.lowered_module_52\n        executorch_call_delegate_52 = torch.ops.higher_order.executorch_call_delegate(lowered_module_52, aten_clone_default_19);  lowered_module_52 = aten_clone_default_19 = None\n        getitem_79: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_52[0];  executorch_call_delegate_52 = None\n        alloc_146: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_20: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_79, out = alloc_146);  getitem_79 = alloc_146 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_53 = self.lowered_module_53\n        executorch_call_delegate_53 = torch.ops.higher_order.executorch_call_delegate(lowered_module_53, aten_clone_default_20, p_blocks_3_ls2_gamma, getitem_76);  lowered_module_53 = aten_clone_default_20 = p_blocks_3_ls2_gamma = getitem_76 = None\n        getitem_80: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_53[0];  executorch_call_delegate_53 = None\n        alloc_147: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_148: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_149: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_8 = torch.ops.aten.native_layer_norm.out(getitem_80, [384], p_blocks_4_norm1_weight, p_blocks_4_norm1_bias, 1e-06, out0 = alloc_147, out1 = alloc_148, out2 = alloc_149);  p_blocks_4_norm1_weight = p_blocks_4_norm1_bias = alloc_147 = alloc_148 = alloc_149 = None\n        getitem_81: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_8[0];  aten_native_layer_norm_default_8 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_54 = self.lowered_module_54\n        executorch_call_delegate_54 = torch.ops.higher_order.executorch_call_delegate(lowered_module_54, getitem_81);  lowered_module_54 = getitem_81 = None\n        getitem_82: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_54[0];  executorch_call_delegate_54 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_39: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_82, [1, 257, 3, 6, 64]);  getitem_82 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_55 = self.lowered_module_55\n        executorch_call_delegate_55 = torch.ops.higher_order.executorch_call_delegate(lowered_module_55, aten_view_copy_default_39);  lowered_module_55 = aten_view_copy_default_39 = None\n        getitem_83: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_55[0];  executorch_call_delegate_55 = None\n        alloc_150: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_13: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 0, out = alloc_150);  alloc_150 = None\n        \n        # No stacktrace found for following nodes\n        alloc_151: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_14: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 1, out = alloc_151);  alloc_151 = None\n        \n        # No stacktrace found for following nodes\n        alloc_152: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_15: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_83, 0, 2, out = alloc_152);  getitem_83 = alloc_152 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_56 = self.lowered_module_56\n        executorch_call_delegate_56 = torch.ops.higher_order.executorch_call_delegate(lowered_module_56, aten_select_copy_int_13, _lifted_tensor_constant36, aten_select_copy_int_14);  lowered_module_56 = aten_select_copy_int_13 = _lifted_tensor_constant36 = aten_select_copy_int_14 = None\n        alloc_153: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_17: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_15, [1, 6, 257, 64], out = alloc_153);  aten_select_copy_int_15 = alloc_153 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_84: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_56[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_85: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_56[1];  executorch_call_delegate_56 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_40: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_17, [6, 257, 64]);  aten_expand_copy_default_17 = None\n        \n        # No stacktrace found for following nodes\n        alloc_154: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_18: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_84, [1, 6, 257, 64], out = alloc_154);  getitem_84 = alloc_154 = None\n        \n        # No stacktrace found for following nodes\n        alloc_155: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_19: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_85, [1, 6, 64, 257], out = alloc_155);  getitem_85 = alloc_155 = None\n        aten_view_copy_default_41: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_18, [6, 257, 64]);  aten_expand_copy_default_18 = None\n        aten_view_copy_default_42: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_19, [6, 64, 257]);  aten_expand_copy_default_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_57 = self.lowered_module_57\n        executorch_call_delegate_57 = torch.ops.higher_order.executorch_call_delegate(lowered_module_57, aten_view_copy_default_41, aten_view_copy_default_42);  lowered_module_57 = aten_view_copy_default_41 = aten_view_copy_default_42 = None\n        getitem_86: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_57[0];  executorch_call_delegate_57 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_43: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_86, [1, 6, 257, 257]);  getitem_86 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_58 = self.lowered_module_58\n        executorch_call_delegate_58 = torch.ops.higher_order.executorch_call_delegate(lowered_module_58, aten_view_copy_default_43);  lowered_module_58 = aten_view_copy_default_43 = None\n        getitem_87: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_58[0];  executorch_call_delegate_58 = None\n        alloc_156: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_21: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_87, out = alloc_156);  getitem_87 = alloc_156 = None\n        \n        # No stacktrace found for following nodes\n        alloc_157: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_20: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_21, [1, 6, 257, 257], out = alloc_157);  aten_clone_default_21 = alloc_157 = None\n        aten_view_copy_default_44: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_20, [6, 257, 257]);  aten_expand_copy_default_20 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_59 = self.lowered_module_59\n        executorch_call_delegate_59 = torch.ops.higher_order.executorch_call_delegate(lowered_module_59, aten_view_copy_default_44, aten_view_copy_default_40);  lowered_module_59 = aten_view_copy_default_44 = aten_view_copy_default_40 = None\n        getitem_88: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_59[0];  executorch_call_delegate_59 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_45: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_88, [1, 6, 257, 64]);  getitem_88 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_60 = self.lowered_module_60\n        executorch_call_delegate_60 = torch.ops.higher_order.executorch_call_delegate(lowered_module_60, aten_view_copy_default_45);  lowered_module_60 = aten_view_copy_default_45 = None\n        getitem_89: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_60[0];  executorch_call_delegate_60 = None\n        alloc_158: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_22: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_89, memory_format = torch.contiguous_format, out = alloc_158);  getitem_89 = alloc_158 = None\n        aten_view_copy_default_46: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_22, [1, 257, 384]);  aten_clone_default_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_61 = self.lowered_module_61\n        executorch_call_delegate_61 = torch.ops.higher_order.executorch_call_delegate(lowered_module_61, aten_view_copy_default_46);  lowered_module_61 = aten_view_copy_default_46 = None\n        getitem_90: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_61[0];  executorch_call_delegate_61 = None\n        alloc_159: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_23: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_90, out = alloc_159);  getitem_90 = alloc_159 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_62 = self.lowered_module_62\n        executorch_call_delegate_62 = torch.ops.higher_order.executorch_call_delegate(lowered_module_62, aten_clone_default_23, p_blocks_4_ls1_gamma, getitem_80);  lowered_module_62 = aten_clone_default_23 = p_blocks_4_ls1_gamma = getitem_80 = None\n        getitem_91: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_62[0];  executorch_call_delegate_62 = None\n        alloc_160: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_161: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_162: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_9 = torch.ops.aten.native_layer_norm.out(getitem_91, [384], p_blocks_4_norm2_weight, p_blocks_4_norm2_bias, 1e-06, out0 = alloc_160, out1 = alloc_161, out2 = alloc_162);  p_blocks_4_norm2_weight = p_blocks_4_norm2_bias = alloc_160 = alloc_161 = alloc_162 = None\n        getitem_92: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_9[0];  aten_native_layer_norm_default_9 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_63 = self.lowered_module_63\n        executorch_call_delegate_63 = torch.ops.higher_order.executorch_call_delegate(lowered_module_63, getitem_92);  lowered_module_63 = getitem_92 = None\n        getitem_93: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_63[0];  executorch_call_delegate_63 = None\n        alloc_163: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_24: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_93, out = alloc_163);  getitem_93 = alloc_163 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_64 = self.lowered_module_64\n        executorch_call_delegate_64 = torch.ops.higher_order.executorch_call_delegate(lowered_module_64, aten_clone_default_24);  lowered_module_64 = aten_clone_default_24 = None\n        getitem_94: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_64[0];  executorch_call_delegate_64 = None\n        alloc_164: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_25: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_94, out = alloc_164);  getitem_94 = alloc_164 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_65 = self.lowered_module_65\n        executorch_call_delegate_65 = torch.ops.higher_order.executorch_call_delegate(lowered_module_65, aten_clone_default_25, p_blocks_4_ls2_gamma, getitem_91);  lowered_module_65 = aten_clone_default_25 = p_blocks_4_ls2_gamma = getitem_91 = None\n        getitem_95: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_65[0];  executorch_call_delegate_65 = None\n        alloc_165: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_166: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_167: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_10 = torch.ops.aten.native_layer_norm.out(getitem_95, [384], p_blocks_5_norm1_weight, p_blocks_5_norm1_bias, 1e-06, out0 = alloc_165, out1 = alloc_166, out2 = alloc_167);  p_blocks_5_norm1_weight = p_blocks_5_norm1_bias = alloc_165 = alloc_166 = alloc_167 = None\n        getitem_96: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_10[0];  aten_native_layer_norm_default_10 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_66 = self.lowered_module_66\n        executorch_call_delegate_66 = torch.ops.higher_order.executorch_call_delegate(lowered_module_66, getitem_96);  lowered_module_66 = getitem_96 = None\n        getitem_97: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_66[0];  executorch_call_delegate_66 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_47: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_97, [1, 257, 3, 6, 64]);  getitem_97 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_67 = self.lowered_module_67\n        executorch_call_delegate_67 = torch.ops.higher_order.executorch_call_delegate(lowered_module_67, aten_view_copy_default_47);  lowered_module_67 = aten_view_copy_default_47 = None\n        getitem_98: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_67[0];  executorch_call_delegate_67 = None\n        alloc_168: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_16: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 0, out = alloc_168);  alloc_168 = None\n        \n        # No stacktrace found for following nodes\n        alloc_169: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_17: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 1, out = alloc_169);  alloc_169 = None\n        \n        # No stacktrace found for following nodes\n        alloc_170: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_18: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_98, 0, 2, out = alloc_170);  getitem_98 = alloc_170 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_68 = self.lowered_module_68\n        executorch_call_delegate_68 = torch.ops.higher_order.executorch_call_delegate(lowered_module_68, aten_select_copy_int_16, _lifted_tensor_constant37, aten_select_copy_int_17);  lowered_module_68 = aten_select_copy_int_16 = _lifted_tensor_constant37 = aten_select_copy_int_17 = None\n        alloc_171: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_18, [1, 6, 257, 64], out = alloc_171);  aten_select_copy_int_18 = alloc_171 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_99: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_68[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_100: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_68[1];  executorch_call_delegate_68 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_48: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_21, [6, 257, 64]);  aten_expand_copy_default_21 = None\n        \n        # No stacktrace found for following nodes\n        alloc_172: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_22: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_99, [1, 6, 257, 64], out = alloc_172);  getitem_99 = alloc_172 = None\n        \n        # No stacktrace found for following nodes\n        alloc_173: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_23: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_100, [1, 6, 64, 257], out = alloc_173);  getitem_100 = alloc_173 = None\n        aten_view_copy_default_49: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_22, [6, 257, 64]);  aten_expand_copy_default_22 = None\n        aten_view_copy_default_50: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_23, [6, 64, 257]);  aten_expand_copy_default_23 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_69 = self.lowered_module_69\n        executorch_call_delegate_69 = torch.ops.higher_order.executorch_call_delegate(lowered_module_69, aten_view_copy_default_49, aten_view_copy_default_50);  lowered_module_69 = aten_view_copy_default_49 = aten_view_copy_default_50 = None\n        getitem_101: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_69[0];  executorch_call_delegate_69 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_51: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_101, [1, 6, 257, 257]);  getitem_101 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_70 = self.lowered_module_70\n        executorch_call_delegate_70 = torch.ops.higher_order.executorch_call_delegate(lowered_module_70, aten_view_copy_default_51);  lowered_module_70 = aten_view_copy_default_51 = None\n        getitem_102: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_70[0];  executorch_call_delegate_70 = None\n        alloc_174: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_26: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_102, out = alloc_174);  getitem_102 = alloc_174 = None\n        \n        # No stacktrace found for following nodes\n        alloc_175: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_24: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_26, [1, 6, 257, 257], out = alloc_175);  aten_clone_default_26 = alloc_175 = None\n        aten_view_copy_default_52: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_24, [6, 257, 257]);  aten_expand_copy_default_24 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_71 = self.lowered_module_71\n        executorch_call_delegate_71 = torch.ops.higher_order.executorch_call_delegate(lowered_module_71, aten_view_copy_default_52, aten_view_copy_default_48);  lowered_module_71 = aten_view_copy_default_52 = aten_view_copy_default_48 = None\n        getitem_103: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_71[0];  executorch_call_delegate_71 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_53: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_103, [1, 6, 257, 64]);  getitem_103 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_72 = self.lowered_module_72\n        executorch_call_delegate_72 = torch.ops.higher_order.executorch_call_delegate(lowered_module_72, aten_view_copy_default_53);  lowered_module_72 = aten_view_copy_default_53 = None\n        getitem_104: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_72[0];  executorch_call_delegate_72 = None\n        alloc_176: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_27: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_104, memory_format = torch.contiguous_format, out = alloc_176);  getitem_104 = alloc_176 = None\n        aten_view_copy_default_54: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_27, [1, 257, 384]);  aten_clone_default_27 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_73 = self.lowered_module_73\n        executorch_call_delegate_73 = torch.ops.higher_order.executorch_call_delegate(lowered_module_73, aten_view_copy_default_54);  lowered_module_73 = aten_view_copy_default_54 = None\n        getitem_105: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_73[0];  executorch_call_delegate_73 = None\n        alloc_177: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_28: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_105, out = alloc_177);  getitem_105 = alloc_177 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_74 = self.lowered_module_74\n        executorch_call_delegate_74 = torch.ops.higher_order.executorch_call_delegate(lowered_module_74, aten_clone_default_28, p_blocks_5_ls1_gamma, getitem_95);  lowered_module_74 = aten_clone_default_28 = p_blocks_5_ls1_gamma = getitem_95 = None\n        getitem_106: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_74[0];  executorch_call_delegate_74 = None\n        alloc_178: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_179: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_180: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_11 = torch.ops.aten.native_layer_norm.out(getitem_106, [384], p_blocks_5_norm2_weight, p_blocks_5_norm2_bias, 1e-06, out0 = alloc_178, out1 = alloc_179, out2 = alloc_180);  p_blocks_5_norm2_weight = p_blocks_5_norm2_bias = alloc_178 = alloc_179 = alloc_180 = None\n        getitem_107: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_11[0];  aten_native_layer_norm_default_11 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_75 = self.lowered_module_75\n        executorch_call_delegate_75 = torch.ops.higher_order.executorch_call_delegate(lowered_module_75, getitem_107);  lowered_module_75 = getitem_107 = None\n        getitem_108: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_75[0];  executorch_call_delegate_75 = None\n        alloc_181: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_29: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_108, out = alloc_181);  getitem_108 = alloc_181 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_76 = self.lowered_module_76\n        executorch_call_delegate_76 = torch.ops.higher_order.executorch_call_delegate(lowered_module_76, aten_clone_default_29);  lowered_module_76 = aten_clone_default_29 = None\n        getitem_109: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_76[0];  executorch_call_delegate_76 = None\n        alloc_182: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_30: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_109, out = alloc_182);  getitem_109 = alloc_182 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_77 = self.lowered_module_77\n        executorch_call_delegate_77 = torch.ops.higher_order.executorch_call_delegate(lowered_module_77, aten_clone_default_30, p_blocks_5_ls2_gamma, getitem_106);  lowered_module_77 = aten_clone_default_30 = p_blocks_5_ls2_gamma = getitem_106 = None\n        getitem_110: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_77[0];  executorch_call_delegate_77 = None\n        alloc_183: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_184: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_185: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_12 = torch.ops.aten.native_layer_norm.out(getitem_110, [384], p_blocks_6_norm1_weight, p_blocks_6_norm1_bias, 1e-06, out0 = alloc_183, out1 = alloc_184, out2 = alloc_185);  p_blocks_6_norm1_weight = p_blocks_6_norm1_bias = alloc_183 = alloc_184 = alloc_185 = None\n        getitem_111: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_12[0];  aten_native_layer_norm_default_12 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_78 = self.lowered_module_78\n        executorch_call_delegate_78 = torch.ops.higher_order.executorch_call_delegate(lowered_module_78, getitem_111);  lowered_module_78 = getitem_111 = None\n        getitem_112: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_78[0];  executorch_call_delegate_78 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_55: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_112, [1, 257, 3, 6, 64]);  getitem_112 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_79 = self.lowered_module_79\n        executorch_call_delegate_79 = torch.ops.higher_order.executorch_call_delegate(lowered_module_79, aten_view_copy_default_55);  lowered_module_79 = aten_view_copy_default_55 = None\n        getitem_113: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_79[0];  executorch_call_delegate_79 = None\n        alloc_186: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_19: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 0, out = alloc_186);  alloc_186 = None\n        \n        # No stacktrace found for following nodes\n        alloc_187: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_20: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 1, out = alloc_187);  alloc_187 = None\n        \n        # No stacktrace found for following nodes\n        alloc_188: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_21: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_113, 0, 2, out = alloc_188);  getitem_113 = alloc_188 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_80 = self.lowered_module_80\n        executorch_call_delegate_80 = torch.ops.higher_order.executorch_call_delegate(lowered_module_80, aten_select_copy_int_19, _lifted_tensor_constant38, aten_select_copy_int_20);  lowered_module_80 = aten_select_copy_int_19 = _lifted_tensor_constant38 = aten_select_copy_int_20 = None\n        alloc_189: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_25: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_21, [1, 6, 257, 64], out = alloc_189);  aten_select_copy_int_21 = alloc_189 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_114: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_80[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_115: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_80[1];  executorch_call_delegate_80 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_56: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_25, [6, 257, 64]);  aten_expand_copy_default_25 = None\n        \n        # No stacktrace found for following nodes\n        alloc_190: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_26: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_114, [1, 6, 257, 64], out = alloc_190);  getitem_114 = alloc_190 = None\n        \n        # No stacktrace found for following nodes\n        alloc_191: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_27: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_115, [1, 6, 64, 257], out = alloc_191);  getitem_115 = alloc_191 = None\n        aten_view_copy_default_57: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_26, [6, 257, 64]);  aten_expand_copy_default_26 = None\n        aten_view_copy_default_58: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_27, [6, 64, 257]);  aten_expand_copy_default_27 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_81 = self.lowered_module_81\n        executorch_call_delegate_81 = torch.ops.higher_order.executorch_call_delegate(lowered_module_81, aten_view_copy_default_57, aten_view_copy_default_58);  lowered_module_81 = aten_view_copy_default_57 = aten_view_copy_default_58 = None\n        getitem_116: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_81[0];  executorch_call_delegate_81 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_59: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_116, [1, 6, 257, 257]);  getitem_116 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_82 = self.lowered_module_82\n        executorch_call_delegate_82 = torch.ops.higher_order.executorch_call_delegate(lowered_module_82, aten_view_copy_default_59);  lowered_module_82 = aten_view_copy_default_59 = None\n        getitem_117: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_82[0];  executorch_call_delegate_82 = None\n        alloc_192: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_31: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_117, out = alloc_192);  getitem_117 = alloc_192 = None\n        \n        # No stacktrace found for following nodes\n        alloc_193: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_28: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_31, [1, 6, 257, 257], out = alloc_193);  aten_clone_default_31 = alloc_193 = None\n        aten_view_copy_default_60: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_28, [6, 257, 257]);  aten_expand_copy_default_28 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_83 = self.lowered_module_83\n        executorch_call_delegate_83 = torch.ops.higher_order.executorch_call_delegate(lowered_module_83, aten_view_copy_default_60, aten_view_copy_default_56);  lowered_module_83 = aten_view_copy_default_60 = aten_view_copy_default_56 = None\n        getitem_118: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_83[0];  executorch_call_delegate_83 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_61: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_118, [1, 6, 257, 64]);  getitem_118 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_84 = self.lowered_module_84\n        executorch_call_delegate_84 = torch.ops.higher_order.executorch_call_delegate(lowered_module_84, aten_view_copy_default_61);  lowered_module_84 = aten_view_copy_default_61 = None\n        getitem_119: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_84[0];  executorch_call_delegate_84 = None\n        alloc_194: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_32: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_119, memory_format = torch.contiguous_format, out = alloc_194);  getitem_119 = alloc_194 = None\n        aten_view_copy_default_62: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_32, [1, 257, 384]);  aten_clone_default_32 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_85 = self.lowered_module_85\n        executorch_call_delegate_85 = torch.ops.higher_order.executorch_call_delegate(lowered_module_85, aten_view_copy_default_62);  lowered_module_85 = aten_view_copy_default_62 = None\n        getitem_120: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_85[0];  executorch_call_delegate_85 = None\n        alloc_195: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_33: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_120, out = alloc_195);  getitem_120 = alloc_195 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_86 = self.lowered_module_86\n        executorch_call_delegate_86 = torch.ops.higher_order.executorch_call_delegate(lowered_module_86, aten_clone_default_33, p_blocks_6_ls1_gamma, getitem_110);  lowered_module_86 = aten_clone_default_33 = p_blocks_6_ls1_gamma = getitem_110 = None\n        getitem_121: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_86[0];  executorch_call_delegate_86 = None\n        alloc_196: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_197: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_198: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_13 = torch.ops.aten.native_layer_norm.out(getitem_121, [384], p_blocks_6_norm2_weight, p_blocks_6_norm2_bias, 1e-06, out0 = alloc_196, out1 = alloc_197, out2 = alloc_198);  p_blocks_6_norm2_weight = p_blocks_6_norm2_bias = alloc_196 = alloc_197 = alloc_198 = None\n        getitem_122: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_13[0];  aten_native_layer_norm_default_13 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_87 = self.lowered_module_87\n        executorch_call_delegate_87 = torch.ops.higher_order.executorch_call_delegate(lowered_module_87, getitem_122);  lowered_module_87 = getitem_122 = None\n        getitem_123: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_87[0];  executorch_call_delegate_87 = None\n        alloc_199: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_34: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_123, out = alloc_199);  getitem_123 = alloc_199 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_88 = self.lowered_module_88\n        executorch_call_delegate_88 = torch.ops.higher_order.executorch_call_delegate(lowered_module_88, aten_clone_default_34);  lowered_module_88 = aten_clone_default_34 = None\n        getitem_124: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_88[0];  executorch_call_delegate_88 = None\n        alloc_200: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_35: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_124, out = alloc_200);  getitem_124 = alloc_200 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_89 = self.lowered_module_89\n        executorch_call_delegate_89 = torch.ops.higher_order.executorch_call_delegate(lowered_module_89, aten_clone_default_35, p_blocks_6_ls2_gamma, getitem_121);  lowered_module_89 = aten_clone_default_35 = p_blocks_6_ls2_gamma = getitem_121 = None\n        getitem_125: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_89[0];  executorch_call_delegate_89 = None\n        alloc_201: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_202: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_203: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_14 = torch.ops.aten.native_layer_norm.out(getitem_125, [384], p_blocks_7_norm1_weight, p_blocks_7_norm1_bias, 1e-06, out0 = alloc_201, out1 = alloc_202, out2 = alloc_203);  p_blocks_7_norm1_weight = p_blocks_7_norm1_bias = alloc_201 = alloc_202 = alloc_203 = None\n        getitem_126: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_14[0];  aten_native_layer_norm_default_14 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_90 = self.lowered_module_90\n        executorch_call_delegate_90 = torch.ops.higher_order.executorch_call_delegate(lowered_module_90, getitem_126);  lowered_module_90 = getitem_126 = None\n        getitem_127: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_90[0];  executorch_call_delegate_90 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_63: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_127, [1, 257, 3, 6, 64]);  getitem_127 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_91 = self.lowered_module_91\n        executorch_call_delegate_91 = torch.ops.higher_order.executorch_call_delegate(lowered_module_91, aten_view_copy_default_63);  lowered_module_91 = aten_view_copy_default_63 = None\n        getitem_128: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_91[0];  executorch_call_delegate_91 = None\n        alloc_204: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_22: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 0, out = alloc_204);  alloc_204 = None\n        \n        # No stacktrace found for following nodes\n        alloc_205: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_23: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 1, out = alloc_205);  alloc_205 = None\n        \n        # No stacktrace found for following nodes\n        alloc_206: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_24: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_128, 0, 2, out = alloc_206);  getitem_128 = alloc_206 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_92 = self.lowered_module_92\n        executorch_call_delegate_92 = torch.ops.higher_order.executorch_call_delegate(lowered_module_92, aten_select_copy_int_22, _lifted_tensor_constant39, aten_select_copy_int_23);  lowered_module_92 = aten_select_copy_int_22 = _lifted_tensor_constant39 = aten_select_copy_int_23 = None\n        alloc_207: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_24, [1, 6, 257, 64], out = alloc_207);  aten_select_copy_int_24 = alloc_207 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_129: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_92[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_130: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_92[1];  executorch_call_delegate_92 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_64: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_29, [6, 257, 64]);  aten_expand_copy_default_29 = None\n        \n        # No stacktrace found for following nodes\n        alloc_208: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_30: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_129, [1, 6, 257, 64], out = alloc_208);  getitem_129 = alloc_208 = None\n        \n        # No stacktrace found for following nodes\n        alloc_209: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_31: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_130, [1, 6, 64, 257], out = alloc_209);  getitem_130 = alloc_209 = None\n        aten_view_copy_default_65: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_30, [6, 257, 64]);  aten_expand_copy_default_30 = None\n        aten_view_copy_default_66: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_31, [6, 64, 257]);  aten_expand_copy_default_31 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_93 = self.lowered_module_93\n        executorch_call_delegate_93 = torch.ops.higher_order.executorch_call_delegate(lowered_module_93, aten_view_copy_default_65, aten_view_copy_default_66);  lowered_module_93 = aten_view_copy_default_65 = aten_view_copy_default_66 = None\n        getitem_131: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_93[0];  executorch_call_delegate_93 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_67: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_131, [1, 6, 257, 257]);  getitem_131 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_94 = self.lowered_module_94\n        executorch_call_delegate_94 = torch.ops.higher_order.executorch_call_delegate(lowered_module_94, aten_view_copy_default_67);  lowered_module_94 = aten_view_copy_default_67 = None\n        getitem_132: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_94[0];  executorch_call_delegate_94 = None\n        alloc_210: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_36: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_132, out = alloc_210);  getitem_132 = alloc_210 = None\n        \n        # No stacktrace found for following nodes\n        alloc_211: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_32: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_36, [1, 6, 257, 257], out = alloc_211);  aten_clone_default_36 = alloc_211 = None\n        aten_view_copy_default_68: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_32, [6, 257, 257]);  aten_expand_copy_default_32 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_95 = self.lowered_module_95\n        executorch_call_delegate_95 = torch.ops.higher_order.executorch_call_delegate(lowered_module_95, aten_view_copy_default_68, aten_view_copy_default_64);  lowered_module_95 = aten_view_copy_default_68 = aten_view_copy_default_64 = None\n        getitem_133: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_95[0];  executorch_call_delegate_95 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_69: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_133, [1, 6, 257, 64]);  getitem_133 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_96 = self.lowered_module_96\n        executorch_call_delegate_96 = torch.ops.higher_order.executorch_call_delegate(lowered_module_96, aten_view_copy_default_69);  lowered_module_96 = aten_view_copy_default_69 = None\n        getitem_134: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_96[0];  executorch_call_delegate_96 = None\n        alloc_212: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_37: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_134, memory_format = torch.contiguous_format, out = alloc_212);  getitem_134 = alloc_212 = None\n        aten_view_copy_default_70: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_37, [1, 257, 384]);  aten_clone_default_37 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_97 = self.lowered_module_97\n        executorch_call_delegate_97 = torch.ops.higher_order.executorch_call_delegate(lowered_module_97, aten_view_copy_default_70);  lowered_module_97 = aten_view_copy_default_70 = None\n        getitem_135: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_97[0];  executorch_call_delegate_97 = None\n        alloc_213: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_38: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_135, out = alloc_213);  getitem_135 = alloc_213 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_98 = self.lowered_module_98\n        executorch_call_delegate_98 = torch.ops.higher_order.executorch_call_delegate(lowered_module_98, aten_clone_default_38, p_blocks_7_ls1_gamma, getitem_125);  lowered_module_98 = aten_clone_default_38 = p_blocks_7_ls1_gamma = getitem_125 = None\n        getitem_136: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_98[0];  executorch_call_delegate_98 = None\n        alloc_214: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_215: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_216: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_15 = torch.ops.aten.native_layer_norm.out(getitem_136, [384], p_blocks_7_norm2_weight, p_blocks_7_norm2_bias, 1e-06, out0 = alloc_214, out1 = alloc_215, out2 = alloc_216);  p_blocks_7_norm2_weight = p_blocks_7_norm2_bias = alloc_214 = alloc_215 = alloc_216 = None\n        getitem_137: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_15[0];  aten_native_layer_norm_default_15 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_99 = self.lowered_module_99\n        executorch_call_delegate_99 = torch.ops.higher_order.executorch_call_delegate(lowered_module_99, getitem_137);  lowered_module_99 = getitem_137 = None\n        getitem_138: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_99[0];  executorch_call_delegate_99 = None\n        alloc_217: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_39: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_138, out = alloc_217);  getitem_138 = alloc_217 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_100 = self.lowered_module_100\n        executorch_call_delegate_100 = torch.ops.higher_order.executorch_call_delegate(lowered_module_100, aten_clone_default_39);  lowered_module_100 = aten_clone_default_39 = None\n        getitem_139: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_100[0];  executorch_call_delegate_100 = None\n        alloc_218: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_40: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_139, out = alloc_218);  getitem_139 = alloc_218 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_101 = self.lowered_module_101\n        executorch_call_delegate_101 = torch.ops.higher_order.executorch_call_delegate(lowered_module_101, aten_clone_default_40, p_blocks_7_ls2_gamma, getitem_136);  lowered_module_101 = aten_clone_default_40 = p_blocks_7_ls2_gamma = getitem_136 = None\n        getitem_140: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_101[0];  executorch_call_delegate_101 = None\n        alloc_219: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_220: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_221: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_16 = torch.ops.aten.native_layer_norm.out(getitem_140, [384], p_blocks_8_norm1_weight, p_blocks_8_norm1_bias, 1e-06, out0 = alloc_219, out1 = alloc_220, out2 = alloc_221);  p_blocks_8_norm1_weight = p_blocks_8_norm1_bias = alloc_219 = alloc_220 = alloc_221 = None\n        getitem_141: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_16[0];  aten_native_layer_norm_default_16 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_102 = self.lowered_module_102\n        executorch_call_delegate_102 = torch.ops.higher_order.executorch_call_delegate(lowered_module_102, getitem_141);  lowered_module_102 = getitem_141 = None\n        getitem_142: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_102[0];  executorch_call_delegate_102 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_71: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_142, [1, 257, 3, 6, 64]);  getitem_142 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_103 = self.lowered_module_103\n        executorch_call_delegate_103 = torch.ops.higher_order.executorch_call_delegate(lowered_module_103, aten_view_copy_default_71);  lowered_module_103 = aten_view_copy_default_71 = None\n        getitem_143: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_103[0];  executorch_call_delegate_103 = None\n        alloc_222: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_25: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 0, out = alloc_222);  alloc_222 = None\n        \n        # No stacktrace found for following nodes\n        alloc_223: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_26: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 1, out = alloc_223);  alloc_223 = None\n        \n        # No stacktrace found for following nodes\n        alloc_224: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_27: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_143, 0, 2, out = alloc_224);  getitem_143 = alloc_224 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_104 = self.lowered_module_104\n        executorch_call_delegate_104 = torch.ops.higher_order.executorch_call_delegate(lowered_module_104, aten_select_copy_int_25, _lifted_tensor_constant40, aten_select_copy_int_26);  lowered_module_104 = aten_select_copy_int_25 = _lifted_tensor_constant40 = aten_select_copy_int_26 = None\n        alloc_225: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_33: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_27, [1, 6, 257, 64], out = alloc_225);  aten_select_copy_int_27 = alloc_225 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_144: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_104[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_145: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_104[1];  executorch_call_delegate_104 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_72: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_33, [6, 257, 64]);  aten_expand_copy_default_33 = None\n        \n        # No stacktrace found for following nodes\n        alloc_226: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_34: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_144, [1, 6, 257, 64], out = alloc_226);  getitem_144 = alloc_226 = None\n        \n        # No stacktrace found for following nodes\n        alloc_227: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_35: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_145, [1, 6, 64, 257], out = alloc_227);  getitem_145 = alloc_227 = None\n        aten_view_copy_default_73: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_34, [6, 257, 64]);  aten_expand_copy_default_34 = None\n        aten_view_copy_default_74: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_35, [6, 64, 257]);  aten_expand_copy_default_35 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_105 = self.lowered_module_105\n        executorch_call_delegate_105 = torch.ops.higher_order.executorch_call_delegate(lowered_module_105, aten_view_copy_default_73, aten_view_copy_default_74);  lowered_module_105 = aten_view_copy_default_73 = aten_view_copy_default_74 = None\n        getitem_146: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_105[0];  executorch_call_delegate_105 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_75: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_146, [1, 6, 257, 257]);  getitem_146 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_106 = self.lowered_module_106\n        executorch_call_delegate_106 = torch.ops.higher_order.executorch_call_delegate(lowered_module_106, aten_view_copy_default_75);  lowered_module_106 = aten_view_copy_default_75 = None\n        getitem_147: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_106[0];  executorch_call_delegate_106 = None\n        alloc_228: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_41: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_147, out = alloc_228);  getitem_147 = alloc_228 = None\n        \n        # No stacktrace found for following nodes\n        alloc_229: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_36: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_41, [1, 6, 257, 257], out = alloc_229);  aten_clone_default_41 = alloc_229 = None\n        aten_view_copy_default_76: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_36, [6, 257, 257]);  aten_expand_copy_default_36 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_107 = self.lowered_module_107\n        executorch_call_delegate_107 = torch.ops.higher_order.executorch_call_delegate(lowered_module_107, aten_view_copy_default_76, aten_view_copy_default_72);  lowered_module_107 = aten_view_copy_default_76 = aten_view_copy_default_72 = None\n        getitem_148: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_107[0];  executorch_call_delegate_107 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_77: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_148, [1, 6, 257, 64]);  getitem_148 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_108 = self.lowered_module_108\n        executorch_call_delegate_108 = torch.ops.higher_order.executorch_call_delegate(lowered_module_108, aten_view_copy_default_77);  lowered_module_108 = aten_view_copy_default_77 = None\n        getitem_149: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_108[0];  executorch_call_delegate_108 = None\n        alloc_230: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_42: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_149, memory_format = torch.contiguous_format, out = alloc_230);  getitem_149 = alloc_230 = None\n        aten_view_copy_default_78: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_42, [1, 257, 384]);  aten_clone_default_42 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_109 = self.lowered_module_109\n        executorch_call_delegate_109 = torch.ops.higher_order.executorch_call_delegate(lowered_module_109, aten_view_copy_default_78);  lowered_module_109 = aten_view_copy_default_78 = None\n        getitem_150: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_109[0];  executorch_call_delegate_109 = None\n        alloc_231: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_43: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_150, out = alloc_231);  getitem_150 = alloc_231 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_110 = self.lowered_module_110\n        executorch_call_delegate_110 = torch.ops.higher_order.executorch_call_delegate(lowered_module_110, aten_clone_default_43, p_blocks_8_ls1_gamma, getitem_140);  lowered_module_110 = aten_clone_default_43 = p_blocks_8_ls1_gamma = getitem_140 = None\n        getitem_151: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_110[0];  executorch_call_delegate_110 = None\n        alloc_232: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_233: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_234: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_17 = torch.ops.aten.native_layer_norm.out(getitem_151, [384], p_blocks_8_norm2_weight, p_blocks_8_norm2_bias, 1e-06, out0 = alloc_232, out1 = alloc_233, out2 = alloc_234);  p_blocks_8_norm2_weight = p_blocks_8_norm2_bias = alloc_232 = alloc_233 = alloc_234 = None\n        getitem_152: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_17[0];  aten_native_layer_norm_default_17 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_111 = self.lowered_module_111\n        executorch_call_delegate_111 = torch.ops.higher_order.executorch_call_delegate(lowered_module_111, getitem_152);  lowered_module_111 = getitem_152 = None\n        getitem_153: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_111[0];  executorch_call_delegate_111 = None\n        alloc_235: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_44: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_153, out = alloc_235);  getitem_153 = alloc_235 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_112 = self.lowered_module_112\n        executorch_call_delegate_112 = torch.ops.higher_order.executorch_call_delegate(lowered_module_112, aten_clone_default_44);  lowered_module_112 = aten_clone_default_44 = None\n        getitem_154: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_112[0];  executorch_call_delegate_112 = None\n        alloc_236: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_45: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_154, out = alloc_236);  getitem_154 = alloc_236 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_113 = self.lowered_module_113\n        executorch_call_delegate_113 = torch.ops.higher_order.executorch_call_delegate(lowered_module_113, aten_clone_default_45, p_blocks_8_ls2_gamma, getitem_151);  lowered_module_113 = aten_clone_default_45 = p_blocks_8_ls2_gamma = getitem_151 = None\n        getitem_155: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_113[0];  executorch_call_delegate_113 = None\n        alloc_237: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_238: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_239: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_18 = torch.ops.aten.native_layer_norm.out(getitem_155, [384], p_blocks_9_norm1_weight, p_blocks_9_norm1_bias, 1e-06, out0 = alloc_237, out1 = alloc_238, out2 = alloc_239);  p_blocks_9_norm1_weight = p_blocks_9_norm1_bias = alloc_237 = alloc_238 = alloc_239 = None\n        getitem_156: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_18[0];  aten_native_layer_norm_default_18 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_114 = self.lowered_module_114\n        executorch_call_delegate_114 = torch.ops.higher_order.executorch_call_delegate(lowered_module_114, getitem_156);  lowered_module_114 = getitem_156 = None\n        getitem_157: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_114[0];  executorch_call_delegate_114 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_79: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_157, [1, 257, 3, 6, 64]);  getitem_157 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_115 = self.lowered_module_115\n        executorch_call_delegate_115 = torch.ops.higher_order.executorch_call_delegate(lowered_module_115, aten_view_copy_default_79);  lowered_module_115 = aten_view_copy_default_79 = None\n        getitem_158: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_115[0];  executorch_call_delegate_115 = None\n        alloc_240: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_28: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 0, out = alloc_240);  alloc_240 = None\n        \n        # No stacktrace found for following nodes\n        alloc_241: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_29: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 1, out = alloc_241);  alloc_241 = None\n        \n        # No stacktrace found for following nodes\n        alloc_242: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_30: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_158, 0, 2, out = alloc_242);  getitem_158 = alloc_242 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_116 = self.lowered_module_116\n        executorch_call_delegate_116 = torch.ops.higher_order.executorch_call_delegate(lowered_module_116, aten_select_copy_int_28, _lifted_tensor_constant41, aten_select_copy_int_29);  lowered_module_116 = aten_select_copy_int_28 = _lifted_tensor_constant41 = aten_select_copy_int_29 = None\n        alloc_243: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_37: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_30, [1, 6, 257, 64], out = alloc_243);  aten_select_copy_int_30 = alloc_243 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_159: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_116[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_160: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_116[1];  executorch_call_delegate_116 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_80: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_37, [6, 257, 64]);  aten_expand_copy_default_37 = None\n        \n        # No stacktrace found for following nodes\n        alloc_244: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_38: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_159, [1, 6, 257, 64], out = alloc_244);  getitem_159 = alloc_244 = None\n        \n        # No stacktrace found for following nodes\n        alloc_245: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_39: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_160, [1, 6, 64, 257], out = alloc_245);  getitem_160 = alloc_245 = None\n        aten_view_copy_default_81: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_38, [6, 257, 64]);  aten_expand_copy_default_38 = None\n        aten_view_copy_default_82: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_39, [6, 64, 257]);  aten_expand_copy_default_39 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_117 = self.lowered_module_117\n        executorch_call_delegate_117 = torch.ops.higher_order.executorch_call_delegate(lowered_module_117, aten_view_copy_default_81, aten_view_copy_default_82);  lowered_module_117 = aten_view_copy_default_81 = aten_view_copy_default_82 = None\n        getitem_161: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_117[0];  executorch_call_delegate_117 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_83: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_161, [1, 6, 257, 257]);  getitem_161 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_118 = self.lowered_module_118\n        executorch_call_delegate_118 = torch.ops.higher_order.executorch_call_delegate(lowered_module_118, aten_view_copy_default_83);  lowered_module_118 = aten_view_copy_default_83 = None\n        getitem_162: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_118[0];  executorch_call_delegate_118 = None\n        alloc_246: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_46: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_162, out = alloc_246);  getitem_162 = alloc_246 = None\n        \n        # No stacktrace found for following nodes\n        alloc_247: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_40: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_46, [1, 6, 257, 257], out = alloc_247);  aten_clone_default_46 = alloc_247 = None\n        aten_view_copy_default_84: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_40, [6, 257, 257]);  aten_expand_copy_default_40 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_119 = self.lowered_module_119\n        executorch_call_delegate_119 = torch.ops.higher_order.executorch_call_delegate(lowered_module_119, aten_view_copy_default_84, aten_view_copy_default_80);  lowered_module_119 = aten_view_copy_default_84 = aten_view_copy_default_80 = None\n        getitem_163: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_119[0];  executorch_call_delegate_119 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_85: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_163, [1, 6, 257, 64]);  getitem_163 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_120 = self.lowered_module_120\n        executorch_call_delegate_120 = torch.ops.higher_order.executorch_call_delegate(lowered_module_120, aten_view_copy_default_85);  lowered_module_120 = aten_view_copy_default_85 = None\n        getitem_164: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_120[0];  executorch_call_delegate_120 = None\n        alloc_248: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_47: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_164, memory_format = torch.contiguous_format, out = alloc_248);  getitem_164 = alloc_248 = None\n        aten_view_copy_default_86: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_47, [1, 257, 384]);  aten_clone_default_47 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_121 = self.lowered_module_121\n        executorch_call_delegate_121 = torch.ops.higher_order.executorch_call_delegate(lowered_module_121, aten_view_copy_default_86);  lowered_module_121 = aten_view_copy_default_86 = None\n        getitem_165: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_121[0];  executorch_call_delegate_121 = None\n        alloc_249: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_48: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_165, out = alloc_249);  getitem_165 = alloc_249 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_122 = self.lowered_module_122\n        executorch_call_delegate_122 = torch.ops.higher_order.executorch_call_delegate(lowered_module_122, aten_clone_default_48, p_blocks_9_ls1_gamma, getitem_155);  lowered_module_122 = aten_clone_default_48 = p_blocks_9_ls1_gamma = getitem_155 = None\n        getitem_166: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_122[0];  executorch_call_delegate_122 = None\n        alloc_250: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_251: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_252: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_19 = torch.ops.aten.native_layer_norm.out(getitem_166, [384], p_blocks_9_norm2_weight, p_blocks_9_norm2_bias, 1e-06, out0 = alloc_250, out1 = alloc_251, out2 = alloc_252);  p_blocks_9_norm2_weight = p_blocks_9_norm2_bias = alloc_250 = alloc_251 = alloc_252 = None\n        getitem_167: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_19[0];  aten_native_layer_norm_default_19 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_123 = self.lowered_module_123\n        executorch_call_delegate_123 = torch.ops.higher_order.executorch_call_delegate(lowered_module_123, getitem_167);  lowered_module_123 = getitem_167 = None\n        getitem_168: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_123[0];  executorch_call_delegate_123 = None\n        alloc_253: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_49: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_168, out = alloc_253);  getitem_168 = alloc_253 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_124 = self.lowered_module_124\n        executorch_call_delegate_124 = torch.ops.higher_order.executorch_call_delegate(lowered_module_124, aten_clone_default_49);  lowered_module_124 = aten_clone_default_49 = None\n        getitem_169: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_124[0];  executorch_call_delegate_124 = None\n        alloc_254: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_50: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_169, out = alloc_254);  getitem_169 = alloc_254 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_125 = self.lowered_module_125\n        executorch_call_delegate_125 = torch.ops.higher_order.executorch_call_delegate(lowered_module_125, aten_clone_default_50, p_blocks_9_ls2_gamma, getitem_166);  lowered_module_125 = aten_clone_default_50 = p_blocks_9_ls2_gamma = getitem_166 = None\n        getitem_170: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_125[0];  executorch_call_delegate_125 = None\n        alloc_255: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_256: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_257: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_20 = torch.ops.aten.native_layer_norm.out(getitem_170, [384], p_blocks_10_norm1_weight, p_blocks_10_norm1_bias, 1e-06, out0 = alloc_255, out1 = alloc_256, out2 = alloc_257);  p_blocks_10_norm1_weight = p_blocks_10_norm1_bias = alloc_255 = alloc_256 = alloc_257 = None\n        getitem_171: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_20[0];  aten_native_layer_norm_default_20 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_126 = self.lowered_module_126\n        executorch_call_delegate_126 = torch.ops.higher_order.executorch_call_delegate(lowered_module_126, getitem_171);  lowered_module_126 = getitem_171 = None\n        getitem_172: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_126[0];  executorch_call_delegate_126 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_87: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_172, [1, 257, 3, 6, 64]);  getitem_172 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_127 = self.lowered_module_127\n        executorch_call_delegate_127 = torch.ops.higher_order.executorch_call_delegate(lowered_module_127, aten_view_copy_default_87);  lowered_module_127 = aten_view_copy_default_87 = None\n        getitem_173: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_127[0];  executorch_call_delegate_127 = None\n        alloc_258: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_31: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 0, out = alloc_258);  alloc_258 = None\n        \n        # No stacktrace found for following nodes\n        alloc_259: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_32: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 1, out = alloc_259);  alloc_259 = None\n        \n        # No stacktrace found for following nodes\n        alloc_260: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_33: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_173, 0, 2, out = alloc_260);  getitem_173 = alloc_260 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_128 = self.lowered_module_128\n        executorch_call_delegate_128 = torch.ops.higher_order.executorch_call_delegate(lowered_module_128, aten_select_copy_int_31, _lifted_tensor_constant42, aten_select_copy_int_32);  lowered_module_128 = aten_select_copy_int_31 = _lifted_tensor_constant42 = aten_select_copy_int_32 = None\n        alloc_261: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_41: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_33, [1, 6, 257, 64], out = alloc_261);  aten_select_copy_int_33 = alloc_261 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_174: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_128[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_175: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_128[1];  executorch_call_delegate_128 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_88: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_41, [6, 257, 64]);  aten_expand_copy_default_41 = None\n        \n        # No stacktrace found for following nodes\n        alloc_262: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_42: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_174, [1, 6, 257, 64], out = alloc_262);  getitem_174 = alloc_262 = None\n        \n        # No stacktrace found for following nodes\n        alloc_263: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_43: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_175, [1, 6, 64, 257], out = alloc_263);  getitem_175 = alloc_263 = None\n        aten_view_copy_default_89: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_42, [6, 257, 64]);  aten_expand_copy_default_42 = None\n        aten_view_copy_default_90: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_43, [6, 64, 257]);  aten_expand_copy_default_43 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_129 = self.lowered_module_129\n        executorch_call_delegate_129 = torch.ops.higher_order.executorch_call_delegate(lowered_module_129, aten_view_copy_default_89, aten_view_copy_default_90);  lowered_module_129 = aten_view_copy_default_89 = aten_view_copy_default_90 = None\n        getitem_176: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_129[0];  executorch_call_delegate_129 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_91: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_176, [1, 6, 257, 257]);  getitem_176 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_130 = self.lowered_module_130\n        executorch_call_delegate_130 = torch.ops.higher_order.executorch_call_delegate(lowered_module_130, aten_view_copy_default_91);  lowered_module_130 = aten_view_copy_default_91 = None\n        getitem_177: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_130[0];  executorch_call_delegate_130 = None\n        alloc_264: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_51: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_177, out = alloc_264);  getitem_177 = alloc_264 = None\n        \n        # No stacktrace found for following nodes\n        alloc_265: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_44: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_51, [1, 6, 257, 257], out = alloc_265);  aten_clone_default_51 = alloc_265 = None\n        aten_view_copy_default_92: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_44, [6, 257, 257]);  aten_expand_copy_default_44 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_131 = self.lowered_module_131\n        executorch_call_delegate_131 = torch.ops.higher_order.executorch_call_delegate(lowered_module_131, aten_view_copy_default_92, aten_view_copy_default_88);  lowered_module_131 = aten_view_copy_default_92 = aten_view_copy_default_88 = None\n        getitem_178: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_131[0];  executorch_call_delegate_131 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_93: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_178, [1, 6, 257, 64]);  getitem_178 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_132 = self.lowered_module_132\n        executorch_call_delegate_132 = torch.ops.higher_order.executorch_call_delegate(lowered_module_132, aten_view_copy_default_93);  lowered_module_132 = aten_view_copy_default_93 = None\n        getitem_179: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_132[0];  executorch_call_delegate_132 = None\n        alloc_266: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_52: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_179, memory_format = torch.contiguous_format, out = alloc_266);  getitem_179 = alloc_266 = None\n        aten_view_copy_default_94: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_52, [1, 257, 384]);  aten_clone_default_52 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_133 = self.lowered_module_133\n        executorch_call_delegate_133 = torch.ops.higher_order.executorch_call_delegate(lowered_module_133, aten_view_copy_default_94);  lowered_module_133 = aten_view_copy_default_94 = None\n        getitem_180: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_133[0];  executorch_call_delegate_133 = None\n        alloc_267: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_53: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_180, out = alloc_267);  getitem_180 = alloc_267 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_134 = self.lowered_module_134\n        executorch_call_delegate_134 = torch.ops.higher_order.executorch_call_delegate(lowered_module_134, aten_clone_default_53, p_blocks_10_ls1_gamma, getitem_170);  lowered_module_134 = aten_clone_default_53 = p_blocks_10_ls1_gamma = getitem_170 = None\n        getitem_181: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_134[0];  executorch_call_delegate_134 = None\n        alloc_268: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_269: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_270: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_21 = torch.ops.aten.native_layer_norm.out(getitem_181, [384], p_blocks_10_norm2_weight, p_blocks_10_norm2_bias, 1e-06, out0 = alloc_268, out1 = alloc_269, out2 = alloc_270);  p_blocks_10_norm2_weight = p_blocks_10_norm2_bias = alloc_268 = alloc_269 = alloc_270 = None\n        getitem_182: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_21[0];  aten_native_layer_norm_default_21 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_135 = self.lowered_module_135\n        executorch_call_delegate_135 = torch.ops.higher_order.executorch_call_delegate(lowered_module_135, getitem_182);  lowered_module_135 = getitem_182 = None\n        getitem_183: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_135[0];  executorch_call_delegate_135 = None\n        alloc_271: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_54: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_183, out = alloc_271);  getitem_183 = alloc_271 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_136 = self.lowered_module_136\n        executorch_call_delegate_136 = torch.ops.higher_order.executorch_call_delegate(lowered_module_136, aten_clone_default_54);  lowered_module_136 = aten_clone_default_54 = None\n        getitem_184: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_136[0];  executorch_call_delegate_136 = None\n        alloc_272: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_55: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_184, out = alloc_272);  getitem_184 = alloc_272 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_137 = self.lowered_module_137\n        executorch_call_delegate_137 = torch.ops.higher_order.executorch_call_delegate(lowered_module_137, aten_clone_default_55, p_blocks_10_ls2_gamma, getitem_181);  lowered_module_137 = aten_clone_default_55 = p_blocks_10_ls2_gamma = getitem_181 = None\n        getitem_185: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_137[0];  executorch_call_delegate_137 = None\n        alloc_273: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_274: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_275: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_22 = torch.ops.aten.native_layer_norm.out(getitem_185, [384], p_blocks_11_norm1_weight, p_blocks_11_norm1_bias, 1e-06, out0 = alloc_273, out1 = alloc_274, out2 = alloc_275);  p_blocks_11_norm1_weight = p_blocks_11_norm1_bias = alloc_273 = alloc_274 = alloc_275 = None\n        getitem_186: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_22[0];  aten_native_layer_norm_default_22 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_138 = self.lowered_module_138\n        executorch_call_delegate_138 = torch.ops.higher_order.executorch_call_delegate(lowered_module_138, getitem_186);  lowered_module_138 = getitem_186 = None\n        getitem_187: \"f32[1, 257, 1152][296064, 1152, 1]\" = executorch_call_delegate_138[0];  executorch_call_delegate_138 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:58 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        aten_view_copy_default_95: \"f32[1, 257, 3, 6, 64][296064, 1152, 384, 64, 1]\" = executorch_exir_memory_view(getitem_187, [1, 257, 3, 6, 64]);  getitem_187 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_139 = self.lowered_module_139\n        executorch_call_delegate_139 = torch.ops.higher_order.executorch_call_delegate(lowered_module_139, aten_view_copy_default_95);  lowered_module_139 = aten_view_copy_default_95 = None\n        getitem_188: \"f32[3, 1, 6, 257, 64][98688, 98688, 16448, 64, 1]\" = executorch_call_delegate_139[0];  executorch_call_delegate_139 = None\n        alloc_276: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_34: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 0, out = alloc_276);  alloc_276 = None\n        \n        # No stacktrace found for following nodes\n        alloc_277: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_35: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 1, out = alloc_277);  alloc_277 = None\n        \n        # No stacktrace found for following nodes\n        alloc_278: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        aten_select_copy_int_36: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.select_copy.int_out(getitem_188, 0, 2, out = alloc_278);  getitem_188 = alloc_278 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_140 = self.lowered_module_140\n        executorch_call_delegate_140 = torch.ops.higher_order.executorch_call_delegate(lowered_module_140, aten_select_copy_int_34, _lifted_tensor_constant43, aten_select_copy_int_35);  lowered_module_140 = aten_select_copy_int_34 = _lifted_tensor_constant43 = aten_select_copy_int_35 = None\n        alloc_279: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_45: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(aten_select_copy_int_36, [1, 6, 257, 64], out = alloc_279);  aten_select_copy_int_36 = alloc_279 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:60 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n        getitem_189: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_call_delegate_140[0]\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        getitem_190: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_call_delegate_140[1];  executorch_call_delegate_140 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_96: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_45, [6, 257, 64]);  aten_expand_copy_default_45 = None\n        \n        # No stacktrace found for following nodes\n        alloc_280: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_46: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = torch.ops.aten.expand_copy.out(getitem_189, [1, 6, 257, 64], out = alloc_280);  getitem_189 = alloc_280 = None\n        \n        # No stacktrace found for following nodes\n        alloc_281: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 64, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_expand_copy_default_47: \"f32[1, 6, 64, 257][98688, 16448, 257, 1]\" = torch.ops.aten.expand_copy.out(getitem_190, [1, 6, 64, 257], out = alloc_281);  getitem_190 = alloc_281 = None\n        aten_view_copy_default_97: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_46, [6, 257, 64]);  aten_expand_copy_default_46 = None\n        aten_view_copy_default_98: \"f32[6, 64, 257][16448, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_47, [6, 64, 257]);  aten_expand_copy_default_47 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_141 = self.lowered_module_141\n        executorch_call_delegate_141 = torch.ops.higher_order.executorch_call_delegate(lowered_module_141, aten_view_copy_default_97, aten_view_copy_default_98);  lowered_module_141 = aten_view_copy_default_97 = aten_view_copy_default_98 = None\n        getitem_191: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_call_delegate_141[0];  executorch_call_delegate_141 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:61 in forward, code: attn = q @ k.transpose(-2, -1)\n        aten_view_copy_default_99: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_view(getitem_191, [1, 6, 257, 257]);  getitem_191 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_142 = self.lowered_module_142\n        executorch_call_delegate_142 = torch.ops.higher_order.executorch_call_delegate(lowered_module_142, aten_view_copy_default_99);  lowered_module_142 = aten_view_copy_default_99 = None\n        getitem_192: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_call_delegate_142[0];  executorch_call_delegate_142 = None\n        alloc_282: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_56: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.clone.out(getitem_192, out = alloc_282);  getitem_192 = alloc_282 = None\n        \n        # No stacktrace found for following nodes\n        alloc_283: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = executorch_exir_memory_alloc(((1, 6, 257, 257), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_expand_copy_default_48: \"f32[1, 6, 257, 257][396294, 66049, 257, 1]\" = torch.ops.aten.expand_copy.out(aten_clone_default_56, [1, 6, 257, 257], out = alloc_283);  aten_clone_default_56 = alloc_283 = None\n        aten_view_copy_default_100: \"f32[6, 257, 257][66049, 257, 1]\" = executorch_exir_memory_view(aten_expand_copy_default_48, [6, 257, 257]);  aten_expand_copy_default_48 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_143 = self.lowered_module_143\n        executorch_call_delegate_143 = torch.ops.higher_order.executorch_call_delegate(lowered_module_143, aten_view_copy_default_100, aten_view_copy_default_96);  lowered_module_143 = aten_view_copy_default_100 = aten_view_copy_default_96 = None\n        getitem_193: \"f32[6, 257, 64][16448, 64, 1]\" = executorch_call_delegate_143[0];  executorch_call_delegate_143 = None\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_view_copy_default_101: \"f32[1, 6, 257, 64][98688, 16448, 64, 1]\" = executorch_exir_memory_view(getitem_193, [1, 6, 257, 64]);  getitem_193 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_144 = self.lowered_module_144\n        executorch_call_delegate_144 = torch.ops.higher_order.executorch_call_delegate(lowered_module_144, aten_view_copy_default_101);  lowered_module_144 = aten_view_copy_default_101 = None\n        getitem_194: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_call_delegate_144[0];  executorch_call_delegate_144 = None\n        alloc_284: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = executorch_exir_memory_alloc(((1, 257, 6, 64), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:66 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        aten_clone_default_57: \"f32[1, 257, 6, 64][98688, 384, 64, 1]\" = torch.ops.aten.clone.out(getitem_194, memory_format = torch.contiguous_format, out = alloc_284);  getitem_194 = alloc_284 = None\n        aten_view_copy_default_102: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_view(aten_clone_default_57, [1, 257, 384]);  aten_clone_default_57 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_145 = self.lowered_module_145\n        executorch_call_delegate_145 = torch.ops.higher_order.executorch_call_delegate(lowered_module_145, aten_view_copy_default_102);  lowered_module_145 = aten_view_copy_default_102 = None\n        getitem_195: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_145[0];  executorch_call_delegate_145 = None\n        alloc_285: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_58: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_195, out = alloc_285);  getitem_195 = alloc_285 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_146 = self.lowered_module_146\n        executorch_call_delegate_146 = torch.ops.higher_order.executorch_call_delegate(lowered_module_146, aten_clone_default_58, p_blocks_11_ls1_gamma, getitem_185);  lowered_module_146 = aten_clone_default_58 = p_blocks_11_ls1_gamma = getitem_185 = None\n        getitem_196: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_146[0];  executorch_call_delegate_146 = None\n        alloc_286: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_287: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_288: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_23 = torch.ops.aten.native_layer_norm.out(getitem_196, [384], p_blocks_11_norm2_weight, p_blocks_11_norm2_bias, 1e-06, out0 = alloc_286, out1 = alloc_287, out2 = alloc_288);  p_blocks_11_norm2_weight = p_blocks_11_norm2_bias = alloc_286 = alloc_287 = alloc_288 = None\n        getitem_197: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_23[0];  aten_native_layer_norm_default_23 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_147 = self.lowered_module_147\n        executorch_call_delegate_147 = torch.ops.higher_order.executorch_call_delegate(lowered_module_147, getitem_197);  lowered_module_147 = getitem_197 = None\n        getitem_198: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_call_delegate_147[0];  executorch_call_delegate_147 = None\n        alloc_289: \"f32[1, 257, 1536][394752, 1536, 1]\" = executorch_exir_memory_alloc(((1, 257, 1536), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_59: \"f32[1, 257, 1536][394752, 1536, 1]\" = torch.ops.aten.clone.out(getitem_198, out = alloc_289);  getitem_198 = alloc_289 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_148 = self.lowered_module_148\n        executorch_call_delegate_148 = torch.ops.higher_order.executorch_call_delegate(lowered_module_148, aten_clone_default_59);  lowered_module_148 = aten_clone_default_59 = None\n        getitem_199: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_148[0];  executorch_call_delegate_148 = None\n        alloc_290: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n        aten_clone_default_60: \"f32[1, 257, 384][98688, 384, 1]\" = torch.ops.aten.clone.out(getitem_199, out = alloc_290);  getitem_199 = alloc_290 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_149 = self.lowered_module_149\n        executorch_call_delegate_149 = torch.ops.higher_order.executorch_call_delegate(lowered_module_149, aten_clone_default_60, p_blocks_11_ls2_gamma, getitem_196);  lowered_module_149 = aten_clone_default_60 = p_blocks_11_ls2_gamma = getitem_196 = None\n        getitem_200: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_call_delegate_149[0];  executorch_call_delegate_149 = None\n        alloc_291: \"f32[1, 257, 384][98688, 384, 1]\" = executorch_exir_memory_alloc(((1, 257, 384), torch.float32))\n        alloc_292: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        alloc_293: \"f32[1, 257, 1][257, 1, 1]\" = executorch_exir_memory_alloc(((1, 257, 1), torch.float32))\n        \n         # File: /home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n        aten_native_layer_norm_default_24 = torch.ops.aten.native_layer_norm.out(getitem_200, [384], p_norm_weight, p_norm_bias, 1e-06, out0 = alloc_291, out1 = alloc_292, out2 = alloc_293);  getitem_200 = p_norm_weight = p_norm_bias = alloc_291 = alloc_292 = alloc_293 = None\n        getitem_201: \"f32[1, 257, 384][98688, 384, 1]\" = aten_native_layer_norm_default_24[0];  aten_native_layer_norm_default_24 = None\n        \n        # No stacktrace found for following nodes\n        alloc_294: \"f32[1, 384][384, 1]\" = executorch_exir_memory_alloc(((1, 384), torch.float32))\n        \n         # File: /home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:325 in forward, code: ret = self.forward_features(*args, **kwargs)\n        aten_select_copy_int_37: \"f32[1, 384][384, 1]\" = torch.ops.aten.select_copy.int_out(getitem_201, 1, 0, out = alloc_294);  getitem_201 = alloc_294 = None\n        \n        # No stacktrace found for following nodes\n        lowered_module_150 = self.lowered_module_150\n        executorch_call_delegate_150 = torch.ops.higher_order.executorch_call_delegate(lowered_module_150, aten_select_copy_int_37);  lowered_module_150 = aten_select_copy_int_37 = None\n        getitem_202: \"f32[1, 3][3, 1]\" = executorch_call_delegate_150[0];  executorch_call_delegate_150 = None\n        return (getitem_202,)\n        \n\nOriginal traceback:\nFile \"/home/eden/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 325, in forward\n    ret = self.forward_features(*args, **kwargs)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "\n",
    "class_names = ['downy', 'healthy', 'powdery']\n",
    "\n",
    "sample_inputs = (torch.randn(1, 3, 224, 224), )\n",
    "\n",
    "\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "# ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ë°ì´í„°ì…‹ì˜ í´ëž˜ìŠ¤ ê°œìˆ˜ì— ë§žì¶° ìžë™ìœ¼ë¡œ ì„¤ì •\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\n",
    "\n",
    "model.load_state_dict(torch.load('/home/eden/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/AI/MobileNet/best_mobilenet_cucumber.pth', weights_only=False, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "et_program = to_edge_transform_and_lower(\n",
    "    torch.export.export(model, sample_inputs),\n",
    "    partitioner=[XnnpackPartitioner()]\n",
    ").to_executorch()\n",
    "\n",
    "with open(\"vit_model.pte\", \"wb\") as f:\n",
    "    f.write(et_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaab797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:134] InternalConsistency verification requested but not available\n",
      "[tensor_util_portable.cpp:130] Check failed (all_contiguous || all_channels_last): 2 input tensors have different dim orders\n",
      "[op_clone.cpp:41] Check failed (tensors_have_same_dim_order(self, out)): \n",
      "[method.cpp:1314] KernelCall failed at instruction 0:78 in operator aten::clone.out: 0x12\n",
      "[method.cpp:1324] arg 0 with type id 1\n",
      "[method.cpp:1324] arg 1 with type id 4\n",
      "[method.cpp:1324] arg 2 with type id 1\n",
      "[method.cpp:1324] arg 3 with type id 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "method->execute() failed with error 0x12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m program = runtime.load_program(\u001b[33m\"\u001b[39m\u001b[33mvit_model.pte\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m method = program.load_method(\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output: List[torch.Tensor] = \u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRun successfully via executorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmobilenetv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MobileNet_V2_Weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/JNU/2025-2/Industry-University-Cooperation-Project/Cucumber-Disease-Prediction/.venv/lib/python3.12/site-packages/executorch/runtime/__init__.py:79\u001b[39m, in \u001b[36mMethod.execute\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Sequence[Any]) -> Sequence[Any]:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes the method with the given inputs.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m        The outputs of the method.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: method->execute() failed with error 0x12"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from executorch.runtime import Runtime\n",
    "from typing import List\n",
    "\n",
    "runtime = Runtime.get()\n",
    "\n",
    "input_tensor: torch.Tensor = torch.randn(1, 3, 224, 224)\n",
    "program = runtime.load_program(\"vit_model.pte\")\n",
    "method = program.load_method(\"forward\")\n",
    "output: List[torch.Tensor] = method.execute([input_tensor])\n",
    "print(\"Run successfully via executorch\")\n",
    "\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "import torchvision.models as models\n",
    "\n",
    "eager_reference_model = model.eval()\n",
    "eager_reference_output = eager_reference_model(input_tensor)\n",
    "\n",
    "print(\"Comparing against original PyTorch module\")\n",
    "print(torch.allclose(output[0], eager_reference_output, rtol=1e-3, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332b49d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cucumber-Disease-Prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
