{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69b7e13-6252-485a-8d7c-a8394a2f822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json \n",
    "\n",
    "# -----------------------------------\n",
    "# 커스텀 데이터셋 클래스 정의\n",
    "# -----------------------------------\n",
    "class CropDataset(Dataset):\n",
    "    \"\"\"\n",
    "    어노테이션을 읽어 이미지를 잘라낸 뒤 반환하는 커스텀 데이터셋\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.classes, self.class_to_idx = self._find_classes(self.image_dir)\n",
    "\n",
    "        # 이미지와 어노테이션 경로, 라벨을 미리 스캔하여 리스트에 저장\n",
    "        for target_class in self.classes:\n",
    "            class_idx = self.class_to_idx[target_class]\n",
    "            img_class_dir = os.path.join(self.image_dir, target_class)\n",
    "            ann_class_dir = os.path.join(self.annotation_dir, target_class)\n",
    "            \n",
    "            for fname in os.listdir(img_class_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(img_class_dir, fname)\n",
    "                    # 어노테이션 파일 경로 생성 (확장자 변경)\n",
    "                    ann_fname = os.path.splitext(fname)[0] + '.json'\n",
    "                    ann_path = os.path.join(ann_class_dir, ann_fname)\n",
    "                    \n",
    "                    if os.path.exists(ann_path):\n",
    "                        item = (img_path, ann_path, class_idx)\n",
    "                        self.samples.append(item)\n",
    "        \n",
    "    def _find_classes(self, dir):\n",
    "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, ann_path, label = self.samples[index]\n",
    "\n",
    "        # 어노테이션 파일 읽기\n",
    "        with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "            annotation = json.load(f)\n",
    "        tmp = annotation['annotations']\n",
    "        tmp2 = tmp['bbox'][0]\n",
    "        box = [tmp2['x'], tmp2['y'], tmp2['x'] + tmp2['w'], tmp2['y'] + tmp2['h']] # [x1, y1, x2, y2]\n",
    "\n",
    "        # 이미지 열고 자르기\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        cropped_image = image.crop(box)\n",
    "\n",
    "        # 데이터 변환 적용\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "        \n",
    "        return cropped_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aaa285c-e948-4678-a7d0-83bec6c2fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading configuration and preparing dataset...\n",
      "--> Using device: cuda\n",
      "--> Found 28315 training images and 5662 validation images.\n",
      "--> Classes: ['downy', 'healthy', 'powdery']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 1. 설정 로드 및 준비 \n",
    "# -----------------------------------\n",
    "print(\"1. Loading configuration and preparing dataset...\")\n",
    "try:\n",
    "    with open(\"dino_train_bound.yaml\", 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: config.yaml 파일을 찾을 수 없습니다. 파일을 생성해주세요.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "MODEL_NAME = config['model_name']\n",
    "DATA_PATH = config['data_path']\n",
    "ANNOTATION_PATH = config['annotation_path'] \n",
    "MODEL_REPO = config['model_repo']\n",
    "NUM_CLASSES = config['num_classes']\n",
    "FREEZE_BACKBONE = config['freeze_backbone']\n",
    "EPOCHS = config['epochs']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "DEVICE = torch.device(config['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--> Using device: {DEVICE}\")\n",
    "\n",
    "# --- 데이터 전처리 및 데이터로더  ---\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # ✅ RandomResizedCrop은 잘라낸 이미지에 적용되므로 크기를 더 유연하게 조절 가능\n",
    "        transforms.Resize(256), \n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(256), \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ✅ datasets.ImageFolder 대신 CropDataset 사용\n",
    "train_img_dir = os.path.join(DATA_PATH, 'train')\n",
    "train_ann_dir = os.path.join(ANNOTATION_PATH, 'train')\n",
    "train_dataset = CropDataset(image_dir=train_img_dir, annotation_dir=train_ann_dir, transform=data_transforms['train'])\n",
    "\n",
    "val_img_dir = os.path.join(DATA_PATH, 'validation')\n",
    "val_ann_dir = os.path.join(ANNOTATION_PATH, 'validation')\n",
    "val_dataset = CropDataset(image_dir=val_img_dir, annotation_dir=val_ann_dir, transform=data_transforms['validation'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"--> Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "print(f\"--> Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046f371c-9bcd-48c7-bc41-c1b760a29d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Loading official DINOv2 model 'dinov2_vits14' from torch.hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Backbone is frozen. Only the classifier head will be trained.\n",
      "--> Model loaded and classifier head replaced successfully.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 2. DINOv2 모델 정의 (torch.hub 사용)\n",
    "# -----------------------------------\n",
    "\n",
    "print(f\"\\n2. Loading official DINOv2 model '{MODEL_NAME}' from torch.hub...\")\n",
    "try:\n",
    "    model = torch.hub.load(MODEL_REPO, MODEL_NAME, pretrained=True)\n",
    "    \n",
    "    if FREEZE_BACKBONE:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"--> Backbone is frozen. Only the classifier head will be trained.\")\n",
    "\n",
    "    # --- ✅ 코드 수정 부분 ---\n",
    "    # ViT-Small의 특징 벡터 크기는 384로 고정되어 있습니다.\n",
    "    num_features = 384 \n",
    "    # 기존의 model.head를 우리의 분류기로 완전히 교체합니다.\n",
    "    model.head = nn.Linear(num_features, NUM_CLASSES) \n",
    "    # --- ✅ 수정 완료 ---\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"--> Model loaded and classifier head replaced successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: An unexpected error occurred during model setup.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a4be06-83ea-49c0-bdf4-4362e18591a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Starting the training process...\n",
      "Epoch 01/20 | Train | [====================] 100%\n",
      "Epoch 01/20 | Time: 52:23 | Train Loss: 0.1685 Acc: 0.9430 | Val Loss: 0.0780 Acc: 0.9763\n",
      "Epoch 02/20 | Train | [====================] 100%\n",
      "Epoch 02/20 | Time: 52:38 | Train Loss: 0.0665 Acc: 0.9797 | Val Loss: 0.0526 Acc: 0.9836\n",
      "Epoch 03/20 | Train | [====================] 100%\n",
      "Epoch 03/20 | Time: 51:57 | Train Loss: 0.0510 Acc: 0.9848 | Val Loss: 0.0427 Acc: 0.9868\n",
      "Epoch 04/20 | Train | [====================] 100%\n",
      "Epoch 04/20 | Time: 51:54 | Train Loss: 0.0434 Acc: 0.9875 | Val Loss: 0.0393 Acc: 0.9871\n",
      "Epoch 05/20 | Train | [====================] 100%\n",
      "Epoch 05/20 | Time: 51:49 | Train Loss: 0.0363 Acc: 0.9889 | Val Loss: 0.0317 Acc: 0.9901\n",
      "Epoch 06/20 | Train | [====================] 100%\n",
      "Epoch 06/20 | Time: 51:46 | Train Loss: 0.0344 Acc: 0.9900 | Val Loss: 0.0319 Acc: 0.9898\n",
      "Epoch 07/20 | Train | [====================] 100%\n",
      "Epoch 07/20 | Time: 52:01 | Train Loss: 0.0315 Acc: 0.9905 | Val Loss: 0.0298 Acc: 0.9912\n",
      "Epoch 08/20 | Train | [====================] 100%\n",
      "Epoch 08/20 | Time: 52:15 | Train Loss: 0.0307 Acc: 0.9906 | Val Loss: 0.0276 Acc: 0.9913\n",
      "Epoch 09/20 | Train | [====================] 100%\n",
      "Epoch 09/20 | Time: 52:23 | Train Loss: 0.0281 Acc: 0.9914 | Val Loss: 0.0260 Acc: 0.9917\n",
      "Epoch 10/20 | Train | [====================] 100%\n",
      "Epoch 10/20 | Time: 51:50 | Train Loss: 0.0278 Acc: 0.9913 | Val Loss: 0.0271 Acc: 0.9913\n",
      "Epoch 11/20 | Train | [====================] 100%\n",
      "Epoch 11/20 | Time: 55:57 | Train Loss: 0.0282 Acc: 0.9900 | Val Loss: 0.0225 Acc: 0.9929\n",
      "Epoch 12/20 | Train | [====================] 100%\n",
      "Epoch 12/20 | Time: 52:32 | Train Loss: 0.0255 Acc: 0.9913 | Val Loss: 0.0239 Acc: 0.9921\n",
      "Epoch 13/20 | Train | [====================] 100%\n",
      "Epoch 13/20 | Time: 52:00 | Train Loss: 0.0241 Acc: 0.9917 | Val Loss: 0.0215 Acc: 0.9936\n",
      "Epoch 14/20 | Train | [====================] 100%\n",
      "Epoch 14/20 | Time: 52:02 | Train Loss: 0.0238 Acc: 0.9919 | Val Loss: 0.0198 Acc: 0.9935\n",
      "Epoch 15/20 | Train | [====================] 100%\n",
      "Epoch 15/20 | Time: 52:01 | Train Loss: 0.0237 Acc: 0.9924 | Val Loss: 0.0254 Acc: 0.9906\n",
      "Epoch 16/20 | Train | [====================] 100%\n",
      "Epoch 16/20 | Time: 51:51 | Train Loss: 0.0220 Acc: 0.9918 | Val Loss: 0.0197 Acc: 0.9942\n",
      "Epoch 17/20 | Train | [====================] 100%\n",
      "Epoch 17/20 | Time: 52:02 | Train Loss: 0.0233 Acc: 0.9925 | Val Loss: 0.0191 Acc: 0.9935\n",
      "Epoch 18/20 | Train | [====================] 100%\n",
      "Epoch 18/20 | Time: 51:53 | Train Loss: 0.0222 Acc: 0.9927 | Val Loss: 0.0212 Acc: 0.9921\n",
      "Epoch 19/20 | Train | [====================] 100%\n",
      "Epoch 19/20 | Time: 51:57 | Train Loss: 0.0206 Acc: 0.9929 | Val Loss: 0.0381 Acc: 0.9859\n",
      "Epoch 20/20 | Train | [====================] 100%\n",
      "Epoch 20/20 | Time: 51:51 | Train Loss: 0.0218 Acc: 0.9929 | Val Loss: 0.0169 Acc: 0.9951\n",
      "\n",
      "Training complete!\n",
      "Model saved to dinov2_bound_crop.pth\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# 3. 학습 (Training)\n",
    "# ----------------------------------\n",
    "\n",
    "print(\"\\n3. Starting the training process...\")\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의 (기존과 동일)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# 총 배치 수 미리 계산 (진행률 표시용)\n",
    "total_batches = len(train_loader)\n",
    "gauge_step = total_batches // 20 # 약 5%마다 게이지를 업데이트하기 위한 스텝\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- 에포크 시작 시간 기록 ---\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # --- 학습 단계 --\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # enumerate를 사용하여 배치 인덱스를 가져옴\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # --- 진행 게이지 출력 ---\n",
    "        # sys.stdout.write와 '\\r'을 사용하여 같은 줄에 덮어쓰기\n",
    "        progress = (batch_idx + 1) / total_batches\n",
    "        gauge_bar = '=' * int(progress * 20)\n",
    "        sys.stdout.write(f\"\\rEpoch {epoch+1:02d}/{EPOCHS} | Train | [{'%-20s' % gauge_bar}] {progress:.0%}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    \n",
    "    # 게이지 줄바꿈 처리\n",
    "    print()\n",
    "\n",
    "    # --- 검증 단계 --\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_corrects.double() / len(val_dataset)\n",
    "\n",
    "    # --- 에포크 종료 시간 기록 및 출력 ---\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    # 분, 초로 변환\n",
    "    epoch_mins, epoch_secs = divmod(epoch_duration, 60)\n",
    "\n",
    "    # 최종 결과 출력\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
    "        f\"Time: {int(epoch_mins):02d}:{int(epoch_secs):02d} | \"\n",
    "        f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "torch.save(model.state_dict(), 'dinov2_bound_crop.pth')\n",
    "print(\"Model saved to dinov2_bound_crop.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea9ad-9b75-4989-8537-1def92400ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultra",
   "language": "python",
   "name": "ultra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
