{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c904e30-1ca8-4642-8d9f-bae7b07b3b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading the fine-tuned model structure and weights...\n",
      "--> Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\51100/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import types\n",
    "# -----------------------------------\n",
    "# 1. 모델 구조 정의 및 가중치 로드\n",
    "# -----------------------------------\n",
    "print(\"1. Loading the fine-tuned model structure and weights...\")\n",
    "\n",
    "# --- 설정값  ---\n",
    "MODEL_REPO = \"facebookresearch/dinov2\"\n",
    "MODEL_NAME = \"dinov2_vits14\"\n",
    "\n",
    "NUM_CLASSES = 3 \n",
    "CLASS_NAMES = ['downy','healthy', 'powdery']\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"--> Using device: {DEVICE}\")\n",
    "# --- 모델 구조 만들기  ---\n",
    "try:\n",
    "    # torch.hub를 이용해 DINOv2 모델 구조 로드\n",
    "    model = torch.hub.load(MODEL_REPO, MODEL_NAME, pretrained=False) # pretrained=False로 설정\n",
    "    \n",
    "    num_features = 384 # ViT-Small의 특징 벡터 크기\n",
    "    model.head = nn.Linear(num_features, NUM_CLASSES)\n",
    "    \n",
    "    # --- 저장된 가중치 불러오기 ---\n",
    "    model.load_state_dict(torch.load('dinov2_hub_finetuned_model.pth', map_location=DEVICE))\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"--> Model loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: Failed to load the model.\")\n",
    "    print(f\"--> Ensure 'dinov2_hub_finetuned_model.pth' is in the same directory.\")\n",
    "    print(f\"--> Original Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bb1276-98ac-43e0-859d-c15b4c740b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2. 이미지 전처리\n",
    "# ----------------------------------\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbfd0a8-8f3d-42a2-9f94-988873f0d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 3. 몽키 패치를 이용한 어텐션 추출 및 시각화\n",
    "# ----------------------------------\n",
    "\n",
    "def predict_and_visualize_attention(image_path):\n",
    "    try:\n",
    "        original_img = cv2.imread(image_path)\n",
    "        original_img = cv2.resize(original_img, (224, 224))\n",
    "\n",
    "        pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = preprocess(pil_img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # ✅ 1. 마지막 어텐션 블록의 원래 forward 함수를 따로 저장\n",
    "        attn_block = model.blocks[-1].attn\n",
    "        attn_forward_orig = attn_block.forward\n",
    "\n",
    "        # ✅ 2. 어텐션 맵을 저장할 새로운 forward 함수 정의\n",
    "        def new_attn_forward(self, x):\n",
    "            # MemEffAttention의 내부 로직을 그대로 수행\n",
    "            B, N, C = x.shape\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "            # Q와 K를 곱하여 어텐션 스코어 계산\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            \n",
    "            # --- 우리가 필요한 어텐션 맵을 self 객체에 저장 ---\n",
    "            self.attention_map = attn\n",
    "            \n",
    "            # 원래 forward 함수의 나머지 로직 수행\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "            x = self.proj(x)\n",
    "            x = self.proj_drop(x)\n",
    "            return x\n",
    "        \n",
    "        # ✅ 3. 마지막 어텐션 블록의 forward 함수를 우리가 만든 새 함수로 교체\n",
    "        attn_block.forward = types.MethodType(new_attn_forward, attn_block)\n",
    "\n",
    "        print(f\"\\n3. Running inference and extracting attention via monkey-patching...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 모델을 실행하면, 교체된 new_attn_forward가 자동으로 실행됨\n",
    "            outputs = model(img_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "            top_prob, top_catid = torch.max(probabilities, 0)\n",
    "            predicted_class = CLASS_NAMES[top_catid]\n",
    "            confidence = top_prob.item()\n",
    "\n",
    "        # ✅ 4. 어텐션 블록의 forward 함수를 원래대로 복원\n",
    "        attn_block.forward = attn_forward_orig\n",
    "        \n",
    "        # --- 어텐션 맵 처리 ---\n",
    "        # self.attention_map에 저장된 맵을 가져옴\n",
    "        attn_map_tensor = attn_block.attention_map\n",
    "        \n",
    "        # (이하 시각화 코드는 모두 동일)\n",
    "        attn_map = attn_map_tensor[0].mean(axis=0).cpu().numpy()\n",
    "        cls_attn_map = attn_map[0, 1:]\n",
    "        grid_size = int(np.sqrt(cls_attn_map.shape[0]))\n",
    "        attention_grid = cls_attn_map.reshape(grid_size, grid_size)\n",
    "        attention_heatmap = cv2.normalize(attention_grid, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "        attention_heatmap = cv2.applyColorMap(attention_heatmap, cv2.COLORMAP_JET)\n",
    "        attention_heatmap = cv2.resize(attention_heatmap, (224, 224))\n",
    "        superimposed_img = cv2.addWeighted(original_img, 0.6, attention_heatmap, 0.4, 0)\n",
    "\n",
    "        # --- 결과 출력 ---\n",
    "        print(\"\\n--- Inference Result ---\")\n",
    "        print(f\"Predicted Class: {predicted_class}\")\n",
    "        print(f\"Confidence: {confidence:.2%}\")\n",
    "        # ... (이하 OpenCV 출력 코드는 동일) ...\n",
    "        cv2.imshow(\"Superimposed Image\", superimposed_img)\n",
    "        cv2.imshow(\"Original\", original_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred: {e}\")\n",
    "        # 만약 forward 함수를 복원하기 전에 오류가 나면 복원해주는 코드\n",
    "        if 'attn_forward_orig' in locals():\n",
    "            model.blocks[-1].attn.forward = attn_forward_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6090b2db-a399-4c4d-bb30-73665274e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Running inference and extracting attention via monkey-patching...\n",
      "\n",
      "--- Inference Result ---\n",
      "Predicted Class: powdery\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# --- 추론할 이미지 경로 지정 ---\n",
    "image_to_predict = 'C:/blooming_AI/classification_dataset/test/powdery/306942_20210914_4_1_a4_3_2_12_2_0.jpg' # 추론할 이미지 경\n",
    "predict_and_visualize_attention(image_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd8d2a-cdef-42e7-a904-cd6c074c1972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultra",
   "language": "python",
   "name": "ultra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
